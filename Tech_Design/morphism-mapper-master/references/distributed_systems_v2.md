# Domain: 分布式系统
# Source: Leslie Lamport, Eric Brewer, Jim Gray, Barbara Liskov, Martin Kleppmann
# Structural_Primitives: consensus, consistency, partition_tolerance, replication, fault_tolerance, availability, clock, message_passing

## Fundamentals (100 基本基石)

### 导语

分布式系统是工程实践中最深刻的妥协艺术。其核心矛盾在于：我们试图用不可靠的组件构建可靠的系统，在没有全局视角的前提下达成全局一致，在必然出现的网络分区中维持持续可用。这里没有银弹，只有残酷的权衡——CAP定理宣告一致性、可用性、分区容错不可兼得；FLP不可能结果证明确定性共识在异步系统中无法实现。每一个分布式系统都是在限制条件下的次优解，理解这些限制，才能做出清醒的工程选择。

---

### 一、哲学观 (18条)

1. 网络分区是必然的而非异常
2. 节点故障是常态而非意外
3. 延迟是分布式系统的本质属性
4. 不存在全局统一时钟
5. 消息可能丢失、延迟或乱序
6. 一致性必然以可用性为代价
7. 异步系统无法确定性共识
8. 容错是分布式系统的核心目标
9. 复制是容错的基础手段
10. 幂等性是可靠性的关键
11. 顺序保证需要协调成本
12. 局部知识无法推知全局状态
13. 拜占庭故障是最困难的挑战
14. 最终一致性是务实的妥协
15. 线性一致性是昂贵的理想
16. 系统设计即权衡的艺术
17. 可观测性先于可控制性
18. 没有完美的分布式系统

---

### 二、核心原则 (22条)

19. 网络延迟无上界
20. 时钟同步不可靠
21. CAP三者最多取其二
22. FLP不可能限制异步共识
23. 两将军问题无完美解
24. 复制提高可用但增加复杂度
25. 一致性需要多数派确认
26. 提交后才算真正完成
27. 幂等操作可安全重试
28. 租约机制处理节点失效
29. 心跳检测识别故障节点
30. 版本向量追踪因果关系
31. 向量时钟记录事件顺序
32. Paxos保证安全性优先
33. Raft追求可理解性
34. Gossip协议实现最终一致
35. 背压防止系统过载
36. 熔断机制快速失败
37. 限流保护系统资源
38. 优雅降级保持核心功能
39. 混沌工程验证弹性
40. 可观测性是调试的前提

---

### 三、思维模型 (28条)

41. 主从复制：一写多读模型
42. 多主复制：多点写入冲突
43. 无主复制：读写仲裁
44. 同步复制：强一致性保证
45. 异步复制：高性能优先
46. 半同步复制：折中方案
47. 共识算法：Paxos协议
48. 共识算法：Raft协议
49. 共识算法：Zab协议
50. 两阶段提交：原子性保证
51. 三阶段提交：阻塞优化
52. 向量时钟：因果一致性
53. 版本向量：冲突检测
54. Merkle树：快速比对
55. Bloom过滤器：高效成员检测
56. 一致性哈希：均匀分布
57. 虚拟节点：负载均衡
58. Quorum机制：读写协调
59. Hinted Handoff：临时存储
60. Read Repair：读时修复
61. Anti-Entropy：后台同步
62. Gossip协议：epidemic传播
63. Phi累积故障检测
64. 分布式事务：Saga模式
65. 分布式事务：TCC模式
66. 事件溯源：状态重建
67. CQRS：读写分离
68. CRDT：无冲突数据类型

---

### 四、关键方法论 (22条)

69. 系统建模：状态机与事件图
70. 一致性级别选择：强到弱
71. 复制策略设计：同步异步
72. 分区容忍设计：降级策略
73. 共识算法实现：Paxos/Raft
74. 故障检测：心跳与超时
75. 成员管理：动态增删节点
76. 负载均衡：请求分发
77. 数据分片：水平扩展
78. 路由设计：请求定位
79. 缓存策略：多级缓存
80. 并发控制：乐观与悲观
81. 冲突解决：Last-Write-Win
82. 冲突解决：业务逻辑合并
83. 监控设计：metrics与tracing
84. 日志聚合：集中式日志
85. 容量规划：性能预测
86. 混沌工程：故障注入
87. 压力测试：极限验证
88. 降级演练：预案验证
89. 容灾设计：异地多活
90. 数据迁移：在线迁移

---

### 五、避坑指南 (10条)

91. 警惕忽视网络分区的可能性
92. 避免假设时钟同步的可靠性
93. 勿将异步当作同步处理
94. 不要忽视幂等性设计
95. 警惕分布式事务的性能陷阱
96. 避免脑裂(split-brain)问题
97. 勿忽略故障检测的延迟
98. 不要过度追求强一致性
99. 警惕复制延迟导致的读取旧数据
100. 避免忽视监控和可观测性

---

## Core Objects (14个)

- **Node**: 分布式系统中的独立计算单元，具有本地存储和处理能力，可能随时故障
  - *本质*: 系统的原子构件
  - *关联*: 消息、状态、故障

- **Message**: 节点间通信的载体，可能丢失、延迟、乱序或重复到达
  - *本质*: 分布式系统的血液
  - *关联*: 节点、网络、协议

- **Consensus**: 多个节点对某一值达成一致的协议状态，分布式系统核心难题
  - *本质*: 分布式一致性的目标
  - *关联*: 节点、协议、决策

- **Clock**: 时间测量机制，分布式系统中不存在全局统一时钟，只有本地时钟
  - *本质*: 事件ordering难题
  - *关联*: 事件、顺序、因果

- **Partition**: 网络故障导致节点间通信中断形成的隔离区域，CAP中的P
  - *本质*: 网络不确定性的体现
  - *关联*: 网络、可用性、一致性

- **Replication**: 数据或状态在多个节点间的副本，提高可用性和容错性
  - *本质*: 冗余以求可靠性
  - *关联*: 节点、一致性、可用性

- **Transaction**: 原子性的操作序列，满足ACID或BASE特性，跨节点协调的挑战
  - *本质*: 原子性的分布式实现
  - *关联*: 一致性、隔离性、持久性

- **Leader**: 协调者节点，处理写操作或决策，单点可能成为瓶颈或故障点
  - *本质*: 集中协调的节点
  - *关联*: 跟随者、选举、故障转移

- **Follower**: 跟随领导者状态的节点，复制日志或数据，可提升为领导者
  - *本质*: 被动复制的节点
  - *关联*: 领导者、复制、选举

- **Quorum**: 多数派机制，读写操作需要获得多数节点确认以保证一致性
  - *本质*: 多数决的数学基础
  - *关联*: 投票、一致性、可用性

- **Vector Clock**: 向量时钟，记录事件的因果关系而非物理时间，用于冲突检测
  - *本质*: 逻辑时间的表示
  - *关联*: 事件、因果、版本

- **Log**: 不可变的操作序列，分布式系统的核心数据结构，复制和恢复的基础
  - *本质*: 状态的变更历史
  - *关联*: 复制、恢复、一致性

- **Shard**: 数据分片，将数据水平分割到不同节点，实现可扩展性
  - *本质*: 水平扩展的基本单元
  - *关联*: 分区、路由、扩展

- **Client**: 访问分布式系统的客户端，可能观察到过时的数据或不一致的状态
  - *本质*: 系统的使用者视角
  - *关联*: 请求、响应、一致性

---

## Core Morphisms (14个)

- **Replication**: 数据/状态在多个节点间复制，提高可用性和容错性
  - *涉及*: 主节点、从节点、数据
  - *动态*: 状态的同步传播

- **Consensus Protocol**: 节点间交换消息达成一致决策，如Paxos、Raft
  - *涉及*: 提议者、接受者、学习者
  - *动态*: 提议、投票、提交

- **Transaction**: 原子性的操作序列，跨节点协调保证ACID特性
  - *涉及*: 协调者、参与者、资源
  - *动态*: 准备、提交、回滚

- **Failure Detection**: 监控节点健康状态，识别崩溃或网络分区
  - *涉及*: 检测者、被检测者、超时
  - *动态*: 心跳、怀疑、确认

- **Leader Election**: 在节点中选出协调者，处理单点决策需求
  - *涉及*: 候选者、投票、任期
  - *动态*: 竞选、投票、就职

- **Message Passing**: 节点间通过消息通信，唯一的信息交换方式
  - *涉及*: 发送者、接收者、网络
  - *动态*: 发送、传输、接收

- **State Synchronization**: 将节点状态与其他节点对齐，达成一致
  - *涉及*: 源节点、目标节点、差异
  - *动态*: 比对、传输、应用

- **Request Routing**: 将客户端请求定向到适当的节点处理
  - *涉及*: 客户端、路由器、服务端
  - *动态*: 解析、选择、转发

- **Load Balancing**: 将请求均匀分布到多个节点，避免单点过载
  - *涉及*: 请求、负载均衡器、后端节点
  - *动态*: 度量、决策、分发

- **Partition Handling**: 网络分区时的决策逻辑，一致性vs可用性选择
  - *涉及*: 分区检测、决策逻辑、执行
  - *动态*: 检测、判断、响应

- **Log Replication**: 将操作日志复制到多个节点，保证持久性和一致性
  - *涉及*: 领导者、跟随者、日志条目
  - *动态*: 追加、复制、提交

- **Snapshot**: 系统状态的时间点捕获，用于快速恢复和备份
  - *涉及*: 状态、存储、时间点
  - *动态*: 捕获、存储、恢复

- **Membership Change**: 动态增删集群节点，保持系统可用性
  - *涉及*: 新节点、旧节点、集群配置
  - *动态*: 加入、同步、退出

- **Retry with Backoff**: 失败后的指数退避重试，避免雪崩
  - *涉及*: 客户端、服务端、等待时间
  - *动态*: 失败、等待、重试

---

## Theorems / Patterns (18个)

### 1. CAP定理
**内容**: 分布式系统不可能同时满足一致性(Consistency)、可用性(Availability)、分区容错性(Partition Tolerance)，最多只能同时满足其中两项。分区容错是网络的基本特性，因此实际是在C和A之间权衡。

**Applicable_Structure**: 需要权衡数据一致性与系统可用性的分布式架构设计

**Mapping_Hint**: 当Domain A面临网络分区或通信中断时，必须做出明确选择：一致性优先(CP)还是可用性优先(AP)。选择CP意味着分区期间拒绝服务以保证数据一致，选择AP意味着继续服务但可能返回过时数据。关键决策点：识别业务场景中哪种属性不可妥协——银行交易选CP，社交网络选AP。

**Case_Study**: 银行转账系统选择CP：分区发生时拒绝交易以保证账户余额一致性；电商商品展示选择AP：分区时继续展示商品信息，允许短暂库存不一致；ZooKeeper选择CP保证配置一致性；Cassandra可配置选择AP保证写入可用性。

---

### 2. FLP不可能结果
**内容**: 在异步系统中（消息延迟无上界），即使只有一个进程可能故障，也不存在确定性的共识算法。这是分布式系统的基本限制，无法通过算法设计绕过。

**Applicable_Structure**: 需要达成共识但无法保证时限的分布式决策场景

**Mapping_Hint**: 当Domain A的协调依赖于异步通信且存在不确定性时，放弃追求100%确定性共识。转而采用：(1) 超时机制：设定合理期限，超时则认为未达成共识；(2) 概率性共识：接受可能失败的共识尝试，通过重试达到最终一致；(3) 引入部分同步假设：假设系统在某些时期是同步的。

**Case_Study**: 区块链系统通过引入随机性和经济激励（而非纯算法）绕过FLP限制，实现概率性共识；Paxos通过leader选举引入部分同步假设，在leader稳定时实现高效共识；任何跨部门决策中，完全共识往往不可能，需要设定决策时限。

---

### 3. 两将军问题
**内容**: 在不可靠信道上，两个将军无法通过有限次消息交换确保达成共识（进攻或撤退）。即使加入确认消息的确认，仍然无法完全确定对方收到。

**Applicable_Structure**: 通信不可靠时的协调难题

**Mapping_Hint**: 当Domain A的协调依赖于不可靠通信时，放弃追求绝对确认，转而设计容错机制。具体步骤：(1) 接受不确定性：承认无法100%确认对方状态；(2) 设计容错行为：即使对方未收到消息，也有合理的默认行为；(3) 引入超时和重试：通过多次尝试提高成功概率；(4) 幂等性设计：允许重复执行而不产生副作用。

**Case_Study**: TCP三次握手只能建立大概率可信的连接，无法绝对保证双方同时准备好通信；分布式事务的两阶段提交无法完全解决协调问题，因此需要Saga等补偿机制；远程团队协作中，无法确认对方是否理解，需要设计可独立执行的子任务。

---

### 4. Quorum机制
**内容**: 读写操作需要获得多数节点确认的机制。R + W > N 保证读写重叠，从而保证一致性。N=副本数，R=读确认数，W=写确认数。

**Applicable_Structure**: 可调一致性级别的分布式存储

**Mapping_Hint**: 当Domain A需要协调多个决策节点时，应用Quorum思维：(1) 确定总节点数N；(2) 设定读确认数R和写确认数W，满足R+W>N；(3) 高一致性场景：提高W，降低R（写时严格，读时宽松）；(4) 高性能场景：降低W，提高R（写时宽松，读时校验）；(5) 权衡公式：一致性强度 = (R+W-N)/N。

**Case_Study**: Amazon Dynamo的NWR模型：N=3，W=2，R=2实现读写一致性；Cassandra的可调一致性：ONE/QUORUM/ALL；投票决策中，设定通过门槛（如2/3多数）；任何需要多方确认的场景，用Quorum平衡效率与可靠性。

---

### 5. 共识算法(Paxos/Raft)
**内容**: 在节点故障情况下仍能达成一致的算法。Paxos保证安全性（不会达成一致的错误值），Raft追求可理解性。核心思想：先选leader，再由leader协调一致性。

**Applicable_Structure**: 需要强一致性保证的多节点决策

**Mapping_Hint**: 当Domain A需要多节点一致决策时，应用共识算法逻辑：(1) 选举阶段：节点竞争成为协调者（leader），获得多数支持即可；(2) 提议阶段：leader提出方案，follower接受或拒绝；(3) 提交阶段：获得多数接受后，方案生效；(4) 关键：任何阶段leader故障，重新选举，新leader继续未完成的提议。

**Case_Study**: etcd使用Raft实现分布式配置一致性；ZooKeeper使用Zab协议管理集群状态；企业决策中，明确决策流程：提案→讨论→表决→执行→确认；任何委员会决策都可应用此模式。

---

### 6. 向量时钟
**内容**: 记录事件的因果关系而非物理时间。每个节点维护一个计数器向量，通过比较向量判断事件的先后或并发关系。

**Applicable_Structure**: 需要确定事件因果关系的分布式系统

**Mapping_Hint**: 当Domain A的事件可能并发发生时，使用向量时钟追踪因果关系：(1) 为每个参与者维护一个计数器；(2) 本地事件：增加自己的计数器；(3) 发送消息：附带完整向量；(4) 接收消息：取各维度最大值，增加自己的计数器；(5) 比较：全维度大于=先后，有大于有小于=并发。

**Case_Study**: DynamoDB使用向量时钟检测写冲突；Git的提交历史本质上也是向量时钟；版本控制系统追踪代码变更的因果关系；任何多人协作编辑场景，需要判断修改的先后顺序或冲突。

---

### 7. Gossip协议
**内容**: Epidemic传播算法，节点随机选择邻居交换信息，像谣言传播一样最终达到全网一致。高效、容错、可扩展。

**Applicable_Structure**: 大规模分布式系统中的信息传播

**Mapping_Hint**: 当Domain A需要在大规模群体中传播信息时，应用Gossip机制：(1) 随机选择：每次随机选择k个邻居传播；(2) 周期性重复：定期重复传播，确保覆盖；(3) 去中心化：没有单点瓶颈；(4) 容错：部分节点故障不影响整体传播；(5) 收敛时间：O(log N)轮传播可覆盖全网。

**Case_Study**: Cassandra、Amazon Dynamo使用Gossip进行成员发现；Redis Cluster使用Gossip同步集群状态；比特币网络层传播交易和区块；病毒式营销利用社交网络的Gossip特性；组织中的非正式信息流动。

---

### 8. 一致性哈希
**内容**: 将数据和节点映射到同一个哈希环上，节点增减只影响相邻数据，最小化重新分配。解决了传统哈希在节点变化时的全量迁移问题。

**Applicable_Structure**: 需要动态扩缩容的分布式存储

**Mapping_Hint**: 当Domain A需要动态调整资源分配时，应用一致性哈希：(1) 构建哈希环：0到2^32-1的环形空间；(2) 映射节点：每个节点计算哈希值，映射到环上位置；(3) 映射数据：数据哈希值顺时针找到第一个节点；(4) 节点增加：只影响新节点到前一节点之间的数据；(5) 节点减少：数据迁移给顺时针下一个节点。

**Case_Study**: Amazon Dynamo的分布式存储；Memcached客户端的路由；Cassandra的数据分区；CDN的内容路由；任何需要动态扩缩容的系统，一致性哈希最小化变化影响范围。

---

### 9. 背压机制
**内容**: 当系统组件处理能力不足时，向上游传递压力信号，实现流量自适应调节而非崩溃。反压是流控的反馈机制。

**Applicable_Structure**: 具有多级处理环节的流水线系统

**Mapping_Hint**: 当Domain A存在多级处理且下游可能过载时，实施背压：(1) 监控下游缓冲区：检测是否接近满载；(2) 信号向上传递：当下游满载时，通知上游减速；(3) 自适应调节：上游根据背压信号调整生产速率；(4) 避免堆积：防止缓冲区无限增长导致OOM。

**Case_Study**: Netflix的RxJava使用背压自动调节生产消费速率；生产线上的看板系统限制在制品数量；组织的逐级汇报机制防止信息过载；任何流水线系统，下游瓶颈应传导到上游而非堆积。

---

### 10. 熔断器模式
**内容**: 当错误率超过阈值时，快速失败而非等待超时，防止级联故障。熔断后定期探测恢复。保护系统免受过载影响。

**Applicable_Structure**: 微服务架构中的故障隔离

**Mapping_Hint**: 当Domain A的某个组件频繁故障影响整体时，应用熔断器：(1) 监控指标：跟踪错误率和响应时间；(2) 设定阈值：错误率超过x%触发熔断；(3) 快速失败：熔断后新请求立即返回错误，不再调用故障组件；(4) 定期探测：熔断后每y秒探测一次，成功后恢复；(5) 降级策略：熔断期间使用备选方案或缓存数据。

**Case_Study**: Netflix的Hystrix实现熔断和降级；Spring Cloud Circuit Breaker；电商系统中，支付服务故障时熔断并引导用户稍后重试；防止雪崩效应的关键机制。

---

### 11. Saga模式
**内容**: 长事务的拆分模式，将大事务拆分为多个本地事务，每个有对应的补偿操作。通过补偿而非回滚实现最终一致性。

**Applicable_Structure**: 跨服务的长时间业务操作

**Mapping_Hint**: 当Domain A的长时间操作跨多个独立组件时，应用Saga模式：(1) 拆分事务：将长事务拆分为多个短事务；(2) 定义正向操作：每个步骤的执行逻辑；(3) 定义补偿操作：每个步骤的撤销逻辑；(4) 编排方式：选择编排式（中心化协调）或协同式（事件驱动）；(5) 失败处理：某步失败时，执行已完步骤的补偿操作。

**Case_Study**: 电商订单：下单-支付-发货-送达，每步可补偿；旅行预订：机票+酒店+租车，任一失败需取消其他；Axon框架、Seata等分布式事务解决方案；任何跨部门的长流程，需要设计每步的可撤销机制。

---

### 12. CQRS模式
**内容**: 读写分离架构，命令（写）和查询（读）使用不同的模型和存储。读模型可优化查询，写模型可保证一致性。

**Applicable_Structure**: 读多写少、读写需求差异大的系统

**Mapping_Hint**: 当Domain A的读和写需求冲突时，应用CQRS：(1) 分离模型：写模型关注一致性，读模型关注查询效率；(2) 独立存储：写存储用关系型，读存储用NoSQL或缓存；(3) 事件同步：写操作发布事件，读模型消费并更新；(4) 最终一致：接受读写短暂不一致；(5) 优化读端：为特定查询设计物化视图。

**Case_Study**: 电商系统：订单写入优化一致性，查询优化性能；事件溯源系统常与CQRS结合；报告的生成与事务数据分离；任何读多写少且读写模式差异大的场景。

---

### 13. 事件溯源
**内容**: 存储状态变更的事件序列而非最终状态，状态可通过重放事件重建。提供了完整的历史和审计能力。

**Applicable_Structure**: 需要完整审计、状态可重建的系统

**Mapping_Hint**: 当Domain A需要完整的历史记录时，应用事件溯源：(1) 存储事件：记录"发生了什么"而非"当前状态"；(2) 不可变日志：事件一旦写入不可修改；(3) 状态重建：通过重放事件计算当前状态；(4) 时间旅行：可以重建任意时间点的状态；(5) 审计能力：完整记录所有变更历史。

**Case_Study**: 银行账户：存储交易记录而非余额；版本控制系统Git；Axon框架、Event Store；任何需要完整变更历史的业务场景，如金融、医疗、法律。

---

### 14. 脑裂问题与解决方案
**内容**: 网络分区时，多个节点同时认为自己是主节点，导致数据冲突。解决方案：Quorum机制、 fencing token、专门仲裁节点。

**Applicable_Structure**: 存在主从切换的分布式系统

**Mapping_Hint**: 当Domain A存在主从结构且可能通信中断时，防范脑裂：(1) 识别风险：网络分区时，原主节点和晋升的新主节点可能同时服务；(2) Quorum预防：只有获得多数节点支持才能成为主节点；(3) Fencing机制：原主节点发现自己失去多数支持时主动降级；(4) 仲裁节点：引入独立第三方仲裁谁才是真正的主节点。

**Case_Study**: MongoDB的副本集使用仲裁节点；Redis Sentinel使用多数派选举；任何存在主备切换的系统，必须防止双主同时存在；组织中的权力交接也需要防止多头领导。

---

### 15. 幂等性设计
**内容**: 操作执行一次或多次效果相同。在不可靠网络中，消息可能重复，幂等性保证重复处理不会导致错误状态。

**Applicable_Structure**: 不可靠通信环境中的可靠性保证

**Mapping_Hint**: 当Domain A的操作可能重复执行时，设计幂等性：(1) 识别幂等操作：天然幂等的（查询、删除指定ID）vs 需设计的（创建、更新）；(2) 唯一标识：为每个操作分配唯一ID，服务端记录已处理ID；(3) 状态检查：执行前检查是否已处理；(4) 版本控制：更新时检查版本号，避免覆盖；(5) 返回一致：重复执行返回与首次相同的结果。

**Case_Study**: HTTP方法的幂等性：GET、PUT、DELETE幂等，POST不幂等；支付系统的去重：同一订单号多次支付只扣一次款；订单处理的幂等设计；任何可能重试的操作都需要幂等性。

---

### 16. 租约机制
**内容**: 节点获取有时间限制的授权（租约），在租约期内拥有某种权限。租约过期需续租，未续租则权限收回。

**Applicable_Structure**: 需要临时授权且能自动回收权限的系统

**Mapping_Hint**: 当Domain A需要临时授权且能自动失效时，应用租约机制：(1) 设定租期：授权有效期，如30秒；(2) 自动续租：持有者在租期内定期续租；(3) 失效检测：超过租期未续租，自动收回权限；(4) 时钟容错：考虑时钟漂移，设置合理误差范围；(5) 优雅处理：租约到期前提前警告，允许宽限期。

**Case_Study**: 分布式锁的租约实现：持有锁的节点必须定期续租；HDFS的租约回收：客户端故障后自动释放租约；缓存过期机制；任何需要"超时自动释放"权限的场景。

---

### 17. 分布式事务的两阶段提交
**内容**: 协调者先询问所有参与者是否可提交（准备阶段），参与者锁定资源并回复；协调者收到全部Yes后发送提交指令（提交阶段）。保证原子性但阻塞。

**Applicable_Structure**: 需要强一致性的跨资源事务

**Mapping_Hint**: 当Domain A需要跨多个独立资源保证原子性时，应用两阶段提交：(1) 准备阶段：协调者询问各参与者是否可以提交，参与者锁定资源并回复Yes/No；(2) 决策阶段：协调者收集回复，全部Yes则决定提交，任一No则决定回滚；(3) 执行阶段：协调者发送决策，参与者执行并解锁；(4) 阻塞风险：任一参与者在准备后故障，其他参与者需等待恢复。

**Case_Study**: 数据库的XA协议实现分布式事务；金融系统的跨行转账；库存与支付的联动；任何需要"要么全做要么全不做"的跨系统操作。

---

### 18. 混沌工程
**内容**: 通过在生产环境注入故障来验证系统弹性。假设系统会有故障，通过主动故障注入来发现和修复弱点。

**Applicable_Structure**: 需要验证容错能力的大规模系统

**Mapping_Hint**: 当Domain A需要验证其容错能力时，实施混沌工程：(1) 建立稳态：定义系统正常运行的指标；(2) 假设验证：提出"如果X故障，系统仍能Y"的假设；(3) 故障注入：在生产环境模拟故障，如随机终止实例；(4) 观察验证：监控系统是否保持稳态；(5) 修复强化：发现弱点后修复，增强系统弹性。

**Case_Study**: Netflix的Chaos Monkey随机终止生产实例验证弹性；Amazon的GameDay定期进行故障演练；Gremlin平台提供故障注入工具；任何关键系统都应定期验证其在故障下的表现。

---

## Tags

- distributed_systems
- cap_theorem
- consensus
- replication
- fault_tolerance
- consistency
- partition
- scalability
- microservices
- byzantine_fault
- paxos
- raft
- eventual_consistency
- distributed_transaction
- chaos_engineering
- load_balancing
- sharding
- message_passing
