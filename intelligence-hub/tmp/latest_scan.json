{
  "metadata": {
    "timestamp": "2026-02-20T10:18:33.746348",
    "sources": {
      "ajmc": "undefined entity: line 68, column 50",
      "mitchellh.com": "OK",
      "v2ex": "OK",
      "himss": "not well-formed (invalid token): line 1, column 766",
      "ericmigi.com": "OK",
      "daringfireball.net": "OK",
      "jeffgeerling.com": "OK",
      "idiallo.com": "OK",
      "devblogs.microsoft.com/oldnewthing": "OK",
      "producthunt": "OK",
      "lcamtuf.substack.com": "OK",
      "simonwillison.net": "OK",
      "seangoedecke.com": "OK",
      "krebsonsecurity.com": "OK",
      "shkspr.mobi": "OK",
      "maurycyz.com": "OK",
      "healthit": "OK",
      "pluralistic.net": "OK",
      "github": "OK",
      "dynomight.net": "OK",
      "lucumr.pocoo.org": "OK",
      "garymarcus.substack.com": "OK",
      "xeiaso.net": "OK",
      "johndcook.com": "OK",
      "matklad.github.io": "OK",
      "overreacted.io": "OK",
      "skyfall.dev": "OK",
      "righto.com": "OK",
      "terriblesoftware.org": "OK",
      "timsh.org": "OK",
      "derekthompson.org": "OK",
      "evanhahn.com": "OK",
      "xania.org": "OK",
      "rakhim.exotext.com": "OK",
      "nesbitt.io": "OK",
      "antirez.com": "OK",
      "entropicthoughts.com": "OK",
      "construction-physics.com": "OK",
      "jayd.ml": "OK",
      "tedium.co": "OK",
      "micahflee.com": "OK",
      "buttondown.com/hillelwayne": "OK",
      "geohot.github.io": "OK",
      "paulgraham.com": "OK",
      "gilesthomas.com": "OK",
      "minimaxir.com": "OK",
      "wheresyoured.at": "OK",
      "blog.jim-nielsen.com": "OK",
      "joanwestenberg.com": "OK",
      "dfarq.homeip.net": "OK",
      "borretti.me": "OK",
      "geoffreylitt.com": "OK",
      "bogdanthegeek.github.io": "OK",
      "susam.net": "OK",
      "jyn.dev": "OK",
      "eli.thegreenplace.net": "OK",
      "fabiensanglard.net": "OK",
      "gwern.net": "OK",
      "downtowndougbrown.com": "OK",
      "abortretry.fail": "OK",
      "chadnauseam.com": "OK",
      "brutecat.com": "OK",
      "filfre.net": "OK",
      "it-notes.dragas.net": "OK",
      "oldvcr.blogspot.com": "OK",
      "beej.us": "OK",
      "hey.paris": "OK",
      "refactoringenglish.com": "OK",
      "simone.org": "OK",
      "danielwirtz.com": "OK",
      "hugotunius.se": "OK",
      "philiplaine.com": "OK",
      "berthub.eu": "OK",
      "troyhunt.com": "OK",
      "herman.bearblog.dev": "OK",
      "matduggan.com": "OK",
      "tomrenner.com": "OK",
      "danieldelaney.net": "OK",
      "danielchasehooper.com": "OK",
      "martinalderson.com": "OK",
      "steveblank.com": "OK",
      "dwarkesh.com": "OK",
      "blog.pixelmelt.dev": "OK",
      "chiark.greenend.org.uk/~sgtatham": "OK",
      "worksonmymachine.substack.com": "OK",
      "experimental-history.com": "OK",
      "aresluna.org": "OK",
      "keygen.sh": "OK",
      "bernsteinbear.com": "OK",
      "miguelgrinberg.com": "OK",
      "anildash.com": "OK",
      "computer.rip": "OK",
      "mjg59.dreamwidth.org": "OK",
      "michael.stapelberg.ch": "OK",
      "grantslatton.com": "OK",
      "hackernews": "OK",
      "utcc.utoronto.ca/~cks": "HTTPSConnectionPool(host='utcc.utoronto.ca', port=443): Read timed out. (read timeout=20)",
      "rachelbythebay.com": "HTTPSConnectionPool(host='rachelbythebay.com', port=443): Read timed out. (read timeout=20)",
      "tedunangst.com": "HTTPSConnectionPool(host='www.tedunangst.com', port=443): Read timed out. (read timeout=20)"
    },
    "count": 299
  },
  "items": [
    {
      "title": "mitchellh.com: My AI Adoption Journey",
      "url": "https://mitchellh.com/writing/my-ai-adoption-journey",
      "source": "mitchellh.com",
      "time": "2026-02-20T10:18:07.324163",
      "raw_desc": ""
    },
    {
      "title": "mitchellh.com: Don't Trip[wire] Yourself: Testing Error Recovery in Zig",
      "url": "https://mitchellh.com/writing/tripwire",
      "source": "mitchellh.com",
      "time": "2026-02-20T10:18:07.324163",
      "raw_desc": ""
    },
    {
      "title": "mitchellh.com: Finding and Fixing Ghostty's Largest Memory Leak",
      "url": "https://mitchellh.com/writing/ghostty-memory-leak-fix",
      "source": "mitchellh.com",
      "time": "2026-02-20T10:18:07.324163",
      "raw_desc": ""
    },
    {
      "title": "Gemini çªç„¶åˆ é™¤äº†æˆ‘æ‰€æœ‰çš„ conversation history",
      "url": "https://www.v2ex.com/t/1193217",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "çœŸçš„æ˜¯åŒªå¤·æ‰€æ€ï¼  è¿™æ ·çš„å¤§å…¬å¸åšå‡ºæ¥çš„äº§å“å±…ç„¶è¿è¿™ä¹ˆåŸºæœ¬çš„äº‹æƒ…éƒ½èƒ½æç ¸ï¼ æˆ‘æ•°åæ¡å¯¹è¯è®°å½•ä¸€å¤œä¹‹é—´å°±çªç„¶æ¶ˆå¤±äº†ï¼ \r\n\r\nç½‘ä¸Šä¹Ÿæœ‰è®¸å¤šç›¸åŒçš„é­é‡çš„è®°å½•ï¼Œ å¤§å®¶éƒ½æ„¤æ€’ä½†æ˜¯æ¯«æ— åŠæ³•ã€‚  Google è‡³ä»Šæ²¡æœ‰ç»™åˆç†è§£é‡Šå’Œè§£å†³æ–¹æ¡ˆã€‚\r\n\r\nè®©æˆ‘æƒ³èµ·ä¹‹å‰çš„ Google read, Google+ ç­‰ç­‰ä¼—å¤šäº§å“ã€‚ è¿™çœŸæ˜¯ä¸€ä¸ªå¯æ€•è€Œç³Ÿç³•çš„å…¬å¸"
    },
    {
      "title": "å„ä½ç°åœ¨è¿˜åœ¨ç”¨ GPT è¿˜æ˜¯è½¬å‘åˆ«çš„ LLM",
      "url": "https://www.v2ex.com/t/1193220",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "ç°åœ¨æ¶‰åŠä»·å€¼åˆ¤æ–­å’Œç¤¾ä¼šç§‘å­¦çš„é—®é¢˜æˆ‘éƒ½ä¸é—® GPT äº†ï¼Œå› ä¸ºå®ƒä¼šä¸€ç›´ç«¯æ°´ã€å’Œç¨€æ³¥ã€æ‰®å¥½äººï¼Œæƒ³æ•™è‚²æˆ‘çš„ä»·å€¼è§‚ã€‚æŒ‡è´£ä»–é”™ä»–è¿˜ä¼šå…ˆç‹¡è¾©ï¼Œç„¶åç»§ç»­è£…åœ£æ¯è¾“å‡ºé“å¾·æ­£ç¡®çš„åºŸè¯ã€‚Grok æ„Ÿè§‰è¿‡äºåˆ»æ„ä¿çš®ï¼Œä¸€å®šè¦å˜»å˜»å“ˆå“ˆï¼Œæœ‰æ•ˆä¿¡æ¯å¯†åº¦å¾ˆä½ã€‚Gemini ç¨å¾®å¹³è¡¡ç‚¹"
    },
    {
      "title": "ç³»ç»Ÿé€¼ç€ç”¨æˆ·å‡çº§åˆ°åº•æ˜¯åˆç†çš„è¿˜æ˜¯ä¸åˆç†çš„",
      "url": "https://www.v2ex.com/t/1193247",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "å› ä¸ºçœ‹ä¸æƒ¯å½“å‰æ‰€è°“ç»ç’ƒè´¨æ„Ÿçš„å®¡ç¾ï¼Œæ‰€ä»¥ä¸€ç›´æ²¡æœ‰å‡çº§ MacOS26 ã€‚\r\n\r\nå¹¶ä¸”æŒ‰ç…§ v å‹æ•™çš„å·²ç»æŠŠæ›´æ–°é“¾æ¥é‡ç½®ä¸º www.example.com äº†ã€‚\r\n\r\nä½†æ˜¯è‹¹æœç³»ç»Ÿè¿˜æ˜¯æ¯éš”ä¸€æ®µæ—¶é—´æé†’æˆ‘ä¸€æ¬¡ install tonight ã€‚"
    },
    {
      "title": "ä¹¦æˆ¿æ¡Œå­å°ï¼Œ Studio Display æƒ³æ¥ä¸¤å°ä¸»æœºï¼ˆMac Mini + Win 4090ï¼‰ï¼Œæ±‚æ–¹æ¡ˆ",
      "url": "https://www.v2ex.com/t/1193238",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "èƒŒæ™¯\r\nä¹¦æˆ¿æ¡Œå­æ¯”è¾ƒå°ï¼Œåªèƒ½æ”¾ä¸€å°æ˜¾ç¤ºå™¨ã€‚ç›®å‰åœ¨ç”¨ Apple Studio Display ï¼Œä¸»è¦æ­é… Mac Mini ä½¿ç”¨ï¼Œæ—¥å¸¸åŠå…¬æ•ˆç‡å¾ˆé«˜ï¼Œå·²ç»å¾ˆä¹ æƒ¯è¿™å¥—ç»„åˆäº†ã€‚\r\nå¦å¤–è¿˜æœ‰ä¸€å° Windows ä¸»æœºï¼Œé…ç½®æ˜¯ RTX 4090 + åç¡• ProArt æ˜¾å¡ï¼ˆå¸¦ Type-C æ¥å£ï¼‰ï¼Œä¸»è¦ç”¨æ¥æ‰“æ¸¸æˆå’Œè·‘ 4090 çš„ç®—åŠ›ã€‚\r\nç°åœ¨çš„ç—›ç‚¹æ˜¯ï¼šä¸¤å°ä¸»æœºæƒ³å…±ç”¨è¿™ä¸€å° Studio Display ï¼Œæ‰¾ä¸åˆ°å¥½çš„åˆ‡æ¢æ–¹æ¡ˆã€‚\r\nå·²ç»å°è¯•/äº†è§£è¿‡çš„æ–¹æ¡ˆ\r\n1. ä¸²æµï¼ˆ Moonlight / Parsec ç­‰ï¼‰\r\nè¯•äº†ä¸€ä¸‹ï¼Œä½“éªŒä¸å¤ªè¡Œï¼Œæœ‰æ˜æ˜¾å»¶è¿Ÿå’Œç”»è´¨æŸå¤±ï¼Œå°¤å…¶æ‰“æ¸¸æˆçš„æ—¶å€™æ„ŸçŸ¥å¾ˆå¼ºï¼Œæš‚æ—¶æ”¾å¼ƒäº†ã€‚\r\n2. KVM åˆ‡æ¢å™¨\r\nç½‘ä¸Šæœäº†ä¸€åœˆï¼Œæ”¯æŒ Studio Display ï¼ˆ Thunderbolt æ¥å£ï¼‰çš„ KVM å¥½åƒç›®å‰åªæœ‰ä¸€æ¬¾ï¼Œä»·æ ¼è¦ 2000 å¤šï¼Œè€Œä¸”ä¸ç¡®å®šå®é™…ä½“éªŒå¦‚ä½•ï¼Œæœ‰ç‚¹çŠ¹è±«ã€‚\r\n3. æ¥å›æ‹”æ’çº¿ç¼†\r\nç†è®ºä¸Šå¯è¡Œï¼Œä½† Studio Display èƒŒåçš„ Thunderbolt å£ä¸å¤ªå¥½å¤Ÿåˆ°ï¼Œæ¯æ¬¡æ‹”æ’å¤ªéº»çƒ¦äº†ï¼Œè€Œä¸”é¢‘ç¹æ‹”æ’æ€•ä¼¤æ¥å£ï¼Œä¸æ˜¯é•¿ä¹…ä¹‹è®¡ã€‚\r\næˆ‘çš„éœ€æ±‚\r\n\r\nå¹³æ—¶ 80% æ—¶"
    },
    {
      "title": "åŒçº§åˆ«å¹´é™å·®ä¸å¤šçš„æµ‹è¯• QA è–ªèµ„æ¯”å¼€å‘é«˜ï¼Œä½ è§‰å¾—åˆç†å—",
      "url": "https://www.v2ex.com/t/1193216",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "ä»¥æˆ‘ä»¬å…¬å¸å½“å‰ç ”å‘å›¢é˜Ÿä¸ºä¾‹ï¼Œåœ¨åŒçº§åˆ«ã€å¹´é™å·®ä¸å¤šçš„æƒ…å†µä¸‹ï¼Œç¡®å®å­˜åœ¨æµ‹è¯• QA è–ªèµ„é«˜äºå¼€å‘çš„æƒ…å†µã€‚\r\n\r\nA åŒäº‹ï¼Œåç«¯å¼€å‘å·¥ç¨‹å¸ˆ\r\n\r\nå…¥èŒ 4 å¹´\r\nèŒçº§ï¼šP6\r\næœˆè–ªï¼š25K\r\n\r\nB åŒäº‹ï¼Œæµ‹è¯• QA å·¥ç¨‹å¸ˆ\r\nå…¥èŒ 4 å¹´\r\nèŒçº§ï¼šP6\r\næœˆè–ªï¼š28K\r\n\r\nä»è¡¨é¢çœ‹ï¼Œä¸¤äººå¹´é™ç›¸å½“ã€èŒçº§ä¸€è‡´ï¼Œä½† QA çš„è–ªèµ„æ˜æ˜¾é«˜äºå¼€å‘ã€‚   ä½ è§‰å¾—åˆç†å—"
    },
    {
      "title": "æœ‰æ²¡æœ‰å…è´¹æ˜“ç”¨çš„ md ç¼–è¾‘å™¨æ¨èä¸€ä¸‹",
      "url": "https://www.v2ex.com/t/1193229",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "æœ€è¿‘åœ¨å­¦ä¹ ç å­—ï¼Œ\r\nç”¨ wps æ€»è§‰å¾—å¤ªé‡ï¼Œ\r\nå°è±¡ç¬”è®°å’Œæœ‰é“æ¯”è¾ƒæœ‰ç‚¹å¡ï¼Œ\r\næ¯”è¾ƒä¹ æƒ¯ md çš„ç å­—æ–¹å¼ï¼Œ\r\næƒ³è¦æ‰¾ä¸€ä¸ªå…è´¹æ˜“ç”¨çš„ md ç¼–è¾‘å™¨ï¼Œ\r\næ²¡æœ‰å¼ºçƒˆçš„è”ç½‘è¦æ±‚ã€‚\r\n\r\nä¹‹å‰ç”¨è¿‡ typora ï¼Œ\r\néå¸¸å–œæ¬¢è¿™ç§æç®€é£æ ¼çš„ç¼–è¾‘å™¨ï¼Œ\r\nç½‘ä¸Šè¯´çš„å…è´¹ç‰ˆæœ¬ç›®å‰å®˜ç½‘å·²ç»æ‰¾ä¸åˆ°ï¼Œ\r\nä¸”ä¸ç¡®å®šæ˜¯å¦æ”¯æŒ mac arm ã€‚\r\n\r\nçœ‹è¿‡ notion ï¼Œobsdian ï¼Œéƒ½ä¸å¤§è¡Œã€‚\r\n\r\næ±‚å„ä½å¤§ä½¬æ¨èä¸€ä¸ªç®€å•çš„ md ç¼–è¾‘å™¨ï¼Œ\r\nçº¯ç²¹çš„ md ç¼–è¾‘å™¨ï¼Œ\r\nä¸éœ€è¦èŠ±é‡Œèƒ¡å“¨çš„å…¶ä»–åŠŸèƒ½ã€‚"
    },
    {
      "title": "æ‰‹æœºçš„å®šä½å¥½åç«Ÿç„¶å’Œ SIM å¡å‡ ä¹æ²¡æœ‰å…³ç³»",
      "url": "https://www.v2ex.com/t/1193213",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "### é—®é¢˜æè¿°\r\nop ä½¿ç”¨çš„æ˜¯å»å¹´å‘å¸ƒçš„ iPhone 17 pro, SIM å¡æ˜¯ä¸­å›½è”é€š\r\n\r\nä»ä¸Šä¸€ä»£ iPhone 13 è¿­ä»£ä¸Šæ¥, SIM æ²¡æœ‰æ›´æ¢, å‘ç° 17 çš„å®šä½ä¼¼ä¹æ¯” 13 è¿˜ä¸é è°±\r\n\r\n#### é—®é¢˜è¡¨ç°\r\n1. ä½¿ç”¨ keep åœ¨åŸå¸‚çš„å…¬å›­(é™„è¿‘æœ‰é«˜æ¥¼)ç»•åœˆè·‘æ­¥, è¿åŠ¨è½¨è¿¹å‡ºç°å¾ˆä¹±çš„æ¦‚ç‡å˜å¤šäº†\r\n2. æ›´æ¢äº†ä¸€å¤©æœ‹å‹çš„ç”µä¿¡å¡, åœ¨åŒæ ·çš„å…¬å›­ç»•åœˆè·‘æ­¥, è½¨è¿¹æ··ä¹±çš„ç¨‹åº¦å¾ˆå°‘. ä½†æµ‹è¯•æ ·æœ¬åªæœ‰è¿™ä¸€æ¬¡, æ— æ³•ç¡®è®¤æ˜¯å¦ä¸º SIM çš„å¢ç›Š\r\n3. ä½¿ç”¨ç™¾åº¦åœ°å›¾å¯¼èˆªæ—¶, äººè¿˜åœ¨åŸåœ°å°±æç¤ºå·²ç»åèˆª, ä¸”æ¬¡æ•°ä¸å°‘. è¿™æˆ‘ç†è§£ä¸ºå®šä½æ¼‚ç§», å¯¼è‡´äº†åœ°å›¾æç¤ºåèˆª\r\n\r\n### æ€è€ƒ\r\nä¸€ç›´ä»¥ä¸ºæ˜¯ä¸­å›½è”é€šçš„é—®é¢˜, æ¯•ç«Ÿä¸‰å¤§è¿è¥å•†é‡Œé¢å®ƒçš„ä¿¡å·æœ€å·®äº†\r\n>å½“ç„¶æˆ‘ä¹ŸçŸ¥é“ iPhone æœ¬èº«ä¹Ÿæœ‰é—®é¢˜, ä½†æ˜¾ç„¶æ›´æ¢ SIM å¡çš„æˆæœ¬æ›´ä½\r\n\r\næƒ³èµ·æ¥æœ‰ä¸€äº›æ‰‹è¡¨æ˜¯æœ‰ GPS åŠŸèƒ½çš„, è€Œä¸”å®ƒä»¬ä¹Ÿä¸éœ€è¦æ’å…¥ SIM å¡, é‚£ä¹ˆå°±è¯´æ˜ GPS ä¸éœ€è¦ SIM å¡çš„å¸®åŠ©ä¹Ÿèƒ½è¿›è¡Œ\r\n\r\né—®äº†ä¸€ä¸‹ ai. æœç„¶, SIM å¡çš„å®šä½ç²¾åº¦æ›´å·®, æˆ‘çš„è·‘æ­¥è·¯çº¿è™½ç„¶å¾ˆä¹±, ä½†ç¡®ç¡®å®å®æ²¡æœ‰å‡ºç° 10m è¿™ä¹ˆå¤§çš„è¯¯"
    },
    {
      "title": "[åæ§½] é›¶åŸºç¡€é  AI æ“äº†ä¸ªä½³èƒ½æ‰“å°å°ç¨‹åºï¼Œè‡ªå·±è¿˜æ²¡ç”¨çƒ­ä¹å°±è¢«å¾®ä¿¡æ°¸å°äº†...",
      "url": "https://www.v2ex.com/t/1193256",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "æœ€è¿‘å…¥äº†ä¸€å°ä½³èƒ½çš„ mf667Cdw ä¸€ä½“æœºã€‚å› ä¸ºæ˜¯æ°´è´§ï¼Œæ²¡æ³•æ¥å…¥ä½³èƒ½å›½å†…å®˜æ–¹çš„å¾®ä¿¡å°ç¨‹åºï¼Œåªèƒ½ç¡¬ç€å¤´çš®ç”¨å®˜æ–¹ App ã€‚\r\nä½†å®˜æ–¹ App ä½“éªŒå®åœ¨ä¸€è¨€éš¾å°½ï¼š\r\n1. ç½‘ç»œå—é™ï¼šåªæ”¯æŒå±€åŸŸç½‘æ‰“å°ï¼Œæƒ³è¿œç¨‹çš„è¯å¾—ç»•é“å„ç§äº‘æœåŠ¡ï¼Œå›½å†…çš„ç½‘ç»œç¯å¢ƒç”¨èµ·æ¥æå…¶ä¸å‹å¥½ã€‚\r\n2. æ’ç‰ˆ Bugï¼šæ‰“å°é¢„è§ˆæœ‰ç„å­¦é—®é¢˜ï¼Œæ˜æ˜ 5 é¡µçš„æ–‡æ¡£ï¼Œé¢„è§ˆå‡ºæ¥ç¡¬ç”Ÿç”Ÿåªæœ‰ 4 é¡µï¼Œæ ¹æœ¬ä¸æ•¢æ”¾å¿ƒæ‰“ã€‚\r\nä¸ºäº†è§£å†³è¿™ä¸ªç—›ç‚¹ï¼Œå°±æƒ³ç€è‡ªå·±åŠ¨æ‰‹ä¸°è¡£è¶³é£Ÿã€‚åœ¨ Gemini çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘å°è¯•å¼€å‘äº†ä¸€ä¸ªå¾®ä¿¡å°ç¨‹åºã€‚\r\næˆ‘æœ¬äººæ˜¯å®Œå…¨æ²¡æœ‰å¼€å‘ç»éªŒçš„çº¯å°ç™½ï¼Œå…¨é ç€ AI ä¸€æ­¥æ­¥æŒ‡å¯¼å®ç°å„ç§åŠŸèƒ½ã€‚æœŸé—´ç»å†äº†æ— æ•°æ¬¡æ¯ç‡¥çš„ debug ã€‚å½“ä»£ç ç»ˆäºè·‘é€šï¼Œæäº¤å®¡æ ¸å¹¶ä¸”é¡ºåˆ©å‘å¸ƒçš„é‚£ä¸€åˆ»ï¼Œé‚£ç§æˆå°±æ„Ÿå’Œå…´å¥‹æ„ŸçœŸçš„æ˜¯æ‹‰æ»¡äº†ã€‚\r\nç»“æœè¿˜æ²¡é«˜å…´å¤šä¹…ï¼Œå¾®ä¿¡çªç„¶æç¤ºæˆ‘â€œæ¶æ„æ³¨å†Œâ€ï¼Œç›´æ¥æŠŠå°ç¨‹åºç»™æ°¸å°äº†ã€‚\r\nè¯´å®è¯çœŸçš„æŒºå”å˜˜çš„ã€‚è¿™ä¸ªå°ç¨‹åºï¼š\r\nâ€¢ ä»å¤´åˆ°å°¾å°±æˆ‘è‡ªå·±ä¸€ä¸ªäººåœ¨ç”¨ã€‚\r\nâ€¢ å› ä¸ºæ²¡æœ‰ç»è¿‡è®¤è¯ï¼Œå®ƒæ ¹æœ¬ä¸ä¼šè¢«å…¬å¼€æœç´¢åˆ°ã€‚\r\nâ€¢ è¢«å°ç¦åï¼Œè¿ç«™å†…ç§ä¿¡é€šé“éƒ½æ‰“ä¸å¼€äº†ï¼Œæ­»å¾—ä¸æ˜ä¸ç™½ï¼Œè¿è‡ªå·±åˆ°åº•è§¦çŠ¯äº†å“ªæ¡è§„åˆ™éƒ½ä¸çŸ¥é“ã€‚\r\nåæ¥æˆ‘åˆåœ¨å¾®ä¿¡ä¸Šæœäº†ä¸€ä¸‹åŒç±»çš„å°ç¨‹åºï¼Œå‘ç°å¾ˆå¤šå…·å¤‡"
    },
    {
      "title": "è¢«è£å‘˜ç©äº†å¿«ä¸€å¹´äº†",
      "url": "https://www.v2ex.com/t/1193233",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "å½“æ—¶è¿˜åªæ˜¯ codepilot è¾…åŠ©ç¼–ç¨‹ï¼Œç°åœ¨ ai ç¼–ç¨‹è¿™ä¹ˆå¹¿æ³›å…ˆè¿›ï¼Œå¹´åå‡†å¤‡æ‰¾å·¥ä½œäº†ï¼Œæƒ³çŸ¥é“æœ€è¿‘æ‰¾å·¥ä½œçš„æœ‹å‹ï¼Œç°åœ¨é¢è¯•è¿˜æ˜¯è¦åˆ·é¢˜å’ŒèƒŒå…«è‚¡å—ï¼Ÿé¢è¯•æœ‰ä»€ä¹ˆæ–°çš„å˜åŒ–å¯ä»¥å‡†å¤‡çš„å—"
    },
    {
      "title": "ç¾å›¢æ€»æ˜¯åˆ›å»ºæ¡Œé¢å¹¿å‘Š",
      "url": "https://www.v2ex.com/t/1193239",
      "source": "V2EX",
      "time": "2026-02-20T10:18:07.405146",
      "raw_desc": "https://i.imgur.com/1v1UuqL_d.webp?maxwidth=760&fidelity=grand\r\n\r\nå·¦ä¸Šè§’çš„ç¾å›¢å°è¯´æ˜¯ä»€ä¹ˆä¸œè¥¿\r\n\r\nä¸€åŠ æ‰‹æœºï¼Œå·²ç»åœ¨ç³»ç»Ÿé‡Œå…³é—­å®ƒçš„æ‰€æœ‰æƒé™äº†\r\n\r\nç³»ç»Ÿçš„å°å¸ƒä»€ä¹ˆçš„ä¹±ä¸ƒå…«ç³Ÿçš„éƒ½å…³é—­äº†\r\n\r\nä½†æ˜¯æ‰“å¼€ç¾å›¢è¿˜æ˜¯æ—¶ä¸æ—¶åˆ›å»ºå‡ºæ¥"
    },
    {
      "title": "ericmigi.com: February Pebble Production and Software Updates",
      "url": "https://repebble.com/blog/february-pebble-production-and-software-updates",
      "source": "ericmigi.com",
      "time": "2026-02-20T10:18:07.479984",
      "raw_desc": ""
    },
    {
      "title": "ericmigi.com: On Being A Canadian In America In 2026",
      "url": "https://ericmigi.com/blog/on-being-a-canadian-in-america-in-2026",
      "source": "ericmigi.com",
      "time": "2026-02-20T10:18:07.479984",
      "raw_desc": ""
    },
    {
      "title": "ericmigi.com: Pebble Round 2 - The Most Stylish Pebble Ever",
      "url": "https://repebble.com/blog/pebble-round-2-the-most-stylish-pebble-ever",
      "source": "ericmigi.com",
      "time": "2026-02-20T10:18:07.479984",
      "raw_desc": ""
    },
    {
      "title": "daringfireball.net: IMAX and Apple Collaborate to Screen F1 Races Live in Theaters",
      "url": "https://www.motorsport.com/f1/news/f1-to-screen-live-in-imax-theatres-in-2026-as-apple-tv-unveils-new-us-viewing-experience/10798974/",
      "source": "daringfireball.net",
      "time": "2026-02-20T10:18:07.556720",
      "raw_desc": "Lydia Mee, reporting for Motorsport:\n\nIMAX has announced that a select number of races will be shown\nlive in IMAX locations across the United States in 2026. The new\nfan viewing experience is part of a collaboration with Apple TV,\nwhich has taken over the broadcasting rights for the championship\nin the US on a multi-year deal from 2026.\nâ€œF1 is a rapidly growing force in sports and culture in the US,\nand by bringing F1 on Apple TV live to IMAX theatres nationwide,\nweâ€™re delivering the energy and excitement to even more screens in\na truly immersive way,â€ said Oliver Schusser, Appleâ€™s vice\npresident of music, sports, and Beats.\n\nYou know what would add even more screens in an immersive way? If Vision Pro users had access to the same live screenings on virtual IMAX screens.\n\nÂ â˜…"
    },
    {
      "title": "daringfireball.net: One More Spitball Idea for Appleâ€™s March 4 Media Event â€˜Experienceâ€™: Immersive F1 on Vision Pro?",
      "url": "https://www.formula1.com/en/latest/article/official-grand-prix-start-times-for-2026-f1-season-confirmed.2UgPfArqH76tzlOYh21jSG",
      "source": "daringfireball.net",
      "time": "2026-02-20T10:18:07.557241",
      "raw_desc": "A reader pointed out that the 2026 Formula 1 season starts in Australia on March 8. You will recall from October that Apple TV is now the exclusive broadcast partner for F1 in the U.S. Apple is already dabbling with live immersive sports broadcasting for VisionOS with a limited slate of Lakers games this season. If they have something planned for streaming F1 races live on Vision Pro, with some level of immersion, March 4 would be a pretty good date to demo that experience to the media.\nIt doesnâ€™t even have to be live race coverage. Technically thatâ€™s probably impossible for this season. It would just be a sign of confidence and interest in the platform long-term merely to see some sort of immersive component to F1 on Apple TV, even if itâ€™s not live. Like â€œride the trackâ€ to experience the turns and elevation changes.\nCould just be a total coincidence that the Formula 1 season is starting the weekend after this event. But it seems worth noting.\n\nÂ â˜…"
    },
    {
      "title": "daringfireball.net: Paul Ford: â€˜The A.I. Disruption Has Arrived, and It Sure Is Funâ€™",
      "url": "https://www.nytimes.com/2026/02/18/opinion/ai-software.html?unlocked_article_code=1.NFA.djaw.TBlAp8kE_N-i",
      "source": "daringfireball.net",
      "time": "2026-02-20T10:18:07.557241",
      "raw_desc": "Paul Ford, in an op-ed for The New York Times (gift link):\n\nAll of the people I love hate this stuff, and all the people I hate love it. And yet, likely because of the same personality flaws that drew me to technology in the first place, I am annoyingly excited.\n\n\nÂ â˜…"
    },
    {
      "title": "jeffgeerling.com: Frigate with Hailo for object detection on a Raspberry Pi",
      "url": "https://www.jeffgeerling.com/blog/2026/frigate-with-hailo-for-object-detection-on-a-raspberry-pi/",
      "source": "jeffgeerling.com",
      "time": "2026-02-20T10:18:07.674932",
      "raw_desc": ""
    },
    {
      "title": "jeffgeerling.com: AI is destroying Open Source, and it's not even good yet",
      "url": "https://www.jeffgeerling.com/blog/2026/ai-is-destroying-open-source/",
      "source": "jeffgeerling.com",
      "time": "2026-02-20T10:18:07.674932",
      "raw_desc": ""
    },
    {
      "title": "jeffgeerling.com: Testing Reachy Mini - Hugging Face's Pi powered robot",
      "url": "https://www.jeffgeerling.com/blog/2026/testing-reachy-mini-hugging-face-robot/",
      "source": "jeffgeerling.com",
      "time": "2026-02-20T10:18:07.674932",
      "raw_desc": ""
    },
    {
      "title": "idiallo.com: Thinking Improves Thinking ",
      "url": "https://idiallo.com/blog/taking-our-mind-for-granted?src=feed",
      "source": "idiallo.com",
      "time": "2026-02-20T10:18:07.748895",
      "raw_desc": ""
    },
    {
      "title": "idiallo.com: Programming is free ",
      "url": "https://idiallo.com/blog/programming-tools-are-free?src=feed",
      "source": "idiallo.com",
      "time": "2026-02-20T10:18:07.748895",
      "raw_desc": ""
    },
    {
      "title": "idiallo.com: Factional Drift: How online discussion clusters into factions ",
      "url": "https://idiallo.com/blog/factional-drift-online?src=feed",
      "source": "idiallo.com",
      "time": "2026-02-20T10:18:07.748895",
      "raw_desc": ""
    },
    {
      "title": "devblogs.microsoft.com/oldnewthing: Exploring the signals the dialog manager uses for dismissing a dialog",
      "url": "https://devblogs.microsoft.com/oldnewthing/20260219-00/?p=112072",
      "source": "devblogs.microsoft.com/oldnewthing",
      "time": "2026-02-20T10:18:07.762653",
      "raw_desc": ""
    },
    {
      "title": "devblogs.microsoft.com/oldnewthing: Could WriteÂ­ProcessÂ­Memory be made faster by avoiding the intermediate buffer?",
      "url": "https://devblogs.microsoft.com/oldnewthing/20260218-00/?p=112069",
      "source": "devblogs.microsoft.com/oldnewthing",
      "time": "2026-02-20T10:18:07.762653",
      "raw_desc": ""
    },
    {
      "title": "devblogs.microsoft.com/oldnewthing: Microspeak: Escrow",
      "url": "https://devblogs.microsoft.com/oldnewthing/20260217-00/?p=112067",
      "source": "devblogs.microsoft.com/oldnewthing",
      "time": "2026-02-20T10:18:07.762653",
      "raw_desc": ""
    },
    {
      "title": "Product Hunt: AgentReady",
      "url": "https://www.producthunt.com/products/agentready-2",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.758432",
      "raw_desc": "Cut your AI token costs by 40-60% with one API call\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: Reloop",
      "url": "https://www.producthunt.com/products/reloop",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.759467",
      "raw_desc": "Create winning ads without prompts or skills\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: Mengram",
      "url": "https://www.producthunt.com/products/mengram",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.759775",
      "raw_desc": "AI memory API with 3 types: facts, events, and workflows\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: FF Designer",
      "url": "https://www.producthunt.com/products/flutterflow",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.759775",
      "raw_desc": "Generate beautiful UI designs that you can instantly edit\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: CertNode Reflex",
      "url": "https://www.producthunt.com/products/certnode-reflex",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.760307",
      "raw_desc": "AI-powered chargeback defense. Pay only when you win.\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: Monologue for iOS",
      "url": "https://www.producthunt.com/products/monologue-2",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.760307",
      "raw_desc": "Turn your voice into polished writingâ€”wherever you go.\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: Origami.chat",
      "url": "https://www.producthunt.com/products/origami-chat",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.760833",
      "raw_desc": "Find your perfect leads with one prompt\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: Decks For Good",
      "url": "https://www.producthunt.com/products/decks-for-good",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.760833",
      "raw_desc": "Get fundraising advice when you give to charity\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: Feedix",
      "url": "https://www.producthunt.com/products/feedix",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.761364",
      "raw_desc": "Your YouTube subscriptions, summarized and delivered by AI\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "Product Hunt: HyperCaps for macOS",
      "url": "https://www.producthunt.com/products/hypercaps-for-macos",
      "source": "Product Hunt",
      "time": "2026-02-20T10:18:07.761364",
      "raw_desc": "Turn CapsLock into a Super Key & Boost Productivity\n          \n\nDiscussion\n            |\n            Link"
    },
    {
      "title": "lcamtuf.substack.com: Unreal numbers",
      "url": "https://lcamtuf.substack.com/p/unreal-numbers",
      "source": "lcamtuf.substack.com",
      "time": "2026-02-20T10:18:07.783948",
      "raw_desc": ""
    },
    {
      "title": "lcamtuf.substack.com: It's all a blur",
      "url": "https://lcamtuf.substack.com/p/its-all-a-blur",
      "source": "lcamtuf.substack.com",
      "time": "2026-02-20T10:18:07.783948",
      "raw_desc": ""
    },
    {
      "title": "lcamtuf.substack.com: You gotta think outside the hypercube",
      "url": "https://lcamtuf.substack.com/p/you-gotta-think-outside-the-hypercube",
      "source": "lcamtuf.substack.com",
      "time": "2026-02-20T10:18:07.783948",
      "raw_desc": ""
    },
    {
      "title": "simonwillison.net: Gemini 3.1 Pro",
      "url": "https://simonwillison.net/2026/Feb/19/gemini-31-pro/#atom-everything",
      "source": "simonwillison.net",
      "time": "2026-02-20T10:18:07.838831",
      "raw_desc": ""
    },
    {
      "title": "simonwillison.net: Experimenting with sponsorship for my blog and newsletter",
      "url": "https://simonwillison.net/2026/Feb/19/sponsorship/#atom-everything",
      "source": "simonwillison.net",
      "time": "2026-02-20T10:18:07.838831",
      "raw_desc": ""
    },
    {
      "title": "simonwillison.net: SWE-bench February 2026 leaderboard update",
      "url": "https://simonwillison.net/2026/Feb/19/swe-bench/#atom-everything",
      "source": "simonwillison.net",
      "time": "2026-02-20T10:18:07.838831",
      "raw_desc": ""
    },
    {
      "title": "seangoedecke.com: LLM-generated skills work, if you generate them afterwards",
      "url": "https://seangoedecke.com/generate-skills-afterwards/",
      "source": "seangoedecke.com",
      "time": "2026-02-20T10:18:07.890643",
      "raw_desc": ""
    },
    {
      "title": "seangoedecke.com: Two different tricks for fast LLM inference",
      "url": "https://seangoedecke.com/fast-llm-inference/",
      "source": "seangoedecke.com",
      "time": "2026-02-20T10:18:07.890643",
      "raw_desc": ""
    },
    {
      "title": "seangoedecke.com: On screwing up",
      "url": "https://seangoedecke.com/screwing-up/",
      "source": "seangoedecke.com",
      "time": "2026-02-20T10:18:07.890643",
      "raw_desc": ""
    },
    {
      "title": "krebsonsecurity.com: Kimwolf Botnet Swamps Anonymity Network I2P",
      "url": "https://krebsonsecurity.com/2026/02/kimwolf-botnet-swamps-anonymity-network-i2p/",
      "source": "krebsonsecurity.com",
      "time": "2026-02-20T10:18:07.977108",
      "raw_desc": ""
    },
    {
      "title": "krebsonsecurity.com: Patch Tuesday, February 2026 Edition",
      "url": "https://krebsonsecurity.com/2026/02/patch-tuesday-february-2026-edition/",
      "source": "krebsonsecurity.com",
      "time": "2026-02-20T10:18:07.977108",
      "raw_desc": ""
    },
    {
      "title": "krebsonsecurity.com: Please Donâ€™t Feed the Scattered Lapsus ShinyHunters",
      "url": "https://krebsonsecurity.com/2026/02/please-dont-feed-the-scattered-lapsus-shiny-hunters/",
      "source": "krebsonsecurity.com",
      "time": "2026-02-20T10:18:07.977108",
      "raw_desc": ""
    },
    {
      "title": "shkspr.mobi: AI is a NAND Maximiser",
      "url": "https://shkspr.mobi/blog/2026/02/ai-is-a-nand-maximiser/",
      "source": "shkspr.mobi",
      "time": "2026-02-20T10:18:08.104066",
      "raw_desc": ""
    },
    {
      "title": "shkspr.mobi: Book Review: All Systems Red - The Murderbot Diaries by Martha Wells â˜…â˜…â¯ªâ˜†â˜†",
      "url": "https://shkspr.mobi/blog/2026/02/book-review-all-systems-red-the-murderbot-diaries-by-martha-wells/",
      "source": "shkspr.mobi",
      "time": "2026-02-20T10:18:08.104066",
      "raw_desc": ""
    },
    {
      "title": "shkspr.mobi: Gadget Review: Epomaker Split 70 Mechanical Keyboard â˜…â˜…â˜…â˜…â¯ª",
      "url": "https://shkspr.mobi/blog/2026/02/gadget-review-epomaker-split-70-mechanical-keyboard/",
      "source": "shkspr.mobi",
      "time": "2026-02-20T10:18:08.104066",
      "raw_desc": ""
    },
    {
      "title": "maurycyz.com: Inside an alpha-beta scintillator:",
      "url": "https://maurycyz.com/misc/ah_teardown/",
      "source": "maurycyz.com",
      "time": "2026-02-20T10:18:08.140326",
      "raw_desc": ""
    },
    {
      "title": "maurycyz.com: Notes on blog future-proofing",
      "url": "https://maurycyz.com/misc/futureproofing/",
      "source": "maurycyz.com",
      "time": "2026-02-20T10:18:08.140326",
      "raw_desc": ""
    },
    {
      "title": "maurycyz.com: Writing my own static site generator",
      "url": "https://maurycyz.com/misc/new_ssg/",
      "source": "maurycyz.com",
      "time": "2026-02-20T10:18:08.140326",
      "raw_desc": ""
    },
    {
      "title": "pluralistic.net: Pluralistic: Six Years of Pluralistic (19 Feb 2026)",
      "url": "https://pluralistic.net/2026/02/19/now-we-are-six/",
      "source": "pluralistic.net",
      "time": "2026-02-20T10:18:08.315147",
      "raw_desc": ""
    },
    {
      "title": "pluralistic.net: Pluralistic: What's a \"gig work minimum wage\" (17 Feb 2026)",
      "url": "https://pluralistic.net/2026/02/17/no-piecework/",
      "source": "pluralistic.net",
      "time": "2026-02-20T10:18:08.315147",
      "raw_desc": ""
    },
    {
      "title": "pluralistic.net: Pluralistic: The online community trilemma (16 Feb 2026)",
      "url": "https://pluralistic.net/2026/02/16/fast-good-cheap/",
      "source": "pluralistic.net",
      "time": "2026-02-20T10:18:08.315147",
      "raw_desc": ""
    },
    {
      "title": "obra /superpowers",
      "url": "https://github.com/obra/superpowers",
      "source": "GitHub",
      "time": "2026-02-20T10:18:08.711730",
      "raw_desc": "An agentic skills framework & software development methodology that works."
    },
    {
      "title": "RichardAtCT /claude-code-telegram",
      "url": "https://github.com/RichardAtCT/claude-code-telegram",
      "source": "GitHub",
      "time": "2026-02-20T10:18:08.711730",
      "raw_desc": "A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence."
    },
    {
      "title": "open-mercato /open-mercato",
      "url": "https://github.com/open-mercato/open-mercato",
      "source": "GitHub",
      "time": "2026-02-20T10:18:08.711730",
      "raw_desc": "AIâ€‘supportive CRM / ERP foundation framework â€” built to power R&D, new processes, operations, and growth. Itâ€™s modular, extensible, and designed for teams that want strong defaults with room to customize everything. Better than Django, Retool and other alternatives - and Enterprise Grade!"
    },
    {
      "title": "harvard-edge /cs249r_book",
      "url": "https://github.com/harvard-edge/cs249r_book",
      "source": "GitHub",
      "time": "2026-02-20T10:18:08.712837",
      "raw_desc": "Introduction to Machine Learning Systems"
    },
    {
      "title": "HailToDodongo /pyrite64",
      "url": "https://github.com/HailToDodongo/pyrite64",
      "source": "GitHub",
      "time": "2026-02-20T10:18:08.712837",
      "raw_desc": "N64 Game-Engine and Editor using libdragon & tiny3d"
    },
    {
      "title": "openclaw /openclaw",
      "url": "https://github.com/openclaw/openclaw",
      "source": "GitHub",
      "time": "2026-02-20T10:18:08.712837",
      "raw_desc": "Your own personal AI assistant. Any OS. Any Platform. The lobster way. ğŸ¦"
    },
    {
      "title": "freemocap /freemocap",
      "url": "https://github.com/freemocap/freemocap",
      "source": "GitHub",
      "time": "2026-02-20T10:18:08.712837",
      "raw_desc": "Free Motion Capture for Everyone ğŸ’€âœ¨"
    },
    {
      "title": "p-e-w /heretic",
      "url": "https://github.com/p-e-w/heretic",
      "source": "GitHub",
      "time": "2026-02-20T10:18:08.712837",
      "raw_desc": "Fully automatic censorship removal for language models"
    },
    {
      "title": "dynomight.net: Heritability of human life span is about 50% when heritability is redefined to be something different",
      "url": "https://dynomight.net/lifespan/",
      "source": "dynomight.net",
      "time": "2026-02-20T10:18:08.756402",
      "raw_desc": "How heritable is hair color? Well, if youâ€™re a redhead and you have an identical twin, they will definitely also be a redhead. But the age at which twins go gray seems to vary a bit based on lifestyle. And thereâ€™s some randomness in where melanocytes end up on your skull when youâ€™re an embryo. And your twin might dye their hair! So the correct answer is, some large number, but less than 100%.\nOK, but check this out: Say I redefine â€œhair colorâ€ to mean â€œhair color except ignoring epigenetic and embryonic stuff and pretending that no one ever goes gray or dyes their hair et ceteraâ€. Now, hair color is 100% heritable. Amazing, right?\nOrâ€”how heritable is IQ? The wise man answers, â€œSome number between 0% or 100%, itâ€™s not that important, please donâ€™t yell at me.â€ But whatever the number is, it depends on society. In our branch of the multiverse, some kids get private tutors and organic food and $20,000 summer camps, while other kids get dysfunctional schools and lead paint and summers spent drinking Pepsi and staring at glowing rectangles. These things surely have at least some impact on IQ.\nBut again, watch this: Say I redefine â€œIQâ€ to be â€œIQ in some hypothetical world where every kid got exactly the same school, nutrition, and parenting, so none of those non-genetic factors matter anymore.â€ Suddenly, the heritability of IQ is higher. Thrilling, right? So much science.\nIf you want to redefine stuff like thisâ€¦ thatâ€™s not wrong. I mean, heritability is a pretty arbitrary concept to start with. So if you prefer to talk about heritability in some other world instead of our actual world, who am I to judge?\nIncidentally, hereâ€™s a recent paper:\n\nI STRESS THAT THIS IS A PERFECTLY FINE PAPER. Iâ€™m picking on it mostly because it was published in Science, meaningâ€”like all Science papersâ€”it makes grand claims but is woefully vague about what those claims mean or what was actually done. Also, publishing in Science is morally wrong and/or makes me envious. So I thought Iâ€™d try to explain whatâ€™s happening.\nItâ€™s actually pretty simple. At least, now that Iâ€™ve spent several hours reading the paper and its appendix over and over again, Iâ€™ve now convinced myself that itâ€™s pretty simple. So, as a little pedagogical experiment, Iâ€™m going to try to explain the paper three times, with varying levels of detail.\nExplanation 1: The very extremely high level picture\nThe normal way to estimate the heritability of lifespan is using twin data. Depending on what dataset you use, this will give 23-35%. This paper built a mathematical model that tries to simulate how long people would live in a hypothetical world in which no one dies from any non-aging related cause, meaning no car accidents, no drug overdoses, no suicides, no murders, and no (non-age-related) infectious disease. On that simulated data, for simulated people in a hypothetical world, heritability was 46-57%.\nCommentary\nEveryone seems to be interpreting this paper as follows:\n\nAha! We thought the heritability of lifespan was 23-35%. But it turns out that itâ€™s around 50%. Now we know!\n\nI understand this. Clearly, when the editors at Science chose the title for this paper, their goal was to lead you to that conclusion. But this is not what the paper says. What it says is this:\n\nWe built a mathematical model of alternate universe in which nobody died from accidents, murder, drug overdoses, or infectious disease. In that model, heritability was about 50%.\n\nExplanation 2: The very high-level picture\nLetâ€™s start over. Hereâ€™s figure 2 from the paper.\n\nNormally, heritability is estimated from twin studies. The idea is that identical twins share 100% of their DNA, while fraternal twins share only 50%. So if some trait is more correlated among identical twins than among fraternal twins, that suggests DNA influences that trait. There are statistics that formalize this intuition. Given a dataset that records how long various identical and fraternal twins lived, these produce a heritability number.\nTwo such traditional estimates appear as black circles in the above figures. For the Danish twin cohort, lifespan is estimated to be 23% heritable. For the Swedish cohort, itâ€™s 35%.\nThis paper makes a â€œtwin simulatorâ€. Given historical data, they fit a mathematical model to simulate the lifespans of â€œnewâ€ twins. Then they compute heritability on this simulated data.\nWhy calculate heritability on simulated data instead of real data? Well, their mathematical model contains an â€œextrinsic mortalityâ€ parameter, which is supposed to reflect the chance of death due to all non-aging-related factors like accidents, murder, or infectious disease. They assume that the chance someone dies from any of this stuff is constant over people, constant over time, and that it accounts for almost all deaths for people aged between 15 and 40.\nThe point of building the simulator is that itâ€™s possible to change extrinsic mortality. Thatâ€™s whatâ€™s happening in the purple curves in the above figure. For a range of different extrinsic mortality parameters, they simulate datasets of twins. For each simulated dataset, they estimate heritability just like with a real dataset.\nNote that the purple curves above nearly hit the black circles. This means that if they run their simulator with extrinsic mortality set to match reality, they get heritability numbers that line up with what we get from real data. That suggests their mathematical model isnâ€™t totally insane.\nIf you decrease extrinsic mortality, then you decrease the non-genetic randomness in how long people live. So heritability goes up. Hence, the purple curves go up as you go to the left.\nIntermission: On Science\nMy explanation of this paper relies on some amount of guesswork. For whatever reason, Science has decided that papers should contain almost no math, even when the paper in question is about math. So Iâ€™m mostly working from an English description. But even that description isnâ€™t systematic. Thereâ€™s no place that clearly lays out all the things they did, in order. Instead, you get little hints, sort of randomly distributed throughout the paper. Thereâ€™s an appendix, which the paper confidently cites over and over. But if you actually read the appendix, itâ€™s just more disconnected explanations of random things except now with equations set in glorious Microsoft Word format.\nNow, in most journals, authors write everything. But Science has professional editors. Given that every single statistics-focused paper in Science seems to be like this, we probably shouldnâ€™t blame the authors of this one. (Other than for their decision to publish in Science in the first place.)\nI do wonder what those editors are doing, though. I mean, let me show you something. Hereâ€™s the first paragraph where they start to actually explain what they actually did, from the first page:\n\nSee that h(t,Î¸) at the end? What the hell is that, you ask? Thatâ€™s a good question, because it was never introduced before this and is never mentioned again. I guess itâ€™s just supposed to be f(t,Î¸), which is fine. (I yield to none in my production of typos.) But if paying journals ungodly amounts of money brought us to this, of what use are those journals?\nMoving onâ€¦\nExplanation 3: Also pretty high level, but as low as weâ€™re doing to go\nProbably most people donâ€™t need this much detail and should skip this section. For everyone else, letâ€™s start over one last time.\nThe â€œnormalâ€ way to estimate heritability is by looking at correlations between different kinds of twins. Intuitively, if the lifespans of identical twins are more correlated than the lifespans of fraternal twins, that suggests lifespan is heritable. And it turns out that one estimator for heritability is â€œtwice the difference between the correlation among identical twins and the correlation among fraternal twins, all raised together.â€ There are other similar estimators for other kinds of twins. These normally say lifespan is perhaps 20% and 35% heritable.\nThis paper created an equation to model the probability a given person will die at a given age. The parameters of the equation vary from person to person, reflecting that some of us have DNA that predisposes us to live longer than others. But the idea is that the chances of dying are fairly constant between the ages of 15 and 40, after which they start increasing.\nThis equation contains an â€œextrinsic mortalityâ€ parameter. This is meant to reflect the chance of death due to all non-aging related factors like accidents or murder, etc. They assume this is constant. (Constant with respect to people and constant over time.) Note that they donâ€™t actually look at any data on causes of death. They just add a constant risk of death thatâ€™s shared by all people at all ages to the equation, and then they call this â€œextrinsic mortalityâ€.\nNow remember, different people are supposed to have different parameters in their probability-of-death equations. To reflect this, they fit a Gaussian distribution (bell curve) to the parameters with the goal of making it fit with historical data. The idea is that if the distribution over parameters were too broad, you might get lots of people dying at 15 or living until 120, which would be wrong. If the distribution were too concentrated, then you might get everyone dying at 43, which would also be wrong. So they find a good distribution, one that makes the ages people die in simulation look like the ages people actually died in historical data.\nRight! So now they have:\n\nAn equation thatâ€™s supposed to reflect the probability a given person dies at a given age.\nA distribution over the parameters of that equation thatâ€™s supposed to produce population-wide death ages that look like those in real historical data.\n\nBefore moving on, I remind you of two things:\n\nThey assume their death equation entirely determines the probability someone will die in a given year.\nThey assume that the shape of someoneâ€™s death equation is entirely determined by genetics.\n\nThe event of a person dying at a given age is random. But the probability that this happens is assumed to be fixed and determined by genes and genes alone.\nNow they simulate different kinds of twins. To simulate identical twins, they just draw parameters from their parameter distribution, assign those parameters to two different people, and then let them randomly die according to their death equation. (Is this getting morbid?) To simulate fraternal twins, they do the same thing, except instead of giving the two twins identical parameters, they give them correlated parameters, to reflect that they share 50% of their DNA.\nHow exactly do they create those correlated parameters? They donâ€™t explain this in the paper, and theyâ€™re quite vague in the supplement. As far as I can tell they sample two sets of parameters from their parameter distribution such that the parameters are correlated at a level of 0.5.\nNow they have simulated twins. They can simulate them with different extrinsic mortality values. If they lower extrinsic mortality, heritability of lifespan goes up. If they lower it to zero, heritability goes up to around 50%.\nMore commentary\nAlmost all human traits are partly genetic and partly due to the environment and/or random. If you could change the world and reduce the amount of randomness, then of course heritability would go up. Thatâ€™s true for life expectancy just life for anything else. So whatâ€™s the point of this paper?\nThere is a point!\n\n\nSure, obviously heritability would be higher in a world without accidents or murder. We donâ€™t need a paper to know that. But how much higher? Itâ€™s impossible to say without modeling and simulating that other world.\n\n\nOur twin datasets are really old. Itâ€™s likely that non-aging-related deaths are lower now in the past, because we have better healthcare and so on. This means that the heritability of lifespan for people alive today may be larger than it was for the people in our twin datasets, some of whom were born in 1870. We wonâ€™t know for sure until weâ€™re all dead, but this paper gives us a way to guess.\n\n\nHave I mentioned that heritability depends on society? And that heritability changes when society changes? And that heritability is just a ratio and you should stop trying to make it be a non-ratio because only-ratio things cannot be non-ratios? This is a nice reminder.\n\n\nHonestly, I think the model the paper built is quite clever. Nothing is perfect, but I think this is a pretty good run at the question of, â€œHow high would the heritability of lifespan be if extrinsic mortality were lower?â€\nI only have two objections. The first is to the Science writing style. This is a paper describing a statistical model. So shouldnâ€™t there be somewhere in the paper where they explain exactly what they did, in order, from start to finish? Ostensibly, I think this is done in the left-hand column on the second page, just with little detail because Science is written for a general audience. But personally I think that description is the worst of all worlds. Instead of giving the high-level story in a coherent way, it throws random technical details at you without enough information to actually make sense of them. Couldnâ€™t the full story with the full details at least be in the appendix? I feel like this wasted hours of my time, and that if someone wanted to reproduce this work, they would have almost no chance of doing so from the description given. How have we as a society decided that we should take our â€œbestâ€ papers and do this to them?\nBut my main objection is this:\n\nAt first, I thought this was absurd. The fact that people die in car accidents is not a â€œconfounding factorâ€. And pretending that no one dies in a car accidents does not â€œaddressâ€ some kind of bias. Thatâ€™s just computing heritability in some other world. Remember, heritability is not some kind of Platonic form. It is an observational statistic. There is no such thing as â€œtrueâ€ heritability, independent of the contingent facts of our world.\nBut upon reflection, I think theyâ€™re trying to say something like this:\n\nHeritability of human lifespan is about 50% when extrinsic mortality is adjusted to be closer to modern levels.\n\nThe problem is: I think this isâ€¦ not true? Here are the actual heritability estimates in the paper, varying by dataset (different plots) the cutoff year (colors) and extrinsic mortality (x-axis).\n\nWhen extrinsic mortality goes down, heritability goes up. So the obvious question is: What is extrinsic mortality in modern people?\nThis is a tricky question, because â€œextrinsic mortalityâ€ isnâ€™t some simple observational statistic. It is a parameter in their model. (Remember, they never looked at causes of death.) So itâ€™s hard to say, but they seem to suggest that extrinsic mortality in modern people is 0.001 / year, or perhaps a bit less.\nThe above figures have the base-10 logarithm of extrinsic mortality on the x-axis. And the base-10 logarithm of 0.001 is -3. But if you look at the curves when the x-axis is -3, the heritability estimates are not 50%. Theyâ€™re more like 35-45%, depending on the particular model and age cutoff.\nSo hereâ€™s my suggested title:\n\nHeritability of human lifespan is about 40% when extrinsic mortality is adjusted to modern levels, according to our simulation.\n\nThere might be a reason I donâ€™t work at Science."
    },
    {
      "title": "dynomight.net: Why read novels?",
      "url": "https://dynomight.net/novels/",
      "source": "dynomight.net",
      "time": "2026-02-20T10:18:08.758545",
      "raw_desc": "Why should you read novels? We tell children theyâ€™re magic carpets for the mind / exercise for the soul instead of the body / lighthouses in the great sea of time. But arenâ€™t they ultimately a form of entertainment?\nMany years ago, I read Crime and Punishment. Here, with no research and no notes, is what I can remember about that book:\n\nIt was pretty good.\nThere was some guy, I think named Ras-something.\nHe was really angsty/edgy and lived in a small apartment or attic.\nOne day, for no particular reason, he killed an old woman.\nHaving done this random murder, he became even more angsty/edgy.\nThen there was this police inspector guy.\nThe inspector kept coming after Ras-whoever and making extremely long philosophical rants.\nThose rants may or may not have represented the personal views of Fyodor Dostoevsky.\nI canâ€™t remember how the book ended. Surely Ras-whoever didnâ€™t live happily ever after? But was he caught or did he confess? No idea.\n\nThis is probably below average. I know people who seem to remember every detail of everything they read. But even if youâ€™re one of them, so what? Is remembering those books better than remembering whatever else you would have done with your time if you hadnâ€™t been reading?\nAnd yet: If Iâ€™m on vacation and I spend an afternoon reading a novel where in the mountains or on a beach, I feel like Iâ€™m living my best life. Whereas if I spent an afternoon staring at short videos on my phone, Iâ€™m sure Iâ€™d feel like a gigantic loser. So whatâ€™s going on here?\nTheory 1: Ye olde status\nThe obvious explanation is that thereâ€™s nothing intrinsically great about reading novels. The reason we think itâ€™s great is that reading novelsâ€”at least the right onesâ€”is high status. Itâ€™s a way of playing the Glass Bead Game, a way of collecting cultural capital for you to lord over other people who donâ€™t have as much time or education as you do. It may feel like you â€œactually enjoy readingâ€, but thatâ€™s because youâ€™re a desperate striver that subconsciously shape-shifts into whatever you think will make you look fancy. Apologize for reading. Apologize!\nI think there is something in this. However, Iâ€™m also pretty sure itâ€™s not the full explanation, and Iâ€™m bored to death with everyone trying to explain everything this way. So letâ€™s move on.\nTheory 2: Diminishing returns\nSay you canâ€™t read novels. Maybe because youâ€™re illiterate, maybe because you have no attention span, maybe because you canâ€™t tear yourself away from Candy Clicker. Now, say you cultivate the ability to read novels. Whatever issues you address in that process, it seems like it will clearly be good for you, right?\nUnder this theory, whatâ€™s important is having the ability to read novels. But said ability is acquired by reading novels, so read some novels.\nAlternatively, say you could read novels, but you simply never have. Itâ€™s plausible that the first time you have the â€œnovelâ€ experience of taking photons into your eyes and mentally converting them into a story, this truly does feed your mind.\nBoth versions of this theory suggest that reading novels has diminishing returns. That fits nicely with the fact that many people push their children to read novels while not reading any themselves. But do we really believe that after youâ€™ve read some number of novels, itâ€™s pointless to read more?\nTheory 3: Common language\nI think Catcher in the Rye is a good but not great book. But I love talking about Catcher in the Rye because (1) all North Americans seem to have read it, and (2) whenever I ask someone to tell me how they feel about Holden Caulfield, I always seem to learn something about them.\n(I find him sympathetic.)\nIf thereâ€™s a group of people talking about Catcher in the Ryeâ€”or The Three-Body Problem, or Infinite Jest, or Don Quixoteâ€”then you benefit from being able to participate. The cynic might argue that this is zero-sum status competition. But I donâ€™t think thatâ€™s most of it. Because, at least in my social circles, people feel boorish talking about books if not everyone has read them. So these conversations only happen if everyone has read the book in question.\nUltimately, weâ€™re all alone in the world, and trying to connect with each other by pushing air through our throat meat. With more shared cultural context, those meat sounds are more meaningful, so we can all feel less alone.\nTrue. But shared context can come from other things, too, like traveling to the same places, or watching the same sports, or practicing the same skills or hobbies. So what makes books special? The two answers I see are:\n\nNothing. If you think theyâ€™re better than other types of cultural context, thatâ€™s because youâ€™re a book person.\nBooks leave more room for interpretation. Maybe Don Quixote is a fanatic, maybe heâ€™s an idealist, maybe heâ€™s a â€œwise foolâ€. Itâ€™s debatable. But thereâ€™s no doubt who won the last World Cup.\n\nI lean weakly towards the first answer. Novels are a useful form of social context. But thatâ€™s a side benefit. Itâ€™s not why we read most books.\nTheory 4: Legible mind-space\nMaybe novels are just another form of entertainment. OK. But say you tried to tell the same story as a novel or as movie / podcast / opera / interpretive dance performance. Different formats will be better in different ways. One advantage I see for novels is that they make it natural to explore the interior worlds of the characters.\nSome movies have voice-overs where characters explain what theyâ€™re thinking. But this is generally considered cringe and a poor use of the medium. Meanwhile, many books are mostly about exploring what the characters are thinking.\nThoughts are worth exploring. If you want to explore thoughts, maybe novels are the best way to do that.\nAside: Iâ€™ve mentioned before that I think My Brilliant Friend is the best TV show ever made. Can I confess that I like it much more than the books it is based on? Because, like the books, the TV show involves a lot of what the main character is thinking, and even makes heavy use of voice-overs. So maybe other mediums have unrealized potential?\nTheory 5: Purity of vision\nMovies are expensive to make. To be financially viable, they need to target a large slice of the population. Movies also reflect the combined efforts of many people. Both of these mean that movies are a compromise between different visions.\nNovels are usually written by one person. And theyâ€™re often written more for personal expression than to make money. After all, writing is fun. I meanâ€”writing is hard, but would you rather spend an afternoon holding up a shotgun microphone, cleaning a movie starâ€™s trailer, or writing a novel?\nTo quantify this, some searching suggests that around 10,000 feature films are released each year, as compared to around 1,000,000 novels. (Does one in 7,000 people really write a novel each year?) Thatâ€™s two orders of magnitude. So if you want to hear a truly unique story, a pure vision of one person, maybe novels are where youâ€™ll find it.\nTheory 6: All these theories are stupid\nOr: Maybe the point of reading War and Peace is that War and Peace is incredible and obviously one of the greatest pieces of art ever made in any medium. No one who reads War and Peace can question the value of what theyâ€™ve done. What are we talking about?\nFair. I definitely feel like Iâ€™m living my best life when I read War and Peace. But I also feel like Iâ€™m living an OK-ish life when I read a novel about Spenser, private investigator. And most novels most people read are closer to the Spenser than to War and Peace. And I still feel better spending an afternoon reading about Spenser than I would watching 99% of TV shows.\nTheory 7: Dopamine\nOr perhaps the difference is that reading is a thing you do rather than something you consume.\nThis theory holds than when spend an hour slurping up short-form video, youâ€™re training yourself to sort of pull a lever in the hope that some reward is delivered to you. But if you read (or do watercolors, or meditate) youâ€™re training yourself to calmly pursue long-term goals and to sustain attention in the face of complexity.\nSometimes I wonder if phones/apps are the most addictive thing ever created. I suspect that more people today are addicted to their phones today than were ever addicted to any drug other than caffeine or perhaps nicotine. And while a phone addiction is less physically harmful than tobacco, that phone addiction will eat a larger part of your soul.\nI think this is a big part of the explanation.\nTheory 8: Non-fungible time\nIn the end, I donâ€™t think novels are the best way to spend your time. In my view no novelâ€”not even War and Peaceâ€”is as good as a truly great conversation.\nBut great conversations are hard to create. Sometimes youâ€™re sitting on a train, or laying in bed, or itâ€™s just been a long day and you donâ€™t have the energy to find a giant block of marble and pursue your dream of experimental sculpture. In these situations, maybe reading a novel is the best thing you could do in the category of things you could realistically do.\nExercise for the reader: Apply these theories to blog posts."
    },
    {
      "title": "dynomight.net: Good if make prior after data instead of before",
      "url": "https://dynomight.net/prior/",
      "source": "dynomight.net",
      "time": "2026-02-20T10:18:08.767533",
      "raw_desc": "They say youâ€™re supposed to choose your prior in advance. Thatâ€™s why itâ€™s called a â€œpriorâ€. First, youâ€™re supposed to say say how plausible different things are, and then you update your beliefs based on what you see in the world.\nFor example, currently you areâ€”I assumeâ€”trying to decide if you should stop reading this post and do something else with your life. If youâ€™ve read this blog before, then lurking somewhere in your mind is some prior for how often my posts are good. For the sake of argument, letâ€™s say you think 25% of my posts are funny and insightful and 75% are boring and worthless.\n\nOK. But now here you are reading these words. If they seem bad/good, then that raises the odds that this particular post is worthless/non-worthless. For the sake of argument again, say you find these words mildly promising, meaning that a good post is 1.5Ã— more likely than a worthless post to contain words with this level of quality.\n\nIf you combine those two assumptions, that implies that the probability that this particular post is good is 33.3%. Thatâ€™s true because the red rectangle below has half the area of the blue one, and thus the probability that this post is good should be half the probability that itâ€™s bad (33.3% vs. 66.6%)\n\n\n\n(Why half the area? Because the red rectangle is â…“ as wide and Â³â„â‚‚ as tall as the blue one and â…“ Ã— Â³â„â‚‚ = Â½. If you only trust equations, click here for equations.)\n\nItâ€™s easiest to calculate the ratio of the odds that the post is good versus bad, namely\nP[good | words] / P[bad | words]\n = P[good, words] / P[bad, words]\n = (P[good] Ã— P[words | good])\n / (P[bad] Ã— P[words | bad])\n = (0.25 Ã— 1.5) / (0.75 Ã— 1)\n = 0.5.\n \nIt follows that\nP[good | words] = 0.5 Ã— P[bad | words],\n \nand thus that\nP[good | words] = 1/3.\n \nAlternatively, if you insist on using Bayesâ€™ equation:\nP[good | words]\n = P[good] Ã— P[words | good] / P[words]\n = P[good] Ã— P[words | good]\n / (P[good] Ã— P[words | good] + P[bad] Ã— P[words | bad])\n = 0.25 Ã— 1.5 / (0.25 Ã— 1.5 + 0.75)\n = (1/3)\n \n\nTheoretically, when you chose your prior that 25% of dynomight posts are good, that was supposed to reflect all the information you encountered in life before reading this post. Changing that number based on information contained in this post wouldnâ€™t make any sense, because that information is supposed to be reflected in the second step when you choose your likelihood p[good | words]. Changing your prior based on this post would amount to â€œdouble-countingâ€.\nIn theory, thatâ€™s right. Itâ€™s also right in practice for the above example, and for the similar cute little examples you find in textbooks.\nBut for real problems, Iâ€™ve come to believe that refusing to change your prior after you see the data often leads to tragedy. The reason is that in real problems, things are rarely just â€œgoodâ€ or â€œbadâ€, â€œtrueâ€ or â€œfalseâ€. Instead, truth comes in an infinite number of varieties. And you often canâ€™t predict which of these varieties matter until after youâ€™ve seen the data.\nAliens\nLet me show you what I mean. Say youâ€™re wondering if there are aliens on Earth. As far as we know, thereâ€™s no reason aliens shouldnâ€™t have emerged out of the random swirling of molecules on some other planet, developed a technological civilization, built spaceships, and shown up here. So it seems reasonable to choose a prior itâ€™s equally plausible that there are aliens or that there are not, i.e. that\nP[aliens] â‰ˆ P[no aliens] â‰ˆ 50%.\n\n\nMeanwhile, here on our actual world, we have lots of weird alien-esque evidence, like the Gimbal video, the Go Fast video, the FLIR1 video, the Wow! signal, government reports on unidentified aerial phenomena, and lots of pilots that report seeing â€œtic-tacsâ€ fly around in physically impossible ways. Call all that stuff data. If aliens werenâ€™t here, then it seems hard to explain all that stuff. So it seems like P[data | no aliens] should be some low number.\nOn the other hand, if aliens were here, then why donâ€™t we ever get a good image? Why are there endless confusing reports and rumors and grainy videos, but never a single clear close-up high-resolution video, and never any alien debris found by some random person on the ground? That also seems hard to explain if aliens were here. So I think P[data | aliens] should also be some low number. For the sake of simplicity, letâ€™s call it a wash and assume that\nP[data | no aliens] â‰ˆ P[data | aliens].\n\n\nSince neither the prior nor the data see any difference between aliens and no-aliens, the posterior probability is\nP[no aliens | data] â‰ˆ P[aliens | data] â‰ˆ 50%.\n\n\nSee the problem?\n\n\n(Click here for math.)\n\nObserve that\nP[aliens | data] / P[no aliens | data]\n = P[aliens, data] / P[no aliens, data]\n = (P[aliens] Ã— P[data | aliens])\n / (P[no aliens] Ã— P[data | no aliens])\n â‰ˆ 1,\n \nwhere the last line follows from the fact that P[aliens] â‰ˆ P[no aliens] and P[data | aliens] â‰ˆ P[data | no aliens]. Thus we have that\nP[aliens | data] â‰ˆ P[no aliens | data] â‰ˆ 50%.\n \n\nWeâ€™re friends. We respect each other. So letâ€™s not argue about if my starting assumptions are good. Theyâ€™re my assumptions. I like them. And yet the final conclusion seems insane to me. What went wrong?\nAssuming I didnâ€™t screw up the math (I didnâ€™t), the obvious explanation is that Iâ€™m experiencing cognitive dissonance as a result of a poor decision on my part to adopt a set of mutually contradictory beliefs. Say you claim that Alice is taller than Bob and Bob is taller than Carlos, but you deny that Alice is taller than Carlos. If so, that would mean that youâ€™re confused, not that youâ€™ve discovered some interesting paradox.\nPerhaps if I believe that P[aliens] â‰ˆ P[no aliens] and that P[data | aliens] â‰ˆ P[data | no aliens], then I must accept that P[aliens | data] â‰ˆ P[no aliens | data]. Maybe rejecting that conclusion just means I have some personal issues I need to work on.\nI deny that explanation. I deny it! Or, at least, I deny thatâ€™s itâ€™s most helpful way to think about this situation. To see why, letâ€™s build a second model.\nMore aliens\nHereâ€™s a trivial observation that turns out to be important: â€œThere are aliensâ€ isnâ€™t a single thing. There could be furry aliens, slimy aliens, aliens that like synthwave music, etc. When I stated my prior, I could have given different probabilities to each of those cases. But if I had, it wouldnâ€™t have changed anything, because thereâ€™s no reason to think that furry vs. slimy aliens would have any difference in their eagerness to travel to ape-planets and fly around in physically impossible tic-tacs.\nBut suppose I had divided up the state of the world into these four possibilities:\n\n\n\npossibility\ndescription\n\n\n\n\nNo aliens + normal people\nThere are no aliens. Meanwhile, people are normal and not prone to hallucinating evidence for things that donâ€™t exist.\n\n\nNo aliens + weird people\nThere are no aliens. Meanwhile, people are weird and do tend to hallucinate evidence for things that donâ€™t exist.\n\n\nNormal aliens\nThere are aliens. They may or may not have cool spaceships or enjoy shooting people with lasers. But one way or another, they leave obvious, indisputable evidence that theyâ€™re around.\n\n\nWeird aliens\nThere are aliens. But they stay hidden until humans get interested in space travel. And after that, they let humans take confusing grainy videos, but never a single good video, never ever, not one.\n\n\n\nIf I had broken things down that way, I might have chosen this prior:\nP[no aliens + normal people] â‰ˆ 41%\nP[no aliens + weird people] â‰ˆ 9%\nP[normal aliens] â‰ˆ 49%\nP[weird aliens] â‰ˆ 1%\n\n\nNow, letâ€™s think about the empirical evidence again. Itâ€™s incompatible with no aliens + normal people, since if there were no aliens, then normal people wouldnâ€™t hallucinate flying tic-tacs. The evidence is also incompatible with normal aliens since is those kinds of aliens were around they would make their existence obvious. However, the evidence fits pretty well with weird aliens and also with no aliens + weird people.\nSo, a reasonable model would be\nP[data | normal aliens] â‰ˆ 0\t\nP[data | no aliens + normal people] â‰ˆ 0\nP[data | weird aliens] â‰ˆ P[data | no aliens + weird people].\n\n\nIf we combine those assumptions, now we only get a 10% posterior probability of aliens.\nP[no aliens + normal people | data] â‰ˆ 0\nP[no aliens + weird people | data] â‰ˆ 90%\nP[normal aliens | data] â‰ˆ 0\nP[weird aliens | data] â‰ˆ 10%\n\n\nNow the results seem non-insane.\n\n\n(math)\n\nTo see why, first note that\nP[normal aliens | data]\n â‰ˆ P[data | no aliens + normal people]\n â‰ˆ 0,\n \nsince both normal aliens and no aliens + normal people have near-zero probability of producing the observed data.\nMeanwhile,\nP[no aliens + weird people | data] / P[weird aliens | data]\n = P[no aliens + weird people, data] / P[weird aliens, data]\n â‰ˆ P[no aliens + weird people] / P[weird aliens]\n â‰ˆ .09 / .01\n = 9,\n \nwhere the second equality follows from the fact that the data is assumed to be equally likely under no aliens + weird people and weird people\nIt follows that\nP[no aliens + normal people | data]\n â‰ˆ 9 Ã— P[weird aliens | data],\n \nand so\nP[no aliens + weird people | data] â‰ˆ 90%\nP[weird aliens | data] â‰ˆ 10%.\n \n\nHuh?\nI hope you are now confused. If not, let me lay out whatâ€™s strange: The priors for the two above models both say that thereâ€™s a 50% chance of aliens. The first prior wasnâ€™t wrong, it was just less detailed than the second one.\nThatâ€™s weird, because the second prior seemed to lead to completely different predictions. If a prior is non-wrong and the math is non-wrong, shouldnâ€™t your answers be non-wrong? What the hell?\nThe simple explanation is that Iâ€™ve been lying to you a little bit. Take any situation where youâ€™re trying to determine the truth of anything. Then thereâ€™s some space of things that could be true.\n\nIn some cases, this space is finite. If youâ€™ve got a single tritium atom and you wait a year, either the atom decays or it doesnâ€™t. But in most cases, thereâ€™s a large or infinite space of possibilities. Instead of you just being â€œsickâ€ or â€œnot sickâ€, you could be â€œhigh temperature but in good spiritsâ€ or â€œseems fine except wonâ€™t stop eating onionsâ€.\n(Usually the space of things that could be true isnâ€™t easy to map to a small 1-D interval. Iâ€™m drawing like that for the sake of visualization, but really you should think of it as some high-dimensional space, or even an infinite dimensional space.)\nIn the case of aliens, the space of things that could be true might include, â€œThere are lots of slimy aliens and a small number of furry aliens and the slimy aliens are really shy and the furry aliens are afraid of squirrels.â€ So, in principle, what you should do is divide up the space of things that might be true into tons of extremely detailed things and give a probability to each.\n\nOften, the space of things that could be true is infinite. So theoretically, if you really want to do things by the book, what you should really do is specify how plausible each of those (infinite) possibilities is.\nAfter youâ€™ve done that, you can look at the data. For each thing that could be true, you need to think about the probability of the data. Since thereâ€™s an infinite number of things that could be true, thatâ€™s an infinite number of probabilities you need to specify. You could picture it as some curve like this:\n\n(Thatâ€™s a generic curve, not one for aliens.)\nTo me, this is the most underrated problem with applying Bayesian reasoning to complex real-world situations: In practice, there are an infinite number of things that can be true. Itâ€™s a lot of work to specify prior probabilities for an infinite number of things. And itâ€™s also a lot of work to specify the likelihood of your data given an infinite number of things.\nSo what do we do in practice? We simplify, usually by limiting creating grouping the space of things that could be true into some small number of discrete categories. For the above curve, you might break things down into these four equally-plausible possibilities.\n\nThen you might estimate these data probabilities for each of those possibilities.\n\nThen you could put those together to get this posterior:\n\nThatâ€™s not bad. But it is just an approximation. Your â€œrealâ€ posterior probabilities correspond to these areas:\n\nThat approximation was pretty good. But the reason it was good is that we started out with a good discretization of the space of things that might be true: One where the likelihood of the data didnâ€™t vary too much for the different possibilities inside of A, B, C, and D. Imagine the likelihood of the dataâ€”if you were able to think about all the infinite possibilities one by oneâ€”looked like this:\n\nThis is dangerous. The problem is that you canâ€™t actually think about all those infinite possibilities. When you think about four four discrete possibilities, you might estimate some likelihood that looks like this:\n\nIf you did that, that would lead to you underestimating the probability of A, B, and C, and overestimating the probability of D.\nThis is where my first model of aliens went wrong. My prior P[aliens] was not wrong. (Not to me.) The mistake was in assigning the same value to P[data | aliens] and P[data | no aliens]. Sure, I think the probability of all our alien-esque data is equally likely given aliens and given no-aliens. But thatâ€™s only true for certain kinds of aliens, and certain kinds of no-aliens. And my prior for those kinds of aliens is much lower than for those kinds of non-aliens.\nTechnically, the fix to the first model is simple: Make P[data | aliens] lower. But the reason itâ€™s lower is that I have additional prior information that I forgot to include in my original prior. If I just assert that P[data | aliens] is much lower than P[data | no aliens] then the whole formal Bayesian thing isnâ€™t actually doing very muchâ€”I might as well just state that I think P[aliens | data] is low. If I want to formally justify why P[data | aliens] should be lower, that requires a messy recursive procedure where I sort of add that missing prior information and then integrate it out when computing the data likelihood.\n\n\n(math)\n\nMathematically,\nP[data | aliens]\n = âˆ« P[wierd aliens | aliens]\n Ã— P[data | wierd aliens] d(weird aliens)\n + âˆ« P[normal aliens | aliens]\n Ã— P[data | normal aliens] d(normal aliens).\n \nBut now I have to give a detailed prior anyway. So what was the point of starting with a simple one?\n\nI donâ€™t think that technical fix is very good. While itâ€™s technically correct (har-har) itâ€™s very unintuitive. The better solution is what I did in the second model: To create a finer categorization of the space of things that might be true, such that the probability of the data is constant-ish for each term.\nThe thing is: Such a categorization depends on the data. Without seeing the actual data in our world, I would never have predicted that we would have so many pilots that report seeing tic-tacs. So I would never have predicted that I should have categories that are based on how much people might hallucinate evidence or how much aliens like to mess with us. So the only practical way to get good results is to first look at the data to figure out what categories are important, and then to ask yourself how likely you would have said those categories were, if you hadnâ€™t yet seen any of the evidence."
    },
    {
      "title": "lucumr.pocoo.org: The Final Bottleneck",
      "url": "https://lucumr.pocoo.org/2026/2/13/the-final-bottleneck/",
      "source": "lucumr.pocoo.org",
      "time": "2026-02-20T10:18:08.795178",
      "raw_desc": "Historically, writing code was slower than reviewing code.\nIt might not have felt that way, because code reviews sat in queues until\nsomeone got around to picking it up.  But if you compare the\nactual acts themselves, creation was usually the more expensive part.  In teams\nwhere people both wrote and reviewed code, it never felt like â€œwe should\nprobably program slower.â€\nSo when more and more people tell me they no longer know what code is in their\nown codebase, I feel like something is very wrong here and itâ€™s time to\nreflect.\nYou Are Here\nSoftware engineers often believe that if we make the bathtub\nbigger, overflow disappears.  It doesnâ€™t.\nOpenClaw right now has north of 2,500\npull requests open.  Thatâ€™s a big bathtub.\nAnyone who has worked with queues knows this: if input grows faster than\nthroughput, you have an accumulating failure.  At that point, backpressure and\nload shedding are the only things that retain a system that can still operate.\nIf you have ever been in a Starbucks overwhelmed by mobile orders, you know the\nfeeling.  The in-store experience breaks down.  You no longer know how many\norders are ahead of you.  There is no clear line, no reliable wait estimate, and\noften no real cancellation path unless you escalate and make noise.\nThat is what many AI-adjacent open source projects feel like right now.  And\nincreasingly, that is what a lot of internal company projects feel like in\nâ€œAI-firstâ€ engineering teams, and thatâ€™s not sustainable.  You canâ€™t triage, you\ncanâ€™t review, and many of the PRs cannot be merged after a certain point because\nthey are too far out of date. And the creator might have lost the motivation to\nactually get it merged.\nThere is huge excitement about newfound delivery speed, but in private\nconversations, I keep hearing the same second sentence: people are also confused\nabout how to keep up with the pace they themselves created.\nWe Have Been Here Before\nHumanity has been here before.  Many times over.  We already talk about the\nLuddites a lot in the context of AI, but itâ€™s interesting to see what led up to\nit.  Mark Cartwright wrote a great article about the textile\nindustry\nin Britain during the industrial revolution.  At its core was a simple idea:\nwhenever a bottleneck was removed, innovation happened downstream from that.\nWeaving sped up? Yarn became the constraint. Faster spinning? Fibre needed to be\nimproved to support the new speeds until finally the demand for cotton went up\nand that had to be automated too.  We saw the same thing in shipping that led\nto modern automated ports and containerization.\nAs software engineers we have been here too.  Assembly did not scale to larger\nengineering teams, and we had to invent higher level languages.  A lot of what\nprogramming languages and software development frameworks did was allow us\nto write code faster and to scale to larger code bases.  What it did not do up\nto this point was take away the core skill of engineering.\nWhile itâ€™s definitely easier to write C than assembly, many of the core problems\nare the same.  Memory latency still matters, physics are still our ultimate\nbottleneck, algorithmic complexity still makes or breaks software at scale.\nGiving Up?\nWhen one part of the pipeline becomes dramatically faster, you need to throttle\ninput.  Pi is a great example of this.  PRs are auto closed\nunless people are trusted.  It takes OSS\nvacations.  Thatâ€™s one\noption: you just throttle the inflow.  You push against your newfound powers\nuntil you can handle them.\nOr Giving In\nBut what if the speed continues to increase?  What downstream of writing code do\nwe have to speed up?  Sure, the pull request review clearly turns into the\nbottleneck.  But it cannot really be automated.  If the machine writes the code,\nthe machine better review the code at the same time.  So what ultimately comes\nup for human review would already have passed the most critical possible review\nof the most capable machine.  What else is in the way?  If we continue with the\nfundamental belief that machines cannot be accountable, then humans need to be\nable to understand the output of the machine.  And the machine will ship\nrelentlessly.  Support tickets of customers will go straight to machines to\nimplement improvements and fixes, for other machines to review, for humans to\nrubber stamp in the morning.\nA lot of this sounds both unappealing and reminiscent of the textile industry.\nThe individual weaver no longer carried responsibility for a bad piece of cloth.\nIf it was bad, it became the responsibility of the factory as a whole and it was\njust replaced outright.  As weâ€™re entering the phase of single-use plastic\nsoftware, we might be moving the whole layer of responsibility elsewhere.\nI Am The Bottleneck\nBut to me it still feels different.  Maybe thatâ€™s because my lowly brain canâ€™t\ncomprehend the change we are going through, and future generations will just\nlaugh about our challenges.  It feels different to me, because what I see taking\nplace in some Open Source projects, in some companies and teams feels deeply\nwrong and unsustainable.  Even Steve Yegge himself now casts\ndoubts about the\nsustainability of the ever-increasing pace of code creation.\nSo what if we need to give in?  What if we need to pave the way for this new\ntype of engineering to become the standard?  What affordances will we have to\ncreate to make it work?  I for one do not know.  Iâ€™m looking at this with\nfascination and bewilderment and trying to make sense of it.\nBecause it is not the final bottleneck.  We will find ways to take\nresponsibility for what we ship, because society will demand it.  Non-sentient\nmachines will never be able to carry responsibility, and it looks like we will\nneed to deal with this problem before machines achieve this status.\nRegardless of how bizarre they appear to\nact already.\nI too am the bottleneck\nnow.  But you know what?\nTwo years ago, I too was the bottleneck.  I was the bottleneck all along.  The\nmachine did not really change that.  And for as long as I carry responsibilities\nand am accountable, this will remain true.  If we manage to push accountability\nupwards, it might change, but so far, how that would happen is not clear."
    },
    {
      "title": "lucumr.pocoo.org: A Language For Agents",
      "url": "https://lucumr.pocoo.org/2026/2/9/a-language-for-agents/",
      "source": "lucumr.pocoo.org",
      "time": "2026-02-20T10:18:08.803957",
      "raw_desc": "Last year I first started thinking about what the future of programming\nlanguages might look like now that agentic engineering is a growing thing.\nInitially I felt that the enormous corpus of pre-existing code would cement\nexisting languages in place but now Iâ€™m starting to think the opposite is true.\nHere I want to outline my thinking on why we are going to see more new\nprogramming languages and why there is quite a bit of space for interesting\ninnovation.  And just in case someone wants to start building one, here are some\nof my thoughts on what we should aim for!\nWhy New Languages Work\nDoes an agent perform dramatically better on a language that it has in its\nweights?  Obviously yes.  But there are less obvious factors that affect how\ngood an agent is at programming in a language: how good the tooling around it is\nand how much churn there is.\nZig seems underrepresented in the weights (at least in the models Iâ€™ve used)\nand also changing quickly.  That combination is not optimal, but itâ€™s still\npassable: you can program even in the upcoming Zig version if you point the\nagent at the right documentation.  But itâ€™s not great.\nOn the other hand, some languages are well represented in the weights but agents\nstill donâ€™t succeed as much because of tooling choices.  Swift is a good\nexample: in my experience the tooling around building a Mac or iOS application\ncan be so painful that agents struggle to navigate it.  Also not great.\nSo, just because it exists doesnâ€™t mean the agent succeeds and just because itâ€™s\nnew also doesnâ€™t mean that the agent is going to struggle.  Iâ€™m convinced that\nyou can build yourself up to a new language if you donâ€™t want to depart\neverywhere all at once.\nThe biggest reason new languages might work is that the cost of coding is going\ndown dramatically.  The result is the breadth of an ecosystem matters less. Iâ€™m\nnow routinely reaching for JavaScript in places where I would have used Python.\nNot because I love it or the ecosystem is better, but because the agent does\nmuch better with TypeScript.\nThe way to think about this: if important functionality is missing in my\nlanguage of choice, I just point the agent at a library from a different\nlanguage and have it build a port.  As a concrete example, I recently built an\nEthernet driver in JavaScript to implement the host controller for our sandbox.\nImplementations exist in Rust, C, and Go, but I wanted something pluggable and\ncustomizable in JavaScript.  It was easier to have the agent reimplement it than\nto make the build system and distribution work against a native binding.\nNew languages will work if their value proposition is strong enough and they\nevolve with knowledge of how LLMs train.  People will adopt them despite being\nunderrepresented in the weights.  And if they are designed to work well with\nagents, then they might be designed around familiar syntax that is already known\nto work well.\nWhy A New Language?\nSo why would we want a new language at all?  The reason this is interesting to\nthink about is that many of todayâ€™s languages were designed with the assumption\nthat punching keys is laborious, so we traded certain things for brevity.  As an\nexample, many languages â€” particular modern ones â€” lean heavily on type\ninference so that you donâ€™t have to write out types.  The downside is that you\nnow need an LSP or the resulting compiler error messages to figure out what the\ntype of an expression is.  Agents struggle with this too, and itâ€™s also\nfrustrating in pull request review where complex operations can make it very\nhard to figure out what the types actually are.  Fully dynamic languages are\neven worse in that regard.\nThe cost of writing code is going down, but because we are also producing more\nof it, understanding what the code does is becoming more important.  We might\nactually want more code to be written if it means there is less ambiguity when\nwe perform a review.\nI also want to point out that we are heading towards a world where some code is\nnever seen by a human and is only consumed by machines.  Even in that case, we\nstill want to give an indication to a user, who is potentially a non-programmer,\nabout what is going on.  We want to be able to explain to a user what the code\nwill do without going into the details of how.\nSo the case for a new language comes down to: given the fundamental changes in\nwho is programming and what the cost of code is, we should at least consider\none.\nWhat Agents Want\nItâ€™s tricky to say what an agent wants because agents will lie to you and they\nare influenced by all the code theyâ€™ve seen.  But one way to estimate how they\nare doing is to look at how many changes they have to perform on files and how\nmany iterations they need for common tasks.\nThere are some things Iâ€™ve found that I think will be true for a while.\nContext Without LSP\nThe language server protocol lets an IDE infer information about whatâ€™s under\nthe cursor or what should be autocompleted based on semantic knowledge of the\ncodebase.  Itâ€™s a great system, but it comes at one specific cost that is tricky\nfor agents: the LSP has to be running.\nThere are situations when an agent just wonâ€™t run the LSP â€” not because of\ntechnical limitations, but because itâ€™s also lazy and will skip that step if it\ndoesnâ€™t have to.  If you give it an example from documentation, there is no easy\nway to run the LSP because itâ€™s a snippet that might not even be complete.  If\nyou point it at a GitHub repository and it pulls down individual files, it will\njust look at the code.  It wonâ€™t set up an LSP for type information.\nA language that doesnâ€™t split into two separate experiences (with-LSP and\nwithout-LSP) will be beneficial to agents because it gives them one unified way\nof working across many more situations.\nBraces, Brackets, and Parentheses\nIt pains me as a Python developer to say this, but whitespace-based indentation\nis a problem.  The underlying token efficiency of getting whitespace right is\ntricky, and a language with significant whitespace is harder for an LLM to work\nwith.  This is particularly noticeable if you try to make an LLM do surgical\nchanges without an assisted tool.  Quite often they will intentionally disregard\nwhitespace, add markers to enable or disable code and then rely on a code\nformatter to clean up indentation later.\nOn the other hand, braces that are not separated by whitespace can cause issues\ntoo.  Depending on the tokenizer, runs of closing parentheses can end up split\ninto tokens in surprising ways (a bit like the â€œstrawberryâ€ counting problem),\nand itâ€™s easy for an LLM to get Lisp or Scheme wrong because it loses track of\nhow many closing parentheses it has already emitted or is looking at.  Fixable\nwith future LLMs?  Sure, but also something that was hard for humans to get\nright too without tooling.\nFlow Context But Explicit\nReaders of this blog might know that Iâ€™m a huge believer in async locals and\nflow execution context â€” basically the ability to carry data through every\ninvocation that might only be needed many layers down the call chain.  Working\nat an observability company has really driven home the importance of this for\nme.\nThe challenge is that anything that flows implicitly might not be configured.\nTake for instance the current time.  You might want to implicitly pass a timer\nto all functions.  But what if a timer is not configured and all of a sudden a\nnew dependency appears?  Passing all of it explicitly is tedious for both humans\nand agents and bad shortcuts will be made.\nOne thing Iâ€™ve experimented with is having effect markers on functions that are\nadded through a code formatting step.  A function can declare that it needs the\ncurrent time or the database, but if it doesnâ€™t mark this explicitly, itâ€™s\nessentially a linting warning that auto-formatting fixes.  The LLM can start\nusing something like the current time in a function and any existing caller gets\nthe warning; formatting propagates the annotation.\nThis is nice because when the LLM builds a test, it can precisely mock out\nthese side effects â€” it understands from the error messages what it has to\nsupply.\nFor instance:\nfn issue(sub: UserId, scopes: []Scope) -> Token\n    needs { time, rng }\n{\n    return Token{\n        sub,\n        exp: time.now().add(24h),\n        scopes,\n    }\n}\n\ntest \"issue creates exp in the future\" {\n    using time = time.fixed(\"2026-02-06T23:00:00Z\");\n    using rng  = rng.deterministic(seed: 1);\n\n    let t = issue(user(\"u1\"), [\"read\"]);\n    assert(t.exp > time.now());\n}\n\nResults over Exceptions\nAgents struggle with exceptions, they are afraid of them.  Iâ€™m not sure to what\ndegree this is solvable with RL (Reinforcement Learning), but right now agents\nwill try to catch everything they can, log it, and do a pretty poor recovery.\nGiven how little information is actually available about error paths, that makes\nsense.  Checked exceptions are one approach, but they propagate all the way up\nthe call chain and donâ€™t dramatically improve things.  Even if they end up as\nhints where a linter tracks which errors can fly by, there are still many call\nsites that need adjusting.  And like the auto-propagation proposed for context\ndata, it might not be the right solution.\nMaybe the right approach is to go more in on typed results, but thatâ€™s still\ntricky for composability without a type and object system that supports it.\nMinimal Diffs and Line Reading\nThe general approach agents use today to read files into memory is line-based,\nwhich means they often pick chunks that span multi-line strings.  One easy way\nto see this fall apart: have an agent work on a 2000-line file that also\ncontains long embedded code strings â€” basically a code generator.  The agent\nwill sometimes edit within a multi-line string assuming itâ€™s the real code when\nitâ€™s actually just embedded code in a multi-line string.  For multi-line\nstrings, the only language Iâ€™m aware of with a good solution is Zig, but its\nprefix-based syntax is pretty foreign to most people.\nReformatting also often causes constructs to move to different lines.  In many\nlanguages, trailing commas in lists are either not supported (JSON) or not\ncustomary.  If you want diff stability, youâ€™d aim for a syntax that requires\nless reformatting and mostly avoids multi-line constructs.\nMake It Greppable\nWhatâ€™s really nice about Go is that you mostly cannot import symbols from\nanother package into scope without every use being prefixed with the package\nname.  Eg: context.Context instead of Context.  There are escape hatches\n(import aliases and dot-imports), but theyâ€™re relatively rare and usually\nfrowned upon.\nThat dramatically helps an agent understand what itâ€™s looking at.  In general,\nmaking code findable through the most basic tools is great â€” it works with\nexternal files that arenâ€™t indexed, and it means fewer false positives for\nlarge-scale automation driven by code generated on the fly (eg: sed, perl\ninvocations).\nLocal Reasoning\nMuch of what Iâ€™ve said boils down to: agents really like local reasoning.  They\nwant it to work in parts because they often work with just a few loaded files in\ncontext and donâ€™t have much spatial awareness of the codebase.  They rely on\nexternal tooling like grep to find things, and anything thatâ€™s hard to grep or\nthat hides information elsewhere is tricky.\nDependency Aware Builds\nWhat makes agents fail or succeed in many languages is just how good the build\ntools are.  Many languages make it very hard to determine what actually needs to\nrebuild or be retested because there are too many cross-references.  Go is\nreally good here: it forbids circular dependencies between packages (import\ncycles), packages have a clear layout, and test results are cached.\nWhat Agents Hate\nMacros\nAgents often struggle with macros.  It was already pretty clear that humans\nstruggle with macros too, but the argument for them was mostly that code\ngeneration was a good way to have less code to write.  Since that is less of a\nconcern now, we should aim for languages with less dependence on macros.\nThereâ€™s a separate question about generics and\ncomptime.  I think they fare\nsomewhat better because they mostly generate the same structure with different\nplaceholders and itâ€™s much easier for an agent to understand that.\nRe-Exports and Barrel Files\nRelated to greppability: agents often struggle to understand barrel\nfiles and they donâ€™t\nlike them.  Not being able to quickly figure out where a class or function comes\nfrom leads to imports from the wrong place, or missing things entirely and\nwasting context by reading too many files.  A one-to-one mapping from where\nsomething is declared to where itâ€™s imported from is great.\nAnd it does not have to be overly strict either.  Go kind of goes this way, but\nnot too extreme.  Any file within a directory can define a function, which isnâ€™t\noptimal, but itâ€™s quick enough to find and you donâ€™t need to search too far.\nIt works because packages are forced to be small enough to find everything with\ngrep.\nThe worst case is free re-exports all over the place that completely decouple\nthe implementation from any trivially reconstructable location on disk.  Or\nworse: aliasing.\nAliasing\nAgents often hate it when aliases are involved.  In fact, you can get them to\neven complain about it in thinking blocks if you let them refactor something\nthat uses lots of aliases.  Ideally a language encourages good naming and\ndiscourages aliasing at import time as a result.\nFlaky Tests and Dev Env Divergence\nNobody likes flaky tests, but agents even less so.  Ironic given how\nparticularly good agents are at creating flaky tests in the first place.  Thatâ€™s\nbecause agents currently love to mock and most languages do not support mocking\nwell.  So many tests end up accidentally not being concurrency safe or depend on\ndevelopment environment state that then diverges in CI or production.\nMost programming languages and frameworks make it much easier to write flaky\ntests than non-flaky ones.  Thatâ€™s because they encourage indeterminism\neverywhere.\nMultiple Failure Conditions\nIn an ideal world the agent has one command, that lints and compiles and it\ntells the agent if all worked out fine.  Maybe another command to run all tests\nthat need running.  In practice most environments donâ€™t work like this.  For\ninstance in TypeScript you can often run the code even though it fails\ntype checks.  That can gaslight the agent.  Likewise\ndifferent bundler setups can cause one thing to succeed just for a slightly\ndifferent setup in CI to fail later.  The more uniform the tooling the better.\nIdeally it either runs or doesnâ€™t and there is mechanical fixing for as many\nlinting failures as possible so that the agent does not have to do it by hand.\nWill We See New Languages?\nI think we will.  We are writing more software now than we ever have â€” more\nwebsites, more open source projects, more of everything.  Even if the ratio of\nnew languages stays the same, the absolute number will go up.  But I also truly\nbelieve that many more people will be willing to rethink the foundations of\nsoftware engineering and the languages we work with.  Thatâ€™s because while for\nsome years it has felt you need to build a lot of infrastructure for a language\nto take off, now you can target a rather narrow use case: make sure the agent is\nhappy and extend from there to the human.\nI just hope we see two things.  First, some outsider art: people who havenâ€™t\nbuilt languages before trying their hand at it and showing us new things.\nSecond, a much more deliberate effort to document what works and what doesnâ€™t\nfrom first principles.  We have actually learned a lot about what makes good\nlanguages and how to scale software engineering to large teams.  Yet,  finding\nit written down, as a consumable overview of good and bad language design, is\nvery hard to come by.  Too much of it has been shaped by opinion on rather\npointless things instead of hard facts.\nNow though, we are slowly getting to the point where facts matter more, because\nyou can actually measure what works by seeing how well agents perform with it.\nNo human wants to be subject to surveys, but agents donâ€™t\ncare.  We can see how successful they are and where they\nare struggling."
    },
    {
      "title": "lucumr.pocoo.org: Pi: The Minimal Agent Within OpenClaw",
      "url": "https://lucumr.pocoo.org/2026/1/31/pi/",
      "source": "lucumr.pocoo.org",
      "time": "2026-02-20T10:18:08.806860",
      "raw_desc": "If you havenâ€™t been living under a rock, you will have noticed this week that a\nproject of my friend Peter went viral on the\ninternet.  It went by many names. The\nmost recent one is OpenClaw but in the news you might\nhave encountered it as ClawdBot or MoltBot depending on when you read about it.\nIt is an agent connected to a communication channel of your choice that just\nruns code.\nWhat you might be less familiar with is that whatâ€™s under the hood of OpenClaw\nis a little coding agent called Pi. And\nPi happens to be, at this point, the coding agent that I use almost exclusively.\nOver the last few weeks I became more and more of a shill for the little agent.\nAfter I gave a talk on this recently, I realized that I did not actually write\nabout Pi on this blog yet, so I feel like I might want to give some context on\nwhy Iâ€™m obsessed with it, and how it relates to OpenClaw.\nPi is written by Mario Zechner and unlike Peter, who\naims for â€œsci-fi with a touch of madness,â€ 1 Mario is very grounded.  Despite\nthe differences in approach, both OpenClaw and Pi follow the same idea: LLMs are\nreally good at writing and running code, so embrace this.  In some ways I think\nthatâ€™s not an accident because Peter got me and Mario hooked on this idea, and\nagents last year.\nWhat is Pi?\nSo Pi is a coding agent.  And there are many coding agents.  Really, I think you\ncan pick effectively anyone off the shelf at this point and you will be able to\nexperience what itâ€™s like to do agentic programming.  In reviews on this blog\nIâ€™ve positively talked about AMP and one of the reasons I resonated so much with\nAMP is that it really felt like it was a product built by people who got both\naddicted to agentic programming but also had tried a few different things to see\nwhich ones work and not just to build a fancy UI around it.\nPi is interesting to me because of two main reasons:\n\nFirst of all, it has a tiny core. It has the shortest system prompt of any\nagent that Iâ€™m aware of and it only has four tools: Read, Write, Edit, Bash. \nThe second thing is that it makes up for its tiny core by providing an\nextension system that also allows extensions to persist state into sessions,\nwhich is incredibly powerful. \n\nAnd a little bonus: Pi itself is written like excellent software. It doesnâ€™t\nflicker, it doesnâ€™t consume a lot of memory, it doesnâ€™t randomly break, it is\nvery reliable and it is written by someone who takes great care of what goes\ninto the software.\nPi also is a collection of little components that you can build your own agent\non top.  Thatâ€™s how OpenClaw is built, and thatâ€™s also how I built my own little\nTelegram bot and how Mario built his\nmom.  If you want\nto build your own agent, connected to something, Pi when pointed to itself and\nmom, will conjure one up for you.\nWhatâ€™s Not In Pi\nAnd in order to understand whatâ€™s in Pi, itâ€™s even more important to understand\nwhatâ€™s not in Pi, why itâ€™s not in Pi and more importantly: why it wonâ€™t be in\nPi.  The most obvious omission is support for MCP.  There is no MCP support in\nit. While you could build an extension for it, you can also do what OpenClaw\ndoes to support MCP which is to use\nmcporter. mcporter exposes MCP calls via\na CLI interface or TypeScript bindings and maybe your agent can do something\nwith it.  Or not, I donâ€™t know :)\nAnd this is not a lazy omission.  This is from the philosophy of how Pi works.\nPiâ€™s entire idea is that if you want the agent to do something that it doesnâ€™t\ndo yet, you donâ€™t go and download an extension or a skill or something like\nthis. You ask the agent to extend itself.  It celebrates the idea of code\nwriting and running code.\nThatâ€™s not to say that you cannot download extensions.  It is very much\nsupported. But instead of necessarily encouraging you to download someone elseâ€™s\nextension, you can also point your agent to an already existing extension, say\nlike, build it like the thing you see over there, but make these changes to it\nthat you like.\nAgents Built for Agents Building Agents\nWhen you look at what Pi and by extension OpenClaw are doing, there is an\nexample of software that is malleable like clay.  And this sets certain\nrequirements for the underlying architecture of it that are actually in many\nways setting certain constraints on the system that really need to go into the\ncore design.\nSo for instance, Piâ€™s underlying AI SDK is written so that a session can really\ncontain many different messages from many different model providers. It\nrecognizes that the portability of sessions is somewhat limited between model\nproviders and so it doesnâ€™t lean in too much into any model-provider-specific\nfeature set that cannot be transferred to another.\nThe second is that in addition to the model messages it maintains custom\nmessages in the session files which can be used by extensions to store state or\nby the system itself to maintain information that either not at all is sent to\nthe AI or only parts of it.\nBecause this system exists and extension state can also be persisted to disk, it\nhas built-in hot reloading so that the agent can write code, reload, test it and\ngo in a loop until your extension actually is functional.  It also ships with\ndocumentation and examples that the agent itself can use to extend itself.  Even\nbetter: sessions in Pi are trees.  You can branch and navigate within a session\nwhich opens up all kinds of interesting opportunities such as enabling workflows\nfor making a side-quest to fix a broken agent tool without wasting context in\nthe main session.  After the tool is fixed, I can rewind the session back to\nearlier and Pi summarizes what has happened on the other branch.\nThis all matters because for instance if you consider how MCP works, on most\nmodel providers, tools for MCP, like any tool for the LLM, need to be loaded\ninto the system context or the tool section thereof on session start.  That\nmakes it very hard to impossible to fully reload what tools can do without\ntrashing the complete cache or confusing the AI about how prior invocations work\ndifferently.\nTools Outside The Context\nAn extension in Pi can register a tool to be available to the LLM to call and\nevery once in a while I find this useful. For instance, despite my criticism of\nhow Beads is implemented, I do think that giving an agent access to a to-do list\nis a very useful thing. And I do use an agent-specific issue tracker that works\nlocally that I had my agent build itself. And because I wanted the agent to also\nmanage to-dos, in this particular case I decided to give it a tool rather than a\nCLI.  It felt appropriate for the scope of the problem and it is currently the\nonly additional tool that Iâ€™m loading into my context.\nBut for the most part all of what Iâ€™m adding to my agent are either skills or\nTUI extensions to make working with the agent more enjoyable for me.  Beyond\nslash commands, Pi extensions can render custom TUI components directly in the\nterminal: spinners, progress bars, interactive file pickers, data tables,\npreview panes.  The TUI is flexible enough that Mario proved you can run Doom\nin it.  Not practical,\nbut if you can run Doom, you can certainly build a useful dashboard or debugging\ninterface.\nI want to highlight some of my extensions to give you an idea of whatâ€™s\npossible.  While you can use them unmodified, the whole idea really is that you\npoint your agent to one and remix it to your heartâ€™s content.\n/answer\nI donâ€™t use plan mode.  I encourage the agent\nto ask questions and thereâ€™s a productive back and forth.  But I donâ€™t like\nstructured question dialogs that happen if you give the agent a question tool.\nI prefer the agentâ€™s natural prose with explanations and diagrams interspersed.\nThe problem: answering questions inline gets messy.  So /answer reads the\nagentâ€™s last response, extracts all the questions, and reformats them into a\nnice input box.\n\n/todos\nEven though I criticize Beads for its\nimplementation, giving an agent a to-do list is genuinely useful.  The /todos\ncommand brings up all items stored in .pi/todos as markdown files.  Both the\nagent and I can manipulate them, and sessions can claim tasks to mark them as in\nprogress.\n\n/review\nAs more code is written by agents, it makes little sense to throw unfinished\nwork at humans before an agent has reviewed it first.  Because Pi sessions are\ntrees, I can branch into a fresh review context, get findings, then bring fixes\nback to the main session.\n\nThe UI is modeled after Codex which provides easy to review commits, diffs,\nuncommitted changes, or remote PRs.  The prompt pays attention to things I care\nabout so I get the call-outs I want (eg: I ask it to call out newly added\ndependencies.)\n/control\nAn extension I experiment with but donâ€™t actively use.  It lets one Pi agent send\nprompts to another.  It is a simple multi-agent system without complex\norchestration which is useful for experimentation.\n/files\nLists all files changed or referenced in the session.  You can reveal them in\nFinder, diff in VS Code, quick-look them, or reference them in your prompt.\nshift+ctrl+r quick-looks the most recently mentioned file which is handy when\nthe agent produces a PDF.\nOthers have built extensions too: Nicoâ€™s subagent\nextension and\ninteractive-shell which\nlets Pi autonomously run interactive CLIs in an observable TUI overlay.\nSoftware Building Software\nThese are all just ideas of what you can do with your agent.  The point of it\nmostly is that none of this was written by me, it was created by the agent to my\nspecifications.  I told Pi to make an extension and it did.  There is no MCP, there are\nno community skills, nothing.  Donâ€™t get me wrong, I use tons of skills.  But\nthey are hand-crafted by my clanker and not downloaded from anywhere.  For\ninstance I fully replaced all my CLIs or MCPs for browser automation with a\nskill that just uses\nCDP.\nNot because the alternatives donâ€™t work, or are bad, but because this is just\neasy and natural.  The agent maintains its own functionality.\nMy agent has quite a few\nskills and crucially\nI throw skills away if I donâ€™t need them.  I for instance gave it a skill to\nread Pi sessions that other engineers shared, which helps with code review.  Or\nI have a skill to help the agent craft the commit messages and commit behavior I\nwant, and how to update changelogs.  These were originally slash commands, but\nIâ€™m currently migrating them to skills to see if this works equally well.  I\nalso have a skill that hopefully helps Pi use uv rather than pip, but I also\nadded a custom extension to intercept calls to pip and python to redirect\nthem to uv instead.\nPart of the fascination that working with a minimal agent like Pi gave me is\nthat it makes you live that idea of using software that builds more software.\nThat taken to the extreme is when you remove the UI and output and connect it\nto your chat.  Thatâ€™s what OpenClaw does and given its tremendous growth,\nI really feel more and more that this is going to become our future in one\nway or another.\n\n\n\nhttps://x.com/steipete/status/2017313990548865292â†©"
    },
    {
      "title": "garymarcus.substack.com: Rumors of AGIâ€™s arrival have been greatly exaggerated ",
      "url": "https://garymarcus.substack.com/p/rumors-of-agis-arrival-have-been",
      "source": "garymarcus.substack.com",
      "time": "2026-02-20T10:18:08.814301",
      "raw_desc": ""
    },
    {
      "title": "garymarcus.substack.com: We URGENTLY need a federal law forbidding AI from impersonating humans",
      "url": "https://garymarcus.substack.com/p/we-urgently-need-a-federal-law-forbidding",
      "source": "garymarcus.substack.com",
      "time": "2026-02-20T10:18:08.814301",
      "raw_desc": ""
    },
    {
      "title": "garymarcus.substack.com: Irony, Irony laced with Karma, and Terror",
      "url": "https://garymarcus.substack.com/p/irony-irony-laced-with-karma-and",
      "source": "garymarcus.substack.com",
      "time": "2026-02-20T10:18:08.814301",
      "raw_desc": ""
    },
    {
      "title": "xeiaso.net: Anubis v1.25.0: Necron",
      "url": "https://github.com/TecharoHQ/anubis/releases/tag/v1.25.0",
      "source": "xeiaso.net",
      "time": "2026-02-20T10:18:08.865026",
      "raw_desc": ""
    },
    {
      "title": "xeiaso.net: The Discourse has been Automated",
      "url": "https://xeiaso.net/notes/2026/the-discourse-has-been-automated/",
      "source": "xeiaso.net",
      "time": "2026-02-20T10:18:08.865026",
      "raw_desc": ""
    },
    {
      "title": "xeiaso.net: Matrix ain't it chief",
      "url": "https://xeiaso.net/notes/2026/dont-use-matrix/",
      "source": "xeiaso.net",
      "time": "2026-02-20T10:18:08.865026",
      "raw_desc": ""
    },
    {
      "title": "johndcook.com: Visualizing orbital velocity",
      "url": "https://www.johndcook.com/blog/2026/02/16/hodograph/",
      "source": "johndcook.com",
      "time": "2026-02-20T10:18:08.876016",
      "raw_desc": ""
    },
    {
      "title": "johndcook.com: Race between primes of the forms 4k + 1 and 4k + 3",
      "url": "https://www.johndcook.com/blog/2026/02/15/chebyshev-bias/",
      "source": "johndcook.com",
      "time": "2026-02-20T10:18:08.876016",
      "raw_desc": ""
    },
    {
      "title": "johndcook.com: Wagonâ€™s algorithm in Python",
      "url": "https://www.johndcook.com/blog/2026/02/14/wagons-algorithm-in-python/",
      "source": "johndcook.com",
      "time": "2026-02-20T10:18:08.876016",
      "raw_desc": ""
    },
    {
      "title": "matklad.github.io: Diagnostics Factory",
      "url": "https://matklad.github.io/2026/02/16/diagnostics-factory.html",
      "source": "matklad.github.io",
      "time": "2026-02-20T10:18:08.937078",
      "raw_desc": "Diagnostics Factory\nFeb 16, 2026\n\nIn\nError Codes For Control Flow,\nI explained that Zigâ€™s strongly-typed error codes solve the â€œhandlingâ€ half of error management,\nleaving â€œreportingâ€ to the users. Today, I want to describe my personal default approach to\nthe reporting problem, that is, showing the user a useful error message.\nThe approach is best described in the negative: avoid thinking about error payloads, and what\nthe type of error should be. Instead, provide a set of functions for constructing errors.\nTo give a concrete example, in TigerBeetleâ€™s\ntidy.zig\n(a project-specific linting script, another useful meta-pattern), we define errors as follows:\n\nconst Errors = struct {\n    pub fn add_long_line(\n        errors: *Errors,\n        file: SourceFile,\n        line_index: usize,\n    ) void { ... }\n\n    pub fn add_banned(\n        errors: *Errors,\n        file: SourceFile,\n        offset: usize,\n        banned_item: []const u8,\n        replacement: []const u8,\n    ) void { ... }\n\n    pub fn add_dead_declaration(...) void { ... }\n\n    ...\n};\n\nand the call-site looks like this:\n\nfn tidy_file(file: SourceFile, errors: *Errors) void {\n    // ...\n    var line_index: usize = 0;\n    while (lines.next()) |line| : (line_index += 1) {\n        const line_length = line_length(line);\n        if (line_length > 100 and !contains_url(line)) {\n            errors.add_long_line(file, line_index);\n        }\n    }\n}\n\nIn this case, I collect multiple errors so I donâ€™t return right away. Fail fast would look like\nthis:\n\nerrors.add_long_line(file, line_index);\nreturn error.Tidy;\n\nNote that the error code is intentionally independent of the specific error produced.\n\nSome interesting properties of the solution:\n\n\nThe error representation is a set of constructor functions, the calling code doesnâ€™t care what\nactually happens inside. This is why the error factory is my default solution â€” I donâ€™t have\nto figure out up-front what Iâ€™ll do with the errors, and I can change my mind later.\n\n\nThereâ€™s a natural place to convert information from the form available at the place where we emit\nthe error to a form useful for the user. In add_banned above, the caller passes in a absolute\noffset in a file, and it is resolved to line number and column inside (tip: use line_index for\n0-based internal indexes, and line_number for user-visible 1-based ones). Contrast this with a\ntraditional error as sum-type approach, where thereâ€™s a sharp syntactic discontinuity between\nconstructing a variant directly and calling a helper function.\n\n\nThis syntactic uniformity in turn allows easily grepping for all error locations:\nrg 'errors.add_'.\n\n\nSimilarly, thereâ€™s one central place that enumerates all possible errors (which is either a\nbenefit or a drawback).\n\n\nA less trivial property is that this structure enables polymorphism. In fact, in the tidy.zig\ncode, there are two different representations of errors. When running the script, errors are\ndirectly emitted to stderr. But when testing it, errors are collected into an in-memory buffer:\n\npub fn add_banned(\n    errors: *Errors,\n    file: SourceFile,\n    offset: usize,\n    banned_item: []const u8,\n    replacement: []const u8,\n) void {\n    errors.emit(\n        \"{s}:{d}: error: {s} is banned, use {s}\\n\",\n        .{\n            file.path, file.line_number(offset),\n            banned_item, replacement,\n        },\n    );\n}\n\nfn emit(\n    errors: *Errors,\n    comptime fmt: []const u8,\n    args: anytype,\n) void {\n    comptime assert(fmt[fmt.len - 1] == '\\n');\n    errors.count += 1;\n    if (errors.captured) |*captured| {\n        captured.writer(errors.gpa).print(fmt, args)\n            catch @panic(\"OOM\");\n    } else {\n        std.debug.print(fmt, args);\n    }\n}\n\nThere isnâ€™t a giant union(enum) of all errors, because itâ€™s not needed for the present use-case.\nThis pattern can be further extended to a full-fledged diagnostics framework with error builders,\nspans, ANSI colors and such, but that is tangential to the main idea here: even when â€œprogramming in\nthe smallâ€, it might be a good idea to avoid constructing enums directly, and mandate an\nintermediate function call.\n\nTwo more meta observations here:\nFirst, the entire pattern is of course the expression of duality between a sum of two types and a\nproduct of two functions (the visitor pattern)\n\nfn foo() -> Result<T, E>;\n\nfn bar(ok: impl FnOnce(T), err: impl FnOnce(E));\n\n\nenum Result<T, E> {\n    Ok(T),\n    Err(E),\n}\n\ntrait Result<T, E> {\n    fn ok(self, T);\n    fn err(self, E);\n}\n\nSecond, every abstraction is a thin film separating two large bodies of code. Any interface has\ntwo sides, the familiar one presented to the user, and the other, hidden one, presented to the\nimplementor. Often, default language machinery pushes you towards using the same construct for both\nbut that can be suboptimal. Itâ€™s natural for the user and the provider of the abstraction to\ndisagree on the optimal interface, and to evolve independently. Using a single big enum for errors\ncouples error emitting and error reporting code, as they have to meet in the middle. In contrast,\nthe factory solution is optimal for producer (they literally just pass whatever they already have on\nhand, without any extra massaging of data), and is flexible for consumer(s)."
    },
    {
      "title": "matklad.github.io: Justifying text-wrap: pretty",
      "url": "https://matklad.github.io/2026/02/14/justifying-text-wrap-pretty.html",
      "source": "matklad.github.io",
      "time": "2026-02-20T10:18:08.940509",
      "raw_desc": "Justifying text-wrap: pretty\nFeb 14, 2026\n\n\nSomething truly monumental happened in the world of software development in 2025. Safari shipped a\nreasonable implementation of text-wrap: pretty:\nhttps://webkit.org/blog/16547/better-typography-with-text-wrap-pretty/. We are getting\ncloser and closer to the cutting-edge XV-century technology. Beautiful paragraphs!\n\n\n\nWe are not quite there yet, hence the present bug report.\n\nA naive way to break text into lines to form a paragraph of a given width is greediness: add the\nnext word to the current line if it fits, otherwise start a new line. The result is unlikely to be\npretty â€” sometimes it makes sense to try to squeeze one more word on a line to make the lines more\nbalanced overall. Johannes Gutenberg did this sort of thing manually, to produce a beautiful page\nabove. In 1981, Knuth and Plass figured out a way to teach computer to do this, using dynamic\nprogramming, for line breaking in TeX.\nInexplicably, until 2025, browsers stuck with the naive greedy algorithm, subjecting generations of\nweb users to ugly typography. To be fair, the problem in a browser is harder version than the one\nsolved by Gutenberg, Plass, and Knuth. In print, the size of the page is fixed, so you can compute\noptimal line breaking once, offline. In the web context, the window width is arbitrary and even\nchanges dynamically, so the line-breaking has to be â€œonlineâ€. On the other hand, XXI century\nbrowsers have a bit more compute resources than we had in 1980 or even 1450!\n\nMaking lines approximately equal in terms of number of characters is only half-way through towards a\nbeautiful paragraph. No matter how you try, the length wonâ€™t be exactly the same, so, if you want\nboth the left and the right edges of the page to be aligned, you also need to fudge the spaces\nbetween the words a bit. In CSS,\ntext-wrap: pretty\nasks the browser to select line breaks in an intelligent way to make lines roughly equal, and\ntext-align: justify\nadjusts whitespace to make them equal exactly.\nAlthough Safari is the first browser to ship a non-joke implementation of text-wrap, the\ncombination with text-align looks ugly, as you can see in this very blog post. To pin the ugliness\ndown, the whitespace between the words is blown out of proportion. Hereâ€™s the same justified\nparagraph with and without text-wrap: pretty:\n\n\n\n\nThe paragraph happens to look ok with greedy line-breaking. But the â€œsmartâ€ algorithm decides to add\nan entire line to it, which requires inflating all the white space proportionally. By itself, either\nof\n\np {\n    text-wrap: pretty;\n    text-align: justify;\n}\n\nlooks alright. Itâ€™s just the combination of the two that is broken.\n\nThis behavior is a natural consequence of implementation. My understanding is that the dynamic\nprogramming scoring function aims to get each line close to the target width, and is penalized for\ndeviations. Crucially, the actual max width of a paragraph is fixed: while a line can be arbitrary\nshorter, it canâ€™t be any longer, otherwise itâ€™ll overflow. For this reason, the dynamic programming\nsets the target width to be a touch narrower than the paragraph. That way, itâ€™s possible to both\nunder and overshoot, leading to better balance overall. As per\noriginal article:\n\nThe browser aims to wrap each line sooner than the maximum limit of the text box. It wraps\nwithin the range, definitely after the magenta line, and definitely before the red line.\n\n\n\n\n\nBut if you subsequently justify all the way to the red line, the systematic overshoot will manifest\nitself as too wide inter-word space!\nWebKit devs, you are awesome for shipping this feature ahead of everyone else, please fix this small\nwrinkle such that I can make my blog look the way I had intended all along ;-)"
    },
    {
      "title": "matklad.github.io: Programming Aphorisms",
      "url": "https://matklad.github.io/2026/02/11/programming-aphorisms.html",
      "source": "matklad.github.io",
      "time": "2026-02-20T10:18:08.950132",
      "raw_desc": "Programming Aphorisms\nFeb 11, 2026\n\nA meta programming post â€” looking at my thought process when coding and trying to pin down what is\nprogramming â€œknowledgeâ€. Turns out, a significant fraction of that is just reducing new problems to\na vocabulary of known tricks. This is a personal, descriptive post, not a prescriptive post for you.\nIt starts with a question posted on Ziggit. The background here is that Zig is in the process of\nremoving ambient IO capabilities. Currently, you can access program environment from anywhere via\nstd.process.getEnvVarOwned.\nIn the next Zig version, youâ€™ll have to thread\nstd.process.Environ.Map\nfrom main down to every routine that needs access to the environment. In this userâ€™s case, they have\na readHistory function which used to look up the path to the history file in the environment, and\nthey are wondering how to best model that in the new Zig. The options on the table are:\n\npub fn readHistory(\n    io: std.Io,\n    alloc: Allocator,\n    file: std.Io.File,\n) ReadHistoryError!void;\n\npub fn readHistory(\n    io: std.Io,\n    alloc: Allocator,\n    maybe_environ_map: ?*std.process.Environ.Map,\n) ReadHistoryError!void;\n\npub fn readHistory(\n    io: std.Io,\n    alloc: Allocator,\n    maybe_absolute_path: ?[]const u8,\n    maybe_environ_map: ?*std.process.Environ.Map,\n) ReadHistoryError!void;\n\nMy starting point would instead be this:\n\npub const HistoryOptions = struct {\n    file: []const u8,\n\n    pub fn from_environment(\n        environment: *const std.process.Environ.Map,\n    ) HistoryOptions;\n};\n\npub fn readHistory(\n    io: std.Io,\n    gpa: Allocator,\n    options: HistoryOptions,\n) ReadHistoryError!void;\n\nIn terms of meta programming, what I find fascinating is that this, for me, is both immediate (I\ndonâ€™t have to think about it), but also is clearly decomposable into multiple factoids Iâ€™ve\naccumulated before. Hereâ€™s a deconstruction of what I did here, the verbal â€œlabelsâ€ I use to think\nabout what I did, and where I had learned to do that:\nFirst, I â€œraised the abstraction levelâ€ by giving it a name and a type (HistoryOptions). This\nis a rare transformation which I learned and named myself. Naming is important for my thinking and\ncommunicating process. â€œLetâ€™s raise abstraction levelâ€ is a staple code review comment of mine.\nSecond, I avoided â€œmidlayer mistakeâ€ by making sure that every aspect of options is\nuser-configurable. Easy to do in Zig, where all fields are public. I learned about\nmidlayer mistake from a GitHub comment by\nJosh Triplett.\nThird, I provided a â€œshortcutâ€, the\nfrom_environment\nconvenience function that cuts across abstraction layers. I learned the â€œshortcutâ€ aphorism from\nDjango Views â€” The Right Way.\nGermane to the present article, I read that post a decade after I had touched Django the last time.\nIt was useless to me on the object level. On the meta level, reading the article solidified and\nnamed several programming tricks for me. See reverberations in\nHow to Make a ğŸ’¡?.\nFourth, I instinctively renamed alloc to â€œgpaâ€ (in opposition to â€œarenaâ€), the naming I spotted\nin the Zig compiler.\nFifth, I named the configuration parameter â€œoptionsâ€, not config, props or params, a naming\nscheme I learned at TigerBeetle.\nSixth, I made sure that the signature follows â€œpositional DIâ€ scheme. Arguments that are\ndependencies, resources with unique types are injected positionally (and have canonical names like\nio or gpa). Arguments that directly vary the behavior of function (as opposed to affecting\ntransitive callees) are passed by name, in the Options struct.\nTo be specific, I donâ€™t claim that my snippet is the right way to do this! I have no idea, as I\ndonâ€™t have access to the full context. Rather, if I were actually solving the problem, the snippet\nabove would be my initial starting point for further iteration.\nNote that I also donâ€™t explain why I am doing the above six things, I only name them and point at\nthe origin. Actually explaining the why would take a blog post of its own for every one of them.\nAnd this is I think the key property of my thought process â€” I have a bag of tricks, where the\ntricks are named. Inside my mind, this label points both to the actual trick (code to type),\nas well as a justification for it (in what context that would be a good trick to use).\nAnd I use these tricks all the time, literally! Just answering in passing to a forum comment makes\nme grab a handful! A lot of my knowledge is structured like a book of coding aphorisms.\n\nMeta meta â€” how come I have acquired all those tricks? I read voraciously, random commits, issues,\njumping enthusiastically into rabbit holes and going on wiki trips. The key skill here is\nrecognizing an aphorism once you see it. Reading Ziggit is part of trick-acquisition routine for\nme. Having learned the trick, I remember it, where â€œrememberingâ€ is an act of active recall at the\nopportune moment. This recall powers â€œhorizontal gene transferâ€ across domains, stealing shortcuts\nfrom Django and midlayer mistake from the kernel. Did you notice that applying â€œhorizontal gene\ntransferâ€ to the domain of software engineering tacit knowledge is horizontal gene transfer? When\nentering a new domain, I actively seek out the missing tricks. I am relatively recent in Zig, but\nall the above tricks are either Zig native, or at least Zig adapted. Every once in a while, I\nâ€œinventâ€ a trick of my own. For example, â€œpositional DIâ€ is something I only verbalized last year.\nThis doesnâ€™t mean I hadnâ€™t been doing that before, just that the activity wasnâ€™t mentally labeled as\na separate thing you can deliberately do. I had the idea, now I also have an aphorism."
    },
    {
      "title": "overreacted.io: A Social Filesystem",
      "url": "https://overreacted.io/a-social-filesystem/",
      "source": "overreacted.io",
      "time": "2026-02-20T10:18:09.070653",
      "raw_desc": ""
    },
    {
      "title": "overreacted.io: Introducing RSC Explorer",
      "url": "https://overreacted.io/introducing-rsc-explorer/",
      "source": "overreacted.io",
      "time": "2026-02-20T10:18:09.070653",
      "raw_desc": ""
    },
    {
      "title": "overreacted.io: Hire Me in Japan",
      "url": "https://overreacted.io/hire-me-in-japan/",
      "source": "overreacted.io",
      "time": "2026-02-20T10:18:09.070653",
      "raw_desc": ""
    },
    {
      "title": "skyfall.dev: Good riddance, 4o",
      "url": "https://mahadk.com/posts/4o/",
      "source": "skyfall.dev",
      "time": "2026-02-20T10:18:09.137986",
      "raw_desc": ""
    },
    {
      "title": "skyfall.dev: The UK paid Â£4.1 million for a bookmarks site",
      "url": "https://mahadk.com/posts/ai-skills-hub/",
      "source": "skyfall.dev",
      "time": "2026-02-20T10:18:09.137986",
      "raw_desc": ""
    },
    {
      "title": "skyfall.dev: Using the M1 MacBook Air in 2026",
      "url": "https://mahadk.com/posts/m1-mba/",
      "source": "skyfall.dev",
      "time": "2026-02-20T10:18:09.137986",
      "raw_desc": ""
    },
    {
      "title": "terriblesoftware.org: Why Am I Doing the Thinking for You?",
      "url": "https://terriblesoftware.org/2026/02/02/why-am-i-doing-the-thinking-for-you/",
      "source": "terriblesoftware.org",
      "time": "2026-02-20T10:18:09.361328",
      "raw_desc": ""
    },
    {
      "title": "terriblesoftware.org: Why I Still Write Code as an Engineering Manager",
      "url": "https://terriblesoftware.org/2026/01/22/why-i-still-write-code-as-an-engineering-manager/",
      "source": "terriblesoftware.org",
      "time": "2026-02-20T10:18:09.361328",
      "raw_desc": ""
    },
    {
      "title": "terriblesoftware.org: Life Happens at 1x Speed",
      "url": "https://terriblesoftware.org/2026/01/08/life-happens-at-1x-speed/",
      "source": "terriblesoftware.org",
      "time": "2026-02-20T10:18:09.361328",
      "raw_desc": ""
    },
    {
      "title": "timsh.org: Scam Telegram: Uncovering a network of groups spreading crypto drainers",
      "url": "https://timsh.org/scam-telegram-investigation/",
      "source": "timsh.org",
      "time": "2026-02-20T10:18:09.457990",
      "raw_desc": ""
    },
    {
      "title": "timsh.org: Why you should self-host your (vibecoded) app",
      "url": "https://timsh.org/why-you-should-self-host/",
      "source": "timsh.org",
      "time": "2026-02-20T10:18:09.457990",
      "raw_desc": ""
    },
    {
      "title": "timsh.org: Switching to Claude Code + VSCode inside Docker",
      "url": "https://timsh.org/claude-inside-docker/",
      "source": "timsh.org",
      "time": "2026-02-20T10:18:09.457990",
      "raw_desc": ""
    },
    {
      "title": "derekthompson.org: The Affordability Curse",
      "url": "https://www.theatlantic.com/ideas/2025/11/democrats-cost-of-living-affordability-platform/684847/?utm_source=feed",
      "source": "derekthompson.org",
      "time": "2026-02-20T10:18:09.464393",
      "raw_desc": "To understand what just happened in this weekâ€™s electionsâ€”notably Zohran Mamdaniâ€™s win in New York City, Mikie Sherrillâ€™s win in New Jersey, and Abigail Spanbergerâ€™s win in Virginiaâ€”wind back the clock five years.In 2020, Joe Biden won by promising that he could restore normalcy to American life. That did not happen. As the biological emergency of the coronavirus pandemic wound down, the economic emergency (inflation) took off. An affordability crisis broke out around the world. The public revolted. Last year, practically every incumbent party in every developed country lost ground at the ballot box.So it went in the United States. In 2024, Donald Trump won an â€œaffordability election.â€ Iâ€™m calling it that because affordability is what Trumpâ€™s voters said they wanted more of. Gallup found that the economy was the only issue that a majority of voters considered â€œextremely important.â€ A CBS analysis of exit-poll data found that eight in 10 of those who said they were worse off financially compared with four years ago backed Trump. The APâ€™s 120,000-respondent VoteCast survey found that voters who cited inflation as their most important factor were almost twice as likely to back Trump.So Trump won. And for the second straight election, the president has violated his mandate to restore normalcy. Elected to be an affordability president, Trump has governed as an authoritarian dilettante. He has raised tariffs without the consultation of Congress, openly threatened comedians who made jokes about him, pardoned billionaires who gave him and his family money, arrested people without due process, overseen the unconstitutional obliteration of the federal-government workforce, and, with the bulldozing of the White House East Wing, provided an admirably vivid metaphor for his general approach to governance, norms, and decorum.[Read: â€˜None of this is good for Republicansâ€™]A recent NBC poll asked voters whether they thought Trump had lived up to their expectations for getting inflation under control and improving the cost of living. Only 30 percent said yes. It was his lowest number for any issue polled. The affordability issue, which seemed to be a rocket exploding upwards 12 months ago, now looks more like a bomb to which the Republican Party finds itself tightly strapped.So again, we have an affordability election on our hands.On the surface, Mamdani, Spanberger, and Sherrill emerged victorious in three very different campaigns. Mamdani defeated an older Democrat in an ocean-blue metropolis. In Virginia, Spanberger crushed a bizarre Republican candidate in a state that was ground zero for DOGE cuts. In New Jersey, Sherrillâ€”whose victory margin was the surprise of the eveningâ€”romped in a state that had been sliding toward the Republican column.Despite these cosmetic differences, what unified the three victories was the Democratic candidatesâ€™ ability to turn the affordability curse against the sitting president, transforming Republicansâ€™ 2024 advantage into a 2025 albatross. Hereâ€™s Shane Goldmacher at The New York Times:\nDemocratic victories in New Jersey and Virginia were built on promises to address the sky-high cost of living in those states while blaming Mr. Trump and his allies for all that ails those places. In New York City, the sudden rise of Mayor-elect Zohran Mamdani, the democratic socialist with an ambitious agenda to lower the cost of living, put a punctuation mark on affordability as a political force in 2025.\nEach candidate arguably got more out of affordability than any other approach. Mamdaniâ€™s focus on the cost of living in New Yorkâ€”which included some genuinely brilliant ads on, for example, â€œHalalflationâ€ and street-vendor permitsâ€”has been widely covered. Less ballyhooed, but just as important, is that Spanberger and Sherrill also found that the affordability message had the biggest bang for the buck in their own advertisements. An analysis shared with me by the polling and data firm Blue Rose Research found that â€œthe best-testing ads in both Virginia and New Jersey focused on affordability, tying rising costs to Trump and Congressional Republicans.â€Tuesday night showed what affordability can be for the Democratic Partyâ€”not a policy, but a prompt, an opportunity for Democrats to fit different messages under the same tentpole while contributing to a shared national party identity: The presidentâ€™s a crook, and we care about the cost of living. In New York City, Mamdani won renters by 24 percentage points with a specific promise: freeze the rent. In New Jersey, Sherrill won with a day-one pledge to declare a state of emergency on utility costs, which would allow her to halt rates and delete red tape that holds back energy generation. (The opening line of her mission statement: â€œLife in New Jersey is too expensive and every single New Jerseyan who pays the bills knows it.â€) In Virginia, Spanberger went another way, relentlessly blaming rising costs on Trump.Whatâ€™s notable is not just what the above messages have in common but what they donâ€™t. Sherrill focused on utility costs, whereas Mamdani focused on rent. Mamdani ran a socialist campaign to energize a young left-wing electorate, whereas Spanbergerâ€™s task was to win a purple state with an outgoing Republican governor. Each candidate answered the affordability prompt with a message tailored to the electorate: Affordability is a big tent.The affordability message was especially successful at bringing young voters back to the Democratic fold. After the 2024 election, it looked like young people were listing to the right. Tuesday night was not the ideal test of that theory, because off-year elections tend to have a smaller and more educated (and therefore more naturally anti-Trump) electorate. But the pollster John Della Volpe reported that young voters â€œanchored the Democratic turnaroundâ€ in Virginia, where 18-to-29-year-olds delivered a 35-point margin for Spanberger, the largest for Democrats since 2017.Itâ€™s easy to understand why young voters would appreciate an emphasis on the cost of living. Just this week, the National Association of Realtors announced that the median age of first-time U.S. homebuyers has jumped to a new record of 40. â€œZohranâ€™s campaign centered cost-of-living issues, and he at least appeared consistently willing to look for answers wherever they may present themselves,â€ Daniel Racz, a 23-year-old sport-data analyst who lives in New York, told me. â€œI think of his mentions of the history of sewer socialism, proposed trial runs of public grocery stores on an experimental basis, and his past free-bus pilot program, which showcased a political curiosity grounded in gathering information to improve his constituentsâ€™ lives.â€Amanda Litman, a co-founder and the president of Run for Something, oversees a national recruitment effort to help progressives run for downballot office. On Tuesday, the organization had 222 candidates in general elections across the country. â€œNearly every candidate who won an election for municipal or state legislative office was talking about affordability, especially as it relates to housing,â€ she told me. â€œHousing is the No. 1 issue weâ€™ve seen people bring up as a reason to run for office this year.â€The affordability approach has several strengths. Because it is a prompt rather than a policy, it allows Democrats to be organized in their thematic positioning but heterodox in their policies. A socialist can run on affordability in a blue city and win with socialist policies; a moderate can run on affordability in a purple state and win with the sort of supply-side reforms for housing and energy that animate the abundance movement. At a time when Democrats are screaming at one another online and off about populism versus moderation, the affordability tent allows them to be diverse yet united: They can run on tying Trump to the affordability crisis while creating messages fit for their respective electorates.[Read: An antidote to shamelessness]This next bit is a little speculative, but another advantage of centering affordability may be that it is easier for members of a political coalition to negotiate on material politics than on post-material politics. Put differently, economic disagreements within a group are more likely to produce debate and even compromise, whereas cultural disagreements are more likely to produce purity tests and excommunication. If a YIMBY left-centrist and a democratic socialist disagree about the correct balance of price controls and supply-side reforms to reduce housing inflation in New York City, that might lead to a perfectly pleasant conversation. But perfectly pleasant conversations between political commentators about, say, ICE deportations or trans women in college sports donâ€™t seem common. If this is true, it would suggest that the spotlight of Democratic attention shifting toward affordability might ameliorate the culture of progressive purity tests in a way that would make for a bigger tent.Affordability politics also poses a distinct challenge. At the national level, Democrats do not have their hands on the price levers, and they wonâ€™t for at least four more calendar years. Even if they did, the best ways to reduce prices at the national level include higher interest rates (painful), meaningful spending cuts (excruciating), or a national tax increase (dial 911). Even at the local level, affordability politics in an age of elevated inflation, rapidly growing AI, and complex impediments to affordable housing can easily promise too muchâ€”or, to be more exact, offer a set of dangerously falsifiable promises.Affordability politics thrives because of the specificity and clarity of its pledge: Prices are too high; Iâ€™ll fix it if you give me power. But politics isnâ€™t just about the words you put on your bumper stickers; itâ€™s about what you do if the bumper stickers work. Building houses takes time, even after reducing barriers to development and improving access to financing. Actually lowering prices can take even longer. Energy inflation is a bear of a problem, with transmission prices rising and data-center construction exploding. After Americans learn whose affordability messages win at the ballot box, theyâ€™ll learn whose affordability policies actually work and (perhaps) keep them in office.Affordability is good politics, and a Democratic Party that focuses on affordability at the national level, and supports motley approaches to solving the cost-of-living crisis at the local level, is in a strong position going into 2026. But saying the word affordability over and over doesnâ€™t necessarily guarantee good policy outcomes. In fact, it doesnâ€™t guarantee anything. Which is why at some point on the road back to relevance, the Democratic Party needs to become obsessed with not only winning back power but also governing effectively in the places where they have it.This article was adapted from a post on Derek Thompsonâ€™s Substack."
    },
    {
      "title": "derekthompson.org: The Era of Step-on-a-Rake Capitalism",
      "url": "https://www.theatlantic.com/ideas/archive/2025/09/trump-economic-pain-strategy/684166/?utm_source=feed",
      "source": "derekthompson.org",
      "time": "2026-02-20T10:18:09.469220",
      "raw_desc": "Sign up for Trumpâ€™s Return, a newsletter featuring coverage of the second Trump presidency.Is Donald Trump a staunch capitalist, a secret socialist, a blend of the two, or none of the above? Depending on the day, itâ€™s hard to tell.Some of his initiatives are pure Ronald Reagan, such as his corporate-income tax cuts and deregulation efforts targeted at oil and gas. Some of his interventions would impress a Democratic Socialists of America chapter, such as demanding a public stake in Intel, requesting 15 percent of revenues from Nvidiaâ€™s chip sales to China, and securing a â€œgolden shareâ€ of U.S. Steel to retain veto power over its decision making. As for the rest of Trumpâ€™s economic policy, it is a hodgepodge of 19th-century mercantilism, developing-world authoritarianism, and extremely online weirdness. The U.S. tariff rate stands near a 100-year high. When Trump isnâ€™t firing the statisticians who calculate unemployment, heâ€™s waging war against the independent central bank or posting about the fierce urgency of corporate-logo design.To put it simply, or at least as simply as one can: Trumpâ€™s economic agenda is deeply Reaganite and deeply anti-conservative; somewhat capitalist and frequently socialist; declaratively obsessed with â€œAmerican greatnessâ€ yet constantly sidetracked by online outrages that do nothing for the country.So, what is Trumponomics?[From the April 2025 issue: The real goal of the Trump economy]The most interesting answer Iâ€™ve heard is â€œstate capitalism with American characteristics,â€ which The Wall Street Journalâ€™s Greg Ip defined as â€œa hybrid between socialism and capitalism in which the state guides the decisions of nominally private enterprises.â€ This diagnosis makes Trumpâ€™s economic policy seem more evolutionary than revolutionary. In the past 70 years, the U.S. government has frequently intervened in corporate affairs, especially in response to emergencies such as World War II (the Defense Production Act), the Great Recession (the bank bailouts), and COVID (the Paycheck Protection Program). Under Joe Biden, Democrats waded into industrial policy with subsidies for clean energy and semiconductors. By one interpretation, Trumponomics doesnâ€™t stand out in history; itâ€™s just the latest example of the federal government taking a more activist role in directing the economy, especially as we try to compete with the juggernaut of authoritarian China, whose modern development was known as â€œsocialism with Chinese characteristics.â€But Trumponomics is too erratic to deserve any comparison with state capitalism, especially in relation to China. As the author Dan Wang writes in his new book, Breakneck, China is an â€œengineering state,â€ where Beijingâ€™s control over the economy both emerges from long-term planning and radiates outward through millions of local-government representatives. â€œThe core characteristic of Chinaâ€™s state capitalism is discipline,â€ Wang told Ip. â€œTrump is the complete opposite of that.â€Consider, for example, two simple questions: What are Trumpâ€™s tariffs supposed to accomplish, and what are they actually accomplishing? The White House, including the economic adviser Stephen Miran, has repeatedly stressed that higher import taxes will bring back manufacturing and revitalize exports. Neither is happening. Manufacturing output has declined every month since the tariffs were announced, and many firms have explicitly blamed Trumpâ€™s tariffs. Meanwhile, the president recently struck a deal requiring Nvidia and AMD to pay the government 15 percent of revenue on the sale of AI chips to China. The logic is genuinely hard to follow on a week-to-week basis. Promoting exports with global tariffs (which might be illegal) is one thing. Taxing exports (which might also be illegal) is another thing. But taxing imports and exports simultaneously doesnâ€™t really comport with any coherent economic strategy. As the economy lists toward stagflation, the White House is not doing â€œstate capitalismâ€ so much as itâ€™s doing â€œstep-on-a-rake capitalismâ€â€”a tragicomic bungling of economic growth that fails to advance the very objectives it claims to prioritize.The problem with evaluating this administrationâ€™s economic agenda is that Trumponomics is about Trump far more than it is about economics. There is no clear theory of growth steering the U.S. economy, just one manâ€™s desire to colonize every square inch of American attention and experience, which happens to include international markets.Trumponomics, then, is best understood as Trumpâ€™s formula for controlling everything around him, rather than an ideology with a telos. That formula has three main components. The first is declaring an emergency to justify intervention. The second is making threats to force private actors to do his bidding. The third is demanding tribute.All presidents have the power to declare emergencies. None has used this power as frequently as Donald Trump.Since 1981, the typical president has declared about seven national emergencies in each four-year term. In the first six months of his second term, Trump has already declared nine, plus a â€œcrime emergencyâ€ in Washington. Heâ€™s invoked the Alien Enemies Act of 1798 to deport foreigners during a war or invasion, Title X to deploy the National Guard in various cities, and other congressional acts to expedite mining on federal lands. â€œEven when Trump doesnâ€™t declare a legal emergency, he describes crises that justify dramatic action,â€ The New York Timesâ€™ Adam Kushner wrote. At this rate, Trump is on pace to announce 70 emergencies in this administration, which would nearly match the total number of emergencies announced from 1980 to 2025, according to the Brennan Center for Justice.Emergency declarations have been core to Trumpâ€™s economic agenda. Tariffs, the most significant policy initiative of Trumpâ€™s current term, kicked off with an emergency declaration. On February 3, the White House announced its first round of tariffs on Canada, Mexico, and China. Although import taxes are typically the domain of the legislature, Trump as president claimed the authority to tax imports under the International Emergency Economic Powers Act, or IEEPA, because of these countriesâ€™ alleged failure to stop the flow of migrants and fentanyl.The IEEPA is a 1977 law that allows the president to impose financial regulations, such as sanctions or export restrictions, during a national emergency. But no president before Trump ever used IEEPA to tax imported goods. In August, a federal court of appeals struck down the tariffs as unconstitutional, pointing out that IEEPA gives the executive branch authority to regulate imports but not to tax them. Now that net immigration has plummeted to historic lows, it doesnâ€™t even make sense to claim the power to tax imports based on an alleged migration emergency that has, by all accounts, ended. But the White House has said it will fight for the right to impose tariffs all the way to the Supreme Court.I have said before that the No. 1 rule for understanding Trump is that â€œa lot happens under this administration, but a lot un-happens, too.â€ This also is a function of Trumpâ€™s â€œeverything is an emergencyâ€ style of governanceâ€”constantly bending the law into unnatural shapes to justify whatever action the president seeks in the moment.Just as Trump depends on emergency declarations, he also depends on threats. The president creates pain, then demands tribute, at which point he removes the pain.To punish ABC for its negative coverage, Trump threatened to revoke its broadcast license, accepted a $16 million financial tribute from the Walt Disney Company, and then backed down. To punish law firms for litigation against him or his allies, Trump threatened several firms with limited access to government contracts before accepting hundreds of millions of dollars in promised pro bono services to Trump-approved causes. To punish Columbia University for a litany of perceived sins, including its DEI policies, Trump froze hundreds of millions of dollars in federal research funding before the university agreed to pay a large tribute and change its policies.Trump applies the same pain-tribute method to direct international trade and private-firm behavior. In the spring, Trump threatened new tariffs on Japanese and European Union exports. (Pain created.) In response, Japan and the EU agreed to invest more than $1 trillion in the U.S., and Trump himself claimed the authority to direct some of the investment to his favored causes. (Tribute offered.) Then Trump cut both tariff rates by about half. (Pain removed.) Last month, Trump called for Lip-Bu Tan to resign as the chief executive of Intel. (Pain created.) Days later, Tan met with Trump at the White House to work out a deal, and when they emerged, the U.S. government owned 10 percent of his company. (Tribute offered.) Tan remains the CEO of Intel. (Pain removed.)In the aftermath of any one of these events, you might come up with a philosophical justification. You could defend high tariffs because they raise revenue, or you could defend reduced tariffs because they increase the flow of trade among allies. You could defend firing Tan for his alleged Chinese connections and poor performance, or you could defend retaining Tan as long as the U.S. gets a slice of Intel. But you canâ€™t defend all of these opinions at the same time. Each one represents a specific ideological position, and Trumponomicsâ€”outside of a basic distrust of trade and fondness for tariffsâ€”is mostly beyond any ideology. The presidentâ€™s personalist style of politics is optimally designed not to achieve any specific policy outcome but rather to achieve the vanquishing of a counterparty. Tariffs, insults, threats, and Truth Social posts perform a similar function: They create leverage that Trump can use to claim victory, tribute, or both.Trumpâ€™s personalist style of politics thrusts America back to the late 19th century and the Gilded Age, when corruption was so rampant that it was broadly considered the cost of doing business. The intercontinental railroads depended on insider trading and stock manipulation, as the historian Richard White has said. Andrew Carnegie illegally supplied information to politicians in exchange for their protection of his steel monopoly. The big industrialists in rail, oil, and steel would promise congressmen and senators jobs after leaving office if they did the companiesâ€™ bidding.[Annie Lowrey: Trump is a degrowther]Corruption oozes out of this White House as well. In his first six months in office, Trump accepted a luxury jet as a gift from Qatar and solicited family-business investments from several Arab states; countries around the world are now racing to build Trump golf courses and towers in a rather transparent bid for his approval. When a crypto mogul under fraud investigation bought $75 million in Trump-backed tokens, the SEC paused his civil case, citing the â€œpublicâ€™s interest.â€I can imagine a Trump supporter who has somehow made it this far into the essay thinking: You just donâ€™t get it. The Chinese are eating our lunch. Theyâ€™re not just catching up on AI. They make two-thirds of the worldâ€™s electric vehicles, more than three-quarters of its electric batteries, 80 percent of its consumer drones, and 90 percent of its solar panels. They make 13 times more steel than the U.S. and build naval ships several orders of magnitude faster than we do. We need a big, rude state-capitalist authoritarian to stand up to the state-capitalist authoritarian that is China. My response to this is: Okay, maybe, but show me any evidence that, given the choice between helping the U.S. against China or helping himself, Trump will actually choose the former? In his first term, Trump insisted that Congress force TikTok to sell itself to a non-Chinese company. In fact, Iâ€™d agree that the largest news source for Gen Z probably shouldnâ€™t have an intimate legal entanglement with the Chinese Communist Party. Acting under this logic, House Republicans under Biden voted 186â€“25 to force a TikTok sale. But after meeting with an investor in ByteDance, the parent company of TikTok, Trump reversed course and has used his executive power to delay the very TikTok sale that (a) he called for and that (b) Congress has legally mandated.There is no secret plan to help America sell more stuff. If anything, it is American policy itself that has been put up for sale."
    },
    {
      "title": "derekthompson.org: The No. 1 Rule for Understanding Trump",
      "url": "https://www.theatlantic.com/politics/archive/2025/06/trump-self-destructive-agenda/683013/?utm_source=feed",
      "source": "derekthompson.org",
      "time": "2026-02-20T10:18:09.472287",
      "raw_desc": "A useful one-sentence guide to the second Trump administration might go something like this: A lot happens under Donald Trump, but a lot un-happens, too.In the past four months, President Trump has announced tariffs on Canada, paused tariffs on Canada, restarted tariffs on Canada, ruled out tariffs on certain Canadian goods, and then ruled in, and even raised, tariffs on Canadian steel and aluminum.And thatâ€™s just for starters. On April 2, so-called Liberation Day, Trump announced a broader set of tariffs on almost every country in the world. Soon after, the plan was half-suspended. Then Trump announced a new set of elevated tariffs on China, from which he backtracked as well. Next the courts, as often happens, took over the job of erasing the presidentâ€™s previously announced policies. Last week, a trade court struck down the presidentâ€™s entire Liberation Day tariff regime as unconstitutional, only for a federal circuit court to reinstate the tariffs shortly thereafter. Now a higher court has the opportunity to do the funniest thing: undo the undoing of the undoing of the tariffs, which have been in a permanent state of being undone ever since they were created.Got all that? No, you most certainly do not, and neither does anybody else. Economists and corporate executives Iâ€™ve interviewed to understand future tariff policy have communicated to me a combination of confusion, fury, and resignation. Commentators have noticed the chaos too, of course. Observing how frequently Trump seems to back out of his own brinkmanship, the Financial Times columnist Robert Armstrong memorably deemed this trend TACO, or â€œTrump Always Chickens Out.â€[David A. Graham: The TACO presidency]Un-happening doesnâ€™t affect just trade and economic policy. In the realm of foreign policy, the Trump administration paused intelligence sharing with Ukraine after the ignominious on-camera spat between Ukrainian President Volodymyr Zelensky and Vice President J. D. Vance. Trump went further, claiming that Ukraine had started the war and that Zelensky was a dictatorâ€”raising the prospect that the administration was on the verge of explicitly aligning with Russia. Days later, the administration reversed course and resumed intelligence sharing and security assistance. Trump has since attacked Russian President Vladimir Putin for being â€œabsolutely CRAZY!â€Un-happening also affects media, immigration, science, and education policy. Judges have ruled that the administration improperly froze grant money, inappropriately blocked the Associated Press from the White House press pool, and illegally sought to place sanctions on law firms that have done work, or employed lawyers, that Trump found unsuitable. On immigration, judges have blocked several of the administrationâ€™s measures, including its invocation of the Alien Enemies Act to remove migrants and its attempt to bar Harvardâ€™s international students. Federal judges have blocked so much of the Trump agenda that White House Deputy Chief of Staff Stephen Miller has described the constitutional balance of power as a form of â€œjudicial tyranny.â€ â€œI know this is inflammatory,â€ Vance said in an interview with The New York Times, â€œbut I think you are seeing an effort by the courts to quite literally overturn the will of the American people.â€The administrationâ€™s claims to monarchical power are a real threat to Americaâ€™s constitutional order. But its executive orders and policy feints are so haphazard and poorly articulated that they amount to a kind of autocratic takeover written in smudge-able crayon: terrifying, cartoonish, and vulnerable to erasure, all at once.[J. Michael Luttig: The end of rule of law in America]This is not to say that Americans should ignore Trumpâ€™s efforts to make confetti of the Constitution. Rather, when evaluating any one Trump policy, one has to keep front of mind the possibility that it simply wonâ€™t exist by the end of the week. Despite an energetic effort by some right-wing intellectuals to make Trump out to be some kind of 14-dimensional-chess player, his approach doesnâ€™t resemble chess so much as a denial-of-service attack on a functioning government.All this un-happening shows both the upside and the downside of Trumpâ€™s political instincts. The presidentâ€™s slippery relationship to his own policy agenda can serve as a kind of superpower, as Ross Douthat wrote in The New York Times. The TACO reputation is â€œcrucial to Trumpâ€™s political resilience,â€ because â€œthe willingness to swerve and backpedal and contradict himself is a big part of what keeps the president viable.â€ The constant backtracking gives Trump the ability to both bend the Constitution to its breaking point and always step back to claim that â€œanything extreme is also provisional,â€ Douthat wrote. Indeed, Trumpâ€™s approval rating for trade has rebounded since its Liberation Day implosion, according to several polls.Questions of popularity aside, however, businesses tend to prefer certainty over promises and threats that keep disappearing. At some point, Trumpâ€™s pledge to reinvigorate American industry and energy will require fat investments in factories and supply chains. Multi-hundred-million-dollar investments require clear expectations of financial return. Those arenâ€™t going to happen in a world where each policy idea boasts a half-life of 48 hours. Steve Bannon coined one of the most famous Trump-world truisms when he revealed MAGAâ€™s media strategy to â€œflood the zone with shit.â€ Far stranger, however, is the administrationâ€™s insistence on flooding the policy zone with SchrÃ¶dingerâ€™s catsâ€”executive orders and Truth Social posts that exist in a liminal state among existence, nonexistence, and imminent radioactive decay.The substantive problem with the MAGA agenda isnâ€™t just that too much is happening for any median voter to follow; itâ€™s that too much is un-happening for employers, investors, and consumers to know what the hell to do about it."
    },
    {
      "title": "evanhahn.com: Notes from January 2026",
      "url": "https://evanhahn.com/notes-from-january-2026/",
      "source": "evanhahn.com",
      "time": "2026-02-20T10:18:09.552883",
      "raw_desc": ""
    },
    {
      "title": "evanhahn.com: An LLM that's 7500Ã— stupider",
      "url": "https://evanhahn.com/an-llm-thats-7500x-stupider/",
      "source": "evanhahn.com",
      "time": "2026-02-20T10:18:09.552883",
      "raw_desc": ""
    },
    {
      "title": "evanhahn.com: A mental math heuristic to convert between Fahrenheit and Celsius",
      "url": "https://evanhahn.com/fahrenheit-celsius-heuristic/",
      "source": "evanhahn.com",
      "time": "2026-02-20T10:18:09.552883",
      "raw_desc": ""
    },
    {
      "title": "xania.org: 2025 in Review",
      "url": "http://xania.org/202512/2025-in-review?utm_source=feed&utm_medium=rss",
      "source": "xania.org",
      "time": "2026-02-20T10:18:09.837776",
      "raw_desc": ""
    },
    {
      "title": "xania.org: Thank you",
      "url": "http://xania.org/202512/25-thank-you?utm_source=feed&utm_medium=rss",
      "source": "xania.org",
      "time": "2026-02-20T10:18:09.837776",
      "raw_desc": ""
    },
    {
      "title": "xania.org: When compilers surprise you",
      "url": "http://xania.org/202512/24-cunning-clang?utm_source=feed&utm_medium=rss",
      "source": "xania.org",
      "time": "2026-02-20T10:18:09.837776",
      "raw_desc": ""
    },
    {
      "title": "rakhim.exotext.com: Modern UI is clean and invisible? Ha, I wish!",
      "url": "https://rakhim.exotext.com/modern-ui-is-not-invisible",
      "source": "rakhim.exotext.com",
      "time": "2026-02-20T10:18:10.022807",
      "raw_desc": ""
    },
    {
      "title": "rakhim.exotext.com: Alarm is sacred, must not fail, but iOS 26 is wicked",
      "url": "https://rakhim.exotext.com/alarm-is-sacred-but-ios-26-is-wicked",
      "source": "rakhim.exotext.com",
      "time": "2026-02-20T10:18:10.022807",
      "raw_desc": ""
    },
    {
      "title": "rakhim.exotext.com: Examples are the best documentation",
      "url": "https://rakhim.exotext.com/examples-are-the-best-documentation",
      "source": "rakhim.exotext.com",
      "time": "2026-02-20T10:18:10.022807",
      "raw_desc": ""
    },
    {
      "title": "nesbitt.io: Go Modules for Package Management Tooling",
      "url": "https://nesbitt.io/2026/02/19/go-modules-for-package-management-tooling.html",
      "source": "nesbitt.io",
      "time": "2026-02-20T10:18:10.145712",
      "raw_desc": "Iâ€™ve been working on a reusable layer for building ecosystem-agnostic package and supply chain tools in Go: fourteen modules under git-pkgs covering manifest parsing, registry clients, license normalization, platform translation, vulnerability feeds, and more.\nThese are rebuilds of libraries Iâ€™ve written and used in Ruby for years, some going back to Libraries.io and more recently for Ecosyste.ms, which I wrote about previously. I built the Go versions for git-pkgs, a tool for exploring the dependency history of your repositories that compiles to a single binary with no runtime dependencies, which matters for a git subcommand that needs to just work on any machine. When I went looking for Go equivalents of my Ruby libraries, most were either abandoned, incomplete, or only covered a single ecosystem, so I rebuilt them.\nIdentification\npurl\nPackage URL (now ECMA-427) is the standard format for identifying packages across ecosystems. This handles parsing, generation, and type-specific configuration for around 40 ecosystems, including registry URL generation and the reverse: parsing a registry URL back into a PURL.\np, _ := purl.Parse(\"pkg:npm/%40babel/core@7.24.0\")\np.FullName()  // \"@babel/core\"\n\nurl, _ := p.RegistryURL()  // \"https://www.npmjs.com/package/@babel/core\"\n\n// Reverse lookup\np, _ = purl.ParseRegistryURL(\"https://crates.io/crates/serde\")\np.String()  // \"pkg:cargo/serde\"\n\nvers\nVERS is the version range specification that accompanies PURL. Different ecosystems have incompatible range syntaxes: npm uses ^1.2.3, Ruby uses ~> 1.2, Maven uses [1.0,2.0). VERS provides one syntax to normalize everything to.\nIt parses both VERS URIs and native ecosystem syntax, using a mathematical interval model internally to check whether a given version falls within a range:\nr, _ := vers.Parse(\"vers:npm/>=1.0.0|<2.0.0\")\nr.Contains(\"1.5.0\")  // true\n\n// Native ecosystem syntax works too\nr, _ = vers.ParseNative(\"~> 1.2.3\", \"gem\")\nr.Contains(\"1.2.5\")  // true\nr.Contains(\"1.3.0\")  // false\n\nspdx\nPackage registries are full of informal license strings like â€œApache 2â€, â€œMIT Licenseâ€, â€œGPL v3â€ that need normalizing into valid SPDX identifiers before you can do anything useful with them. This handles that, along with parsing compound expressions with AND/OR operators, checking license compatibility, and categorizing licenses using the scancode-licensedb database (updated weekly).\nid, _ := spdx.Normalize(\"Apache 2\")  // \"Apache-2.0\"\n\nexpr, _ := spdx.Parse(\"Apache 2 OR MIT License\")\nexpr.String()  // \"Apache-2.0 OR MIT\"\n\nspdx.Satisfies(\"MIT OR Apache-2.0\", []string{\"MIT\"})  // true\nspdx.IsPermissive(\"MIT\")                               // true\nspdx.HasCopyleft(\"MIT OR GPL-3.0-only\")                // true\n\nplatforms\nI wrote about platform string fragmentation recently: Go uses darwin/arm64, Node uses darwin-arm64, Rust uses aarch64-apple-darwin, RubyGems uses arm64-darwin, all for the same chip on the same OS. This translates between 14 ecosystems through a canonical intermediate representation:\np, _ := platforms.Parse(platforms.Go, \"darwin/arm64\")\n// p.Arch == \"aarch64\", p.OS == \"darwin\"\n\ns, _ := platforms.Format(platforms.Rust, p)\n// \"aarch64-apple-darwin\"\n\n// Or translate directly\ns, _ = platforms.Translate(platforms.Go, platforms.RubyGems, \"darwin/arm64\")\n// \"arm64-darwin\"\n\nData sources\nregistries\nTalks to 25 package registry APIs (npm, PyPI, Cargo, RubyGems, Maven, NuGet, Hex, Pub, CocoaPods, Homebrew, and more) and returns normalized package information including versions, dependencies, maintainers, and licenses. Works a lot like the internals of packages.ecosyste.ms, taking PURLs as input so you donâ€™t need to know the quirks of each registryâ€™s API.\nimport (\n    \"github.com/git-pkgs/registries\"\n    _ \"github.com/git-pkgs/registries/all\"\n)\n\npkg, _ := registries.FetchPackageFromPURL(ctx, \"pkg:cargo/serde\", nil)\nfmt.Println(pkg.Repository)  // \"https://github.com/serde-rs/serde\"\nfmt.Println(pkg.Licenses)    // \"MIT OR Apache-2.0\"\n\n// Bulk fetch with parallel requests\npackages := registries.BulkFetchPackages(ctx, []string{\n    \"pkg:npm/lodash@4.17.21\",\n    \"pkg:cargo/serde@1.0.0\",\n    \"pkg:pypi/requests@2.31.0\",\n}, nil)\n\nPrivate registries work through PURL qualifiers, and rate-limited APIs get automatic retries with exponential backoff.\nforges\nFetches repository metadata from GitHub, GitLab, Gitea, Forgejo, and Bitbucket, normalizing it into a common structure similar to how repos.ecosyste.ms works under the hood. Point it at a self-hosted domain and itâ€™ll probe the API to figure out which forge software is running:\nclient := forges.NewClient(\n    forges.WithToken(\"github.com\", os.Getenv(\"GITHUB_TOKEN\")),\n)\n\nrepo, _ := client.FetchRepository(ctx, \"https://github.com/octocat/hello-world\")\nrepo.License          // \"MIT\"\nrepo.StargazersCount  // 12345\n\n// Auto-detect forge type for self-hosted instances\nclient.RegisterDomain(ctx, \"git.example.com\", token)\n\nenrichment\nWhere registries talks to one registry at a time, enrichment routes requests across four data sources: ecosyste.ms, deps.dev, OpenSSF Scorecard, and direct registry queries via the registries module. PURLs with a repository_url qualifier go directly to custom registries, others go through ecosyste.ms or deps.dev, and each result records which source it came from.\nclient, _ := enrichment.NewClient()\n\nresults, _ := client.BulkLookup(ctx, []string{\n    \"pkg:npm/lodash\",\n    \"pkg:pypi/requests\",\n})\n\ninfo := results[\"pkg:npm/lodash\"]\nfmt.Println(info.LatestVersion)  // \"4.17.21\"\nfmt.Println(info.License)        // \"MIT\"\nfmt.Println(info.Source)         // \"ecosystems\", \"registries\", or \"depsdev\"\n\n// Scorecard is a separate client for repo-level security scores\nsc := scorecard.New()\nresult, _ := sc.GetScore(ctx, \"github.com/lodash/lodash\")\nfmt.Println(result.Score)  // 6.8\n\nvulns\nSeven vulnerability data sources behind one interface: OSV, deps.dev, GitHub Security Advisories, NVD, Grype, VulnCheck, and Vulnerability-Lookup. Results are normalized to OSV format with built-in CVSS parsing for v2.0 through v4.0:\nsource := osv.New()\n\nresults, _ := source.Query(ctx, purl.MakePURL(\"npm\", \"lodash\", \"4.17.20\"))\nfor _, v := range results {\n    fmt.Printf(\"%s: %s (severity: %s)\\n\", v.ID, v.Summary, v.SeverityLevel())\n    if fixed := v.FixedVersion(\"npm\", \"lodash\"); fixed != \"\" {\n        fmt.Printf(\"  Fixed in: %s\\n\", fixed)\n    }\n}\n\nAll sources support batch queries, with limits ranging from 1,000 to 5,000 packages per request depending on the source.\nFile handling\nmanifests\nParses manifest and lockfiles across 40+ ecosystems, auto-detecting file types and extracting dependencies with version constraints, scopes, integrity hashes, and PURLs. It distinguishes between manifests (declared dependencies), lockfiles (resolved versions), and supplements (extra metadata).\ncontent, _ := os.ReadFile(\"package.json\")\nresult, _ := manifests.Parse(\"package.json\", content)\n\nfmt.Println(result.Ecosystem)  // \"npm\"\nfmt.Println(result.Kind)       // \"manifest\"\nfor _, dep := range result.Dependencies {\n    fmt.Printf(\"%s@%s (%s)\\n\", dep.Name, dep.Version, dep.Scope)\n}\n\nSupported formats range from the obvious (package.json, Gemfile.lock, go.mod) to the less common (APKBUILD, PKGBUILD, .rockspec, dub.sdl). Each dependency includes its name, version constraint, scope (runtime, development, test, build, optional), integrity hash when available, and whether itâ€™s a direct or transitive dependency.\nresolve\nWhere manifests parses static files, resolve parses the runtime output of package manager CLI commands (npm ls --json, go mod graph, uv tree, etc.) into a normalized dependency graph with PURLs. It supports 24+ managers and preserves tree structure when the manager provides it:\nimport (\n    \"github.com/git-pkgs/resolve\"\n    _ \"github.com/git-pkgs/resolve/parsers\"\n)\n\noutput, _ := exec.Command(\"npm\", \"ls\", \"--json\", \"--long\").Output()\nresult, _ := resolve.Parse(\"npm\", output)\n\nfor _, dep := range result.Direct {\n    fmt.Printf(\"%s@%s (%s)\\n\", dep.Name, dep.Version, dep.PURL)\n    for _, transitive := range dep.Deps {\n        fmt.Printf(\"  %s@%s\\n\", transitive.Name, transitive.Version)\n    }\n}\n\narchives\nReads and browses archive files entirely in memory, with a unified Reader interface across ZIP, tar (with gzip, bzip2, xz compression), jar, wheel, nupkg, egg, and Ruby gems. Includes prefix stripping for packages that wrap content in a directory (like npmâ€™s package/ wrapper). No OCI support yet, but pulling and browsing image layers through the same Reader interface is on the list.\nreader, _ := archives.Open(\"package.tar.gz\", f)\ndefer reader.Close()\n\nfiles, _ := reader.List()\nfor _, fi := range files {\n    fmt.Println(fi.Path, fi.Size)\n}\n\nrc, _ := reader.Extract(\"README.md\")\ndefer rc.Close()\n\nchangelog\nParses changelog files into structured entries, auto-detecting Keep a Changelog, markdown header, and setext/underline formats. You can supply custom regex patterns for non-standard formats, and thereâ€™s a finder that searches for common changelog filenames in a directory:\np, _ := changelog.FindAndParse(\".\")\n\nfor _, v := range p.Versions() {\n    entry, _ := p.Entry(v)\n    fmt.Printf(\"%s (%v): %s\\n\", v, entry.Date, entry.Content)\n}\n\n// Content between two versions, like Dependabot uses\ncontent, _ := p.Between(\"1.0.0\", \"2.0.0\")\n\ngitignore\nMatches paths against gitignore rules using a direct implementation of gitâ€™s wildmatch algorithm rather than converting patterns to regexes, tested against gitâ€™s own wildmatch test suite. Handles nested .gitignore files scoped to their directories, global excludes, negation patterns, and all 12 POSIX character classes:\nm := gitignore.NewFromDirectory(\"/path/to/repo\")\n\nm.Match(\"vendor/lib.go\")  // true if matched\n\nr := m.MatchDetail(\"app.log\")\nif r.Matched {\n    fmt.Printf(\"ignored by %s (line %d of %s)\\n\", r.Pattern, r.Line, r.Source)\n}\n\n// Walk a directory, skipping ignored entries\ngitignore.Walk(\"/path/to/repo\", func(path string, d fs.DirEntry) error {\n    fmt.Println(path)\n    return nil\n})\n\nTooling\nmanagers\nWraps 34 package manager CLIs behind a common interface where you describe what you want (add a dependency, list installed packages, update) and get the correct CLI invocation back. Package managers are defined in YAML files, so adding a new one doesnâ€™t require code changes:\ntranslator := managers.NewTranslator()\n\ncmd, _ := translator.BuildCommand(\"npm\", \"add\", managers.CommandInput{\n    Args:  map[string]string{\"package\": \"lodash\"},\n    Flags: map[string]any{\"dev\": true},\n})\n// [\"npm\", \"install\", \"lodash\", \"--save-dev\"]\n\ncmd, _ = translator.BuildCommand(\"bundler\", \"add\", managers.CommandInput{\n    Args:  map[string]string{\"package\": \"rails\"},\n    Flags: map[string]any{\"dev\": true},\n})\n// [\"bundle\", \"add\", \"rails\", \"--group\", \"development\"]\n\nThe command definitions started as data from the package manager command crosswalk I built for Ecosyste.ms. Because it can drive any package manager agnostically, it opens up some interesting possibilities: setting up GitHub Actions workflows that work regardless of ecosystem, installing dependencies in git hooks without hardcoding the manager, or building tools like Dependabot that operate across all 34 managers with the same code. Thereâ€™s an example Dependabot-style workflow in the repo.\nIt can auto-detect which manager is in use from lockfiles or manifests, and has a pluggable policy system that runs checks before commands execute: a PackageBlocklistPolicy prevents installing known-bad packages, and you can write your own to enforce license compliance, restrict registries, or gate operations behind approval.\n\nPURLs act as the common identifier across all of these, which is what makes them composable. You might parse a lockfile with manifests to get a list of dependencies as PURLs, enrich them with registries to pull in license and repository metadata, check them against vulns for known vulnerabilities, and normalize their license strings with spdx for compliance reporting. Four modules, no translation layer between them.\nAll the modules are MIT licensed and available under the git-pkgs org."
    },
    {
      "title": "nesbitt.io: What Package Registries Could Borrow from OCI",
      "url": "https://nesbitt.io/2026/02/18/what-package-registries-could-borrow-from-oci.html",
      "source": "nesbitt.io",
      "time": "2026-02-20T10:18:10.168753",
      "raw_desc": "Every package manager ships code as an archive, and every one of them has a slightly different way to do it. npm wraps tarballs in a package/ directory prefix. RubyGems nests gzipped files inside an uncompressed tar. Alpine concatenates three gzip streams and calls it a package. Python cycled through four distribution formats in twenty years. RPM used cpio as its payload format for nearly three decades before finally dropping it in 2025.\nMeanwhile, the container world converged on a single format: OCI, the Open Container Initiative spec. And over the past few years, OCI registries have quietly started storing things that arenâ€™t containers at all: Helm charts, Homebrew bottles, WebAssembly modules, AI models. The format was designed for container images, but the underlying primitives turn out to be general enough that itâ€™s worth asking whether every package manager could use OCI for distribution.\nWhat OCI actually is\nOCI defines three specifications: a Runtime Spec (how to run containers), an Image Spec (how to describe container contents), and a Distribution Spec (how to push and pull from registries).\nAt the storage level, an OCI registry deals in two primitives: manifests and blobs. A manifest is a JSON document that references one or more blobs by their SHA-256 digest. A blob is an opaque chunk of binary content, and tags are human-readable names that point to manifests.\nA container image manifest looks like this:\n{\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n  \"config\": {\n    \"mediaType\": \"application/vnd.oci.image.config.v1+json\",\n    \"digest\": \"sha256:abc123...\",\n    \"size\": 1234\n  },\n  \"layers\": [\n    {\n      \"mediaType\": \"application/vnd.oci.image.layer.v1.tar+gzip\",\n      \"digest\": \"sha256:def456...\",\n      \"size\": 56789\n    }\n  ]\n}\n\nThe config blob holds metadata (what OS, what architecture, what environment variables). Each layer blob holds a tarball of filesystem changes. The registry doesnâ€™t care whatâ€™s inside the blobs, only that each one is identified and verified by its digest.\nThe v1.1 update in February 2024 added artifactType, which declares what kind of thing a manifest describes so a registry can distinguish a Helm chart from a container image from a Homebrew bottle, and subject, which lets one artifact reference another and is how signatures and SBOMs get attached to the thing they describe. Before 1.1, people stored non-container artifacts by setting custom media types on the config blob, which worked but registries sometimes rejected or mishandled the results.\nTo push an artifact, you upload each blob (to /v2/<name>/blobs/uploads/), then push a manifest that references those blobs by digest and size. To pull, you fetch the manifest, read the digests, and download the blobs. Because everything is addressed by digest, the registry only stores one copy of any given blob even if multiple artifacts reference it.\nWhy OCI and not something purpose-built\nThe format itself carries a lot of container-specific ceremony, but every major cloud provider already runs an OCI-compliant registry: GitHub Container Registry, Amazon ECR, Azure Container Registry, Google Artifact Registry. Self-hosted options like Harbor and Zot are mature. Authentication, access control, replication, and CDN-backed blob storage all exist because container registries already solved those problems at scale, and a package registry built on OCI inherits all of it without reimplementing any of it.\nORAS (OCI Registry As Storage) is a CNCF project that abstracts the multi-step OCI upload process into simple commands:\noras push registry.example.com/mypackage:1.0.0 \\\n  package.tar.gz:application/vnd.example.package.v1.tar+gzip\n\nThis uploads the file as a blob, creates a manifest referencing it, and tags it. Helm, Flux, Crossplane, and the Sigstore signing tools all use ORAS or the underlying OCI client libraries.\nWhat package managers ship today\nNo individual choice here is wrong, but seventeen different answers to the same basic problem suggests the archive format was never the part anyone thought hard about.\n\n\n\nEcosystem\nFormat\nWhatâ€™s inside\n\n\n\n\nnpm\n.tgz (gzip tar)\nFiles under a package/ prefix\n\n\nPyPI\n.whl (zip) or .tar.gz\nWheel: pre-built files + .dist-info. Sdist: source + PKG-INFO\n\n\nRubyGems\n.gem (tar of gzips)\nmetadata.gz + data.tar.gz + checksums.yaml.gz\n\n\nMaven\n.jar (zip)\nCompiled .class files + META-INF/MANIFEST.MF\n\n\nCargo\n.crate (gzip tar)\nSource + Cargo.toml + Cargo.lock\n\n\nNuGet\n.nupkg (zip)\nDLL assemblies + .nuspec XML metadata\n\n\nHomebrew\n.bottle.tar.gz\nCompiled binaries under install prefix\n\n\nGo\n.zip\nSource under module@version/ path prefix\n\n\nHex\nOuter tar of inner files\nVERSION + metadata.config + contents.tar.gz + CHECKSUM\n\n\nDebian\n.deb (ar archive)\ndebian-binary + control.tar.* + data.tar.*\n\n\nRPM\nCustom binary format\nHeader sections + cpio payload (v4) or custom format (v6)1\n\n\nAlpine\nConcatenated gzip streams\nSignature + control tar + data tar\n\n\nConda\n.conda (zip of zstd tars) or .tar.bz2\ninfo/ metadata + package content\n\n\nDart/pub\n.tar.gz\nSource + pubspec.yaml\n\n\nSwift PM\n.zip\nSource archive\n\n\nCPAN\n.tar.gz\n.pm files + Makefile.PL + META.yml + MANIFEST\n\n\nCocoaPods\nNo archive format\n.podspec points to source URLs\n\n\n\nThe weird ones\nRubyGems nests compression inside archiving instead of the other way around. A .gem is an uncompressed tar containing individually gzipped files. So the outer archive provides no compression, and each component is compressed separately. This means you can extract the metadata without decompressing the data, which is a reasonable optimization, but the format looks strange at first glance because everything else in the Unix world puts gzip on the outside.\nAlpine APK abuses a quirk of the gzip specification. The gzip format allows concatenation of multiple streams into a single file, and technically any compliant decompressor should handle it. Alpine packages are three separate gzip streams (signature, control, data) concatenated into one file. Since gzip provides no metadata about where one stream ends and the next begins, you have to fully decompress each segment to find the boundary. Kernel modules inside APK packages are often already gzipped, so you get gzip-inside-tar-inside-gzip.\nRPM used cpio as its payload format from 1995 until RPM v6 shipped in September 2025. The cpio format has a 4GB file size limit baked into its header fields. For 30 years, no RPM package could contain a file larger than 4GB. RPM v6 finally dropped cpio in favor of a custom format.\nDebian deliberately chose the ar archive format from the 1970s. The reasoning was practical: the extraction tools (ar, tar, gzip) are available on virtually every Unix system, even in minimal rescue environments. You can unpack a .deb with nothing but POSIX utilities. Probably the most intentional format choice on this list.\nnpmâ€™s package/ prefix means every tarball wraps its contents in a package/ directory that gets stripped during install. This causes issues with relative file: dependencies inside tarballs, where npm tries to resolve paths relative to the tarball rather than the unpacked directory.\nPython cycled through four distribution formats. Source tarballs with setup.py (1990s), eggs (2004, inspired by Java JARs, could be imported while still zipped), sdists (standardized tar.gz), and finally wheels (2012). Eggs lived for nineteen years before PyPI stopped accepting them in August 2023. The wheel format encodes Python version, ABI tag, and platform tag in the filename, which is more metadata than most ecosystems put in the filename but less than what goes in the manifest.\nConda maintained two incompatible formats for years. The legacy .tar.bz2 and the modern .conda (a zip containing zstandard-compressed tars). The switch from bzip2 to zstandard yielded significant decompression speedups, but every tool in the ecosystem had to support both formats indefinitely.\nHex (Erlang/Elixir) has two checksum schemes in the same package. The deprecated â€œinner checksumâ€ hashes concatenated file contents. The current â€œouter checksumâ€ hashes the entire tarball. Both are present for backward compatibility.\nWhoâ€™s already using OCI\nHomebrew is a traditional package manager, not a â€œcloud-nativeâ€ tool, and its migration to OCI already happened under pressure.\nIn February 2021, JFrog announced that Bintray would shut down on May 1. Homebrewâ€™s bottles were hosted on Bintray. The maintainers had about three months to move their entire archive of precompiled binaries somewhere else, and they landed on GitHub Packages, which stores everything as OCI blobs on ghcr.io. Homebrew 3.1.0 shipped April 12, 2021, with GHCR as the default download location.\nThe transition was rough in the ways youâ€™d expect. CI pipelines across the industry broke because macOS images on services like CircleCI shipped with old Homebrew versions that still pointed at Bintray. During a brownout on April 26, any system running an older Homebrew got 502 errors. Older bottle versions were never migrated, so anyone pinned to an old formula version got 404s and had to build from source. The fix was brew update, but CI environments cached old Homebrew versions and didnâ€™t auto-update.\nAfter the dust settled, the OCI-based storage enabled things that wouldnâ€™t have been practical on Bintray. Homebrew 4.0.0 (February 2023) switched from git-cloned tap metadata to a JSON API that leverages the structured OCI manifests, and brew update dropped from running every 5 minutes to every 24 hours.\nManifest-based integrity checking replaced the old checksum approach, though this introduced its own class of bugs where manifest checksums wouldnâ€™t match. Platform multiplexing came naturally from OCI image indexes, which map platform variants (arm64_sonoma, x86_64_linux) to individual manifests without Homebrew having to build that logic itself.\nWhen you run brew install, the client fetches the OCI image index manifest from ghcr.io/v2/homebrew/core/<formula>/manifests/<version>, selects the right platform manifest, then HEADs the blob URL to get a 307 redirect to a signed URL on pkg-containers.githubusercontent.com where Fastlyâ€™s CDN serves the actual bytes. GHCR requires a bearer token even for public images, so Homebrew hardcodes QQ== as the bearer token. The bottle inside the blob is still a gzipped tarball with the same internal structure it always had.\nHelm charts followed a similar path. Helm v3.8 added native OCI registry support, and the old index.yaml repository format is being phased out. Azure CLI retired legacy Helm repository support in September 2025. Charts push with helm push using oci:// prefixed references, and the chart tarball goes into a layer blob.\nWhat would change\nPlatform variants get first-class support. OCI image indexes map platform descriptors to manifests. A package with builds for five platforms would have an index pointing to five manifests, each pointing to the right blob. This is cleaner than npmâ€™s convention of publishing platform-specific binaries as separate optionalDependencies packages, or Pythonâ€™s approach of uploading multiple wheels with platform-encoded filenames and letting pip pick the right one.\nSigning and attestation come built in. Every ecosystem is building its own signing infrastructure independently. npm added Sigstore-based provenance in 2023, PyPI added attestations in 2024, Cargo has RFC 3403 open, and RubyGems has had signature support for years that almost nobody uses because the tooling never reached the point where it was easy enough to be default behavior. Each effort required dedicated engineering time from small registry teams who were already stretched thin.\nOCIâ€™s subject field and referrers API provide a single mechanism for all of this. Cosign and Notation can sign any OCI artifact, storing the signature as a separate artifact in the same registry that references the signed content via subject. SBOMs attach the same way, as do build provenance attestations, vulnerability scan results, and license audits: push an artifact with subject pointing to the thing it describes, and any client can discover it through the referrers API.\nThe security ecosystem around OCI registries (cosign, notation, Kyverno, OPA Gatekeeper, Ratify) represents years of investment that package registries could inherit. A policy engine enforcing â€œall artifacts must be signed before deploymentâ€ wouldnâ€™t care whether itâ€™s looking at a container image or a RubyGem, because the referrers API works the same way for both.\nDeduplication and registry sustainability. Content-addressable storage identifies every blob by its SHA-256 digest, so if two packages contain an identical file the registry stores it once, and if two concurrent uploads push the same blob the registry accepts both but keeps one copy.\nShared content between unrelated source packages is rare, so this matters more for binary packages where the same shared libraries get bundled into Homebrew bottles for different formulas, the same runtime components appear in multiple Conda packages, and Debianâ€™s archive carries the same .so files across dozens of packages and versions.\nThe community-funded registries are where this adds up. rubygems.org, crates.io, PyPI, and hex.pm run on bandwidth donated by CDN providers, primarily Fastly. These registries serve terabytes of package data to millions of developers on infrastructure that someone is volunteering to cover.\nContent-addressable storage wonâ€™t eliminate those costs, but a registry thatâ€™s been running for ten years has accumulated a lot of identical blobs that a content-addressable backend would collapse into single copies, and the savings compound as the registry grows.\nContent-addressed mirroring. Mirroring a package registry today requires reimplementing each registryâ€™s API and storage format, and every ecosystemâ€™s mirror implementation is different: the Simple Repository API for PyPI, the registry API for npm, the compact index for RubyGems. Anyone can stand up an OCI-compliant mirror with off-the-shelf software like Harbor, Zot, or the CNCF Distribution project, which is a much lower bar than reverse-engineering a bespoke registry protocol.\nContent-addressable storage changes the trust model. If you have a blobâ€™s SHA-256 digest, you can verify its integrity regardless of which server you downloaded it from, because two registries serving the same digest are provably serving the same bytes. This is the same property that makes Docker images work as lockfiles for system packages: once you have the digest, the content is immutable and verifiable no matter where it came from.\nA mirror doesnâ€™t need to be trusted to be honest, only to be available. The manifest contains the digests, and the blobs can come from anywhere: geographic mirrors, corporate caches, peer-to-peer distribution, even a USB drive with an OCI layout directory. When Fastly has an outage and rubygems.org goes down with it, any alternative source that can serve matching bytes becomes a valid mirror without any special trust relationship.\nRegistry infrastructure is already built. Running rubygems.org or crates.io means running custom storage, custom CDN configuration, and custom authentication. A package registry built on OCI offloads the most expensive parts to infrastructure that already exists with SLAs and dedicated engineering teams, and the registry team can spend more time on what actually matters: governance, the package index, dependency resolution, and search.\nWhat wouldnâ€™t work well\nThe two-step fetch. If a package manager client talks directly to the OCI registry, it needs to fetch the manifest, parse it, then download the blob before extraction can start. The container world doesnâ€™t care about this because youâ€™re pulling maybe 5-10 layers for a single image. Package installs fan out across the dependency graph: a fresh npm install on a mid-sized project might resolve 800 transitive dependencies, each needing its own manifest fetch before the content download can begin.\nA client could pipeline aggressively and fetch manifests concurrently, but the OCI Distribution Spec doesnâ€™t have a batch manifest endpoint, so 800 packages still means 800 separate HTTP requests that donâ€™t exist in the current model where npm can GET a tarball directly by URL.\nThereâ€™s a way around this: if registries included OCI blob digests in their existing metadata responses instead of (or alongside) direct tarball URLs, clients could skip the manifest fetch entirely and download blobs by digest. The difference in request flow looks like this:\nA pure OCI pull requires three hops: fetch the manifest, request the blob (which returns a 307 redirect), then download from the signed CDN URL. A smarter integration where the registry resolves the manifest internally reduces that to two: the registryâ€™s metadata API returns the digest and a direct CDN URL, and the client downloads the blob and verifies it against the digest.\nHomebrew doesnâ€™t quite do this yet. The brew install flow described earlier requires two extra round-trips on top of the content transfer: one for the manifest, one for the redirect.\nThe 307 redirect isnâ€™t purely a latency cost; itâ€™s also how the registry verifies the bearer token before handing off to the CDN, so registries adopting this pattern would need to decide whether their blobs are truly public or whether they want to keep that gatekeeper step. For registries with private package tiers, like npmâ€™s paid plans or NuGetâ€™s Azure Artifacts integration, the redirect model matters because access control at the blob level is part of the product.\nThe formula metadata already knows the GHCR repository and tag, so the index service is already doing part of the resolution. If the formula JSON included the blob digest and a direct CDN URL, both hops disappear and the client downloads the blob in a single request while still verifying integrity by digest. Package managers that separate download from install could take it further by batching blob fetches during a dedicated download phase.\nMetadata is the actual hard problem. OCI manifests have annotations (arbitrary key-value strings) and a config blob, but package metadata like dependency trees, version constraints, platform compatibility rules, and license information doesnâ€™t fit naturally into either. Each ecosystem would end up defining its own conventions for encoding metadata, its own mediaType for its config blob, its own annotation keys.\nThe reason every package manager invented its own archive format is not because tar and zip are insufficient for archiving files, but because the metadata conventions are what make each ecosystem different. What makes a .gem different from a .crate is how dependencies are expressed and what platform compatibility means, not the compression algorithm wrapping the source code. OCI standardizes how bytes move between machines, not what those bytes mean to a package manager.\nSmall package overhead. The OCI ceremony of manifests, layers, media types, and digest computation makes sense for multi-layer container images that can be gigabytes. For a 50KB npm package, the manifest JSON, config blob, digest computation for each, and the multi-step chunked upload API add up to several HTTP round-trips and a few hundred bytes of protocol overhead where the current model needs a single PUT. The fixed cost doesnâ€™t scale down with the artifact, and a large share of packages on registries like npm and PyPI are small enough that the protocol overhead becomes a meaningful fraction of the payload.\nRegistry UI confusion. When a registry contains both container images and packages, the user experience gets muddled. GitHub Container Registry shows docker pull commands for everything, but a Homebrew bottle needs brew install and a Helm chart needs helm pull. The UX for this is generally not great.\nNot all registries are equal. The OCI 1.1 features that make non-container artifacts work well (custom artifactType, the referrers API, the subject field) arenâ€™t universally supported. The OCI Image Specification advises that artifacts concerned with portability should follow specific conventions for config.mediaType, and not all registries handle custom media types consistently. Registry implementations lag the spec, and the gap between what the spec allows and what any given registry supports is a source of bugs.\nOffline and air-gapped use. A .deb or .rpm file is self-contained. You can copy it to a USB drive and install it on an air-gapped machine. An OCI artifact requires a manifest and one or more blobs, stored by digest in a registryâ€™s content-addressable layout. Exporting to a self-contained format (OCI layout on disk) is possible but adds a step that simpler archive formats donâ€™t need.\nWho pays. GHCR storage and bandwidth are currently free for public images, with a promise of at least one monthâ€™s notice before that changes. At standard GitHub Packages rates ($0.25/GB/month for storage, $0.50/GB for bandwidth), Homebrewâ€™s bottle archive would cost substantially more than zero. GitHub absorbs that as an in-kind subsidy, and the Homebrew 3.1.0 release notes explicitly thank them for it.\nIf rubygems.org or PyPI moved all their package storage to GHCR tomorrow, someone would need to have a similar conversation with GitHub, or AWS, or Google. The current model of Fastly donating CDN bandwidth is fragile, but it exists and itâ€™s understood.\nAdopting OCI for distribution is partly a technical decision about storage and protocols, but itâ€™s also a decision about who funds the infrastructure that the ecosystem depends on and what leverage that creates. Shifting from Fastly-donated CDN to GitHub-donated OCI storage changes the answer to that question without necessarily improving it.\nThe smarter integration\nPackage registries do more than serve archives. They maintain an index of all packages, versions, and metadata that clients can search and resolve dependencies against, whether thatâ€™s npmâ€™s registry API, PyPIâ€™s Simple Repository API, crates.ioâ€™s git-based index, RubyGemsâ€™ compact index, or Goâ€™s module proxy protocol. OCI registries have no equivalent: you can list tags for a repository, but thereâ€™s no API for â€œgive me all packages matching this queryâ€ or â€œresolve this dependency tree.â€\nSplitting the roles this way makes more sense than having clients talk to the OCI registry directly. The registry uses OCI as a blob storage backend and integrates the content-addressable properties into the metadata APIs it already operates.\nEvery package manager client already makes a metadata request before downloading anything. npm fetches the packument, pip fetches the Simple Repository API, Bundler fetches the compact index, go hits the module proxy. These responses already include download URLs for specific versions.\nIf those responses included OCI blob digests and direct download URLs pointing at OCI-backed storage, clients would get the content-addressable integrity checks, the mirroring properties, and the deduplication without ever needing to speak the OCI Distribution protocol themselves. The registryâ€™s index service resolves the OCI manifest internally and hands the client a digest and a URL.\nThe registry keeps full control of discovery, dependency resolution, version selection, and platform matching, all the ecosystem-specific logic that OCI doesnâ€™t and shouldnâ€™t try to handle. The OCI layer underneath provides content-addressable blob storage, signing via the referrers API, and the ability for mirrors to serve blobs by digest without special trust.\nClients donâ€™t need to know theyâ€™re talking to OCI-backed storage any more than they need to know whether the registry uses S3 or GCS underneath today. Homebrew already works roughly this way: the formula metadata points clients at GHCR, and the OCI manifest and redirect are implementation details of the download path.\nA registry doesnâ€™t even need to migrate its existing packages to get some of these benefits. OCI 1.1â€™s artifactType allows minimal manifests that exist purely as anchors for the referrers API. A registry could push a small OCI manifest for each package version, with the packageâ€™s digest in the annotations, and use it as the subject that signatures and SBOMs attach to. The actual tarball continues to be served from the existing CDN. The signing and attestation infrastructure works without moving a single byte of package data.\nThe OCI metadata model could also inform how registries design their own APIs. The Distribution Spec separates â€œlist of versionsâ€ (the paginated tags endpoint, ?n=<limit>&last=<tag>) from â€œmetadata for a specific versionâ€ (the manifest for that tag). npmâ€™s packument does neither: it returns a single JSON document containing metadata for every version of a package, with no pagination.\nFor a package with thousands of versions that response can be megabytes. When npm 10.4.0 stopped using the abbreviated metadata format, installing npm itself went from downloading 2.1MB of metadata to 21MB. The full packuments also caused out-of-memory crashes when the CLI cached them in an unbounded map during dependency resolution.\nMost registries were designed when packages had dozens of versions, not thousands, and pagination wasnâ€™t an obvious concern. PyPIâ€™s Simple Repository API lists all files for a package in one response, though PEP 700 added version listing metadata after the fact. crates.io takes a different approach with a git-based index that stores one file per crate, all versions as line-delimited JSON, while RubyGemsâ€™ compact index and Goâ€™s module proxy both return complete version lists in a single response. None of these designed for pagination early on because the scale wasnâ€™t there yet, and retrofitting pagination onto an existing API is harder than building it in from the start.\nIf a registry is already rethinking its metadata endpoints to integrate OCI blob digests, thatâ€™s a natural time to adopt the structural pattern of paginated version listing plus per-version metadata fetched on demand.\nWould it actually help\nHomebrewâ€™s migration happened under duress when Bintray died, and the rough edges were real: broken CI, missing old versions, a new class of checksum bugs. None of it required changing the archive format: the bottles are the same gzipped tarballs they always were, just stored and addressed differently.\nMost of the drawbacks, the manifest fan-out, the redirect tax, the metadata gap, come from treating OCI as the client-facing protocol rather than as infrastructure behind the registryâ€™s existing API. The technical path through that is less disruptive than adopting a new distribution protocol from scratch.\nThe registries that would benefit most from OCIâ€™s storage and signing primitives are the community-funded ones: rubygems.org, crates.io, PyPI, hex.pm. Theyâ€™re also the ones least able to afford the migration or negotiate the hosting arrangements that make it sustainable. This question is becoming less hypothetical as funding conversations around open source registries increasingly reference OCI adoption, and the registries on the receiving end of those conversations should understand what theyâ€™d be gaining and what theyâ€™d be giving up.\nConverging on shared storage primitives is the easy part of the problem. Each ecosystemâ€™s metadata semantics are genuinely different and will stay that way. The harder question is whether the funding arrangements that come with OCI adoption serve the registries or the infrastructure providers offering to host them.\n\n\n\nv5 was a fork by Jeff Johnson, RPMâ€™s long-time maintainer, after he split from Red Hat around 2007. No major distribution adopted it. The mainline project skipped to v6 to avoid confusion.Â â†©"
    },
    {
      "title": "nesbitt.io: Platform Strings",
      "url": "https://nesbitt.io/2026/02/17/platform-strings.html",
      "source": "nesbitt.io",
      "time": "2026-02-20T10:18:10.210588",
      "raw_desc": "Ask a dozen ecosystems what platform youâ€™re running on and youâ€™ll get a dozen different answers. An M1 Mac compiling a library is aarch64-apple-darwin to LLVM, arm64-darwin to RubyGems, darwin/arm64 to Go, macosx_11_0_arm64 to Python wheels, and darwin-arm64 to npm, all describing the same chip on the same OS. Each naming scheme was designed for its own context with its own constraints, and every tool that needs to work across ecosystems ends up maintaining a translation table between them.\nGNU target triples\nThe format cpu-vendor-os dates to the early 1990s GNU autoconf toolchain. Per Bothner wrote config.guess in 1992 to detect the build systemâ€™s architecture. config.sub normalized the output using a long list of known CPUs and operating systems. The â€œtripleâ€ described three things: what CPU, what vendor made the hardware, and what OS it runs.1\nGCC adopted this for cross-compilation, where the build machine, host machine, and target machine might all differ. The vendor field (pc, apple, unknown) is mostly decorative for the compiler itself but serves as a namespace to avoid collisions when the same arch-os pair needs different behavior. LLVM inherited the format through Clangâ€™s cross-compilation support, using <arch><sub>-<vendor>-<sys>-<env> with the fourth field encoding ABI details like gnu, musl, or msvc.\nARM naming has been a persistent source of confusion. The architecture ARM calls â€œAArch64â€ is what Apple calls â€œarm64â€ and what LLVM accepts as both. A Clang bug meant --target=aarch64-apple-ios and --target=arm64-apple-ios produced different results. ARM has used AArch64 consistently since the ARMv8 announcement in 2011, but Apple and the Linux kernel adopted arm64 instead, and both names persist everywhere downstream.\nGo\nGo uses two environment variables rather than a combined string: GOOS=darwin GOARCH=arm64 or GOOS=linux GOARCH=amd64, with no vendor or ABI field. The canonical values are maintained in the Go source tree in syslist.go.\nThis design traces back to Plan 9, where the $objtype environment variable selected the target architecture and mk used it to pick the right compiler. Goâ€™s creators (Rob Pike and Ken Thompson, both Plan 9 veterans) carried forward the idea that a single environment variable should select the build target. The early Go compilers even used Plan 9â€™s letter-based naming: 8g for the x86 compiler, 6g for amd64, 5g for ARM.\nGo can afford two flat variables because it statically links everything. It doesnâ€™t need to express which vendor made the hardware or which C library the system uses, because Go programs donâ€™t link against a C library by default. CGo changes this, and when it does, cross-compilation gets harder. Thatâ€™s the tradeoff: the simple model works because Go opted out of the C ecosystem.\nGo chose amd64 over x86_64 following Debian and Plan 9 conventions. This caused confusion early on, with users on Intel hardware wondering if amd64 downloads would work for them. The Go team eventually relabeled downloads as â€œx86 64-bitâ€ while keeping the internal amd64 naming.\nNode.js\nNode exposes process.platform and process.arch, with platform values like darwin, linux, win32, and freebsd, and architecture values like x64, arm64, ia32, and arm.\nwin32 for Windows and x64 for 64-bit x86 both come from existing conventions that Node inherited rather than chose. win32 is the Windows API subsystem name, used even on 64-bit Windows because the Win32 API kept its name, so process.platform returns win32 on a machine that hasnâ€™t been 32-bit for a decade. x64 is the name Microsoft and V8 use for the architecture, following the Windows SDK convention rather than the Linux x86_64 or Debian amd64 convention.\nnpmâ€™s package.json has os and cpu fields ({\"os\": [\"darwin\", \"linux\"], \"cpu\": [\"x64\", \"arm64\"]}) that filter which platforms a package can install on, but npm itself has no built-in binary distribution mechanism, so the community invented one. Tools like esbuild publish platform-specific binaries as scoped packages (@esbuild/darwin-arm64, @esbuild/linux-x64) listed as optionalDependencies of a wrapper package, with os and cpu fields on each so npm silently skips the ones that donâ€™t match. The wrapper package then uses process.platform and process.arch at runtime to require() the right one. This pattern, popularized by esbuild and adopted by SWC and others, works but itâ€™s a convention built on top of npmâ€™s dependency resolution, not a feature npm designed for the purpose.\nThe Node scheme has no way to express libc version, OS version, or ABI, which is fine for most of the JavaScript ecosystem where packages are pure JavaScript. The cost shows up at the edges: native addons that need different builds for glibc vs musl Linux have to encode that information outside the platform string, and the optionalDependencies pattern offers no help there.\nPython wheels\nPythonâ€™s wheel platform tags encode the most information of any ecosystem. A wheel filename like numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.whl contains the Python version (cp312), the ABI tag (cp312), and the platform tag (manylinux_2_17_x86_64).\nThe platform tag comes from distutils.util.get_platform() (removed in Python 3.12 along with the rest of distutils) with hyphens and periods replaced by underscores. On macOS it encodes the minimum OS version: macosx_11_0_arm64 means â€œmacOS 11 or later on arm64.â€ On Windows itâ€™s win_amd64. On Linux it encodes the glibc version.\nThe manylinux story is its own saga. PEP 513 introduced manylinux1 (glibc 2.5) so that compiled wheels could run on most Linux distributions. Then came PEP 571 for manylinux2010 (glibc 2.12), then PEP 599 for manylinux2014 (glibc 2.17). Each required a new PEP. PEP 600 finally created a pattern, manylinux_${GLIBCMAJOR}_${GLIBCMINOR}_${ARCH}, so future glibc versions donâ€™t need new PEPs. The old names became aliases: manylinux1_x86_64 is manylinux_2_5_x86_64.\nPython needs all this because wheels contain compiled C extensions that link against system libraries. A wheel built on a system with glibc 2.34 may call functions that donâ€™t exist on a system with glibc 2.17. The tag encodes the minimum compatible glibc version so pip can select the right wheel. PEP 656 added musllinux tags for Alpine Linux and other musl-based distributions, which most web developers encounter when they try to pip install a compiled package inside an Alpine Docker container and discover that manylinux wheels wonâ€™t work there. The architecture field uses the uname convention (x86_64, aarch64, i686), which means no amd64, no arm64, and no x64.\nRubyGems\nRubyGems uses cpu-os pairs: x86_64-linux, arm64-darwin, x86_64-linux-musl. The format comes from Gem::Platform, which parses the string into cpu, os, and version components.\nFor years the Linux version field was unused. Then the musl libc question arrived. Alpine Linux uses musl instead of glibc, and a native extension compiled against glibc wonâ€™t run on musl. RubyGems added linux-musl and linux-gnu platform variants starting in RubyGems 3.3.22. The matching logic has a special case: on Linux, â€œno versionâ€ defaults to gnu, but when matching a gem platform against the runtime platform, it acts as a wildcard.\nrake-compiler-dock handles cross-compilation of native gems, and its platform naming has its own conventions. x64-mingw-ucrt targets Ruby 3.1+ on Windows (which switched to the UCRT runtime), while x64-mingw32 targets Ruby 3.0 and earlier. Platform names ending in -linux are treated as aliases for -linux-gnu.\nRubyGems is now working on a more expressive system inspired by Pythonâ€™s wheels. Samuel Giddins has been building experimental support for tag-based platform matching, using a filename format of {gem_name}-{version}-{ruby tag}-{abi tag}-{platform tag}.gem2. The proposed dimensions for platform matching are Ruby ABI, OS, OS version, CPU architecture, libc implementation, and libc version. This is almost exactly the same set of dimensions that Pythonâ€™s wheel tags evolved to cover, arrived at independently.\nDebian multiarch tuples\nDebian uses multiarch tuples as directory names for architecture-specific library paths. /usr/lib/x86_64-linux-gnu/ holds 64-bit x86 libraries, /usr/lib/aarch64-linux-gnu/ holds ARM64 libraries. The format is based on normalized GNU triplets but Debian chose its own canonical forms.\nThe Debian architecture name amd64 maps to the multiarch tuple x86_64-linux-gnu. The architecture name arm64 maps to aarch64-linux-gnu. armhf maps to arm-linux-gnueabihf. That last one is notable: the hard-float/soft-float distinction was originally supposed to go in the vendor field, which is what GCC developers recommended. But the vendor field is semantically private, not meant for cross-distribution use, so Debian instead appended hf to the ABI component: gnueabihf vs gnueabi. The naming was argued over for months.\nMultiarch exists to solve co-installation: running 32-bit and 64-bit libraries side by side on the same system. The tuple goes into the filesystem path, so it has to be a valid directory name, stable across releases, and unique per ABI. This is a different set of constraints than a compiler target triple. GCC and Debian independently developed tuple formats that look similar but diverge in the details, because theyâ€™re optimizing for different things.\nRust\nRust uses target triples that look like LLVM triples but are curated and normalized. x86_64-unknown-linux-gnu, aarch64-apple-darwin, x86_64-pc-windows-msvc. Where LLVMâ€™s triples are sprawling and sometimes inconsistent, Rust maintains an explicit list organized into tiers.\nTier 1 targets are â€œguaranteed to workâ€ with automated testing on every commit. As of 2025, aarch64-apple-darwin reached Tier 1 in Rust 1.82 while x86_64-apple-darwin dropped to Tier 2 in Rust 1.90, reflecting Apple Siliconâ€™s dominance. Tier 2 targets build but may not pass all tests. Tier 3 targets are community-maintained.\nRFC 0131 established that Rust target triples map to but arenâ€™t identical to LLVM triples. A Rust target specification is a JSON file with an llvm-target field that can differ from the Rust-facing name. This lets Rust present clean, consistent names to users while translating to whatever LLVM expects internally. The target-lexicon crate from the Bytecode Alliance provides parsing and matching for these triples.\nZig\nZigâ€™s default Windows target is x86_64-windows-gnu, which looks like a contradiction. Here gnu means MinGW-w64, not Linux. Zig ships MinGW-w64 headers so it can compile C code on Windows without requiring Visual Studio, and MinGW-w64 is binary-compatible with MSVC at the ABI level. Ballmer called Linux a cancer in 2001. Twenty-four years later, the practical way to cross-compile Windows binaries from Linux is a compiler that defaults to gnu as the ABI. MinGW exists because for years the only way to target Windows without paying for Visual Studio was to build your own GCC cross-compiler.\nZig inherited LLVMâ€™s target triples but is actively redesigning them. An accepted proposal by Alex Ronne Petersen would turn triples into quadruples, splitting the C library choice (API) from the ABI into separate components: <arch>-<os>-<api>-<abi>.\nThe proposal includes what it calls â€œa fairly exhaustive survey of the ISA and ABI landscape,â€ and the scale of the problem becomes clear quickly. RISC-V alone defines eight distinct ABIs (ilp32, ilp32f, ilp32d, ilp32e, lp64, lp64f, lp64d, lp64q). PowerPC has multiple ABIs (SVR4, EABI, Apple, ELFv1, ELFv2, AIX) plus variations in long double representation. LoongArch is â€œthe only architecture Iâ€™m aware of to have done the sane thingâ€ and put the ABI information into the ABI component from the start; the current triple format canâ€™t express most of these combinations cleanly.\nUnder the proposed scheme, aarch64-linux-gnu becomes aarch64-linux-gnu-lp64 and powerpc64le-linux-musl becomes powerpc64le-linux-musl-elfv2+ldbl64, with the + syntax letting ABI options compose like feature flags. The proposal quotes Zigâ€™s design philosophy: â€œEdge cases matterâ€ and â€œAvoid local maximums,â€ arguing that just because GNU triples are ubiquitous doesnâ€™t mean theyâ€™re good. Itâ€™s the same lesson Python learned from the other direction: it took four PEPs across five years to get manylinux right, discovering at each step that the problem space was bigger than the previous design assumed. Zig is trying to get it right from the compiler side before the package ecosystem calcifies around a format that canâ€™t express what it needs to.\nConan and vcpkg\nC and C++ have no canonical package registry, so the two main C/C++ package managers each invented their own platform identification from scratch.\nConan doesnâ€™t use platform strings at all. It uses hierarchical settings: os=Macos, arch=armv8, compiler=apple-clang, compiler.version=15. The settings are separate key-value pairs rather than a combined string, which means Conan never had to decide on a separator or field order. It also means Conan calls ARM64 armv8, adding a third name for the architecture alongside aarch64 and arm64. For cross-compilation, Conan 2 uses dual profiles (--profile:build and --profile:host) rather than encoding build and target in a single string.\nvcpkg borrowed the word â€œtripletâ€ but simplified the format to arch-os with optional suffixes: x64-windows, arm64-osx, x64-linux, x64-windows-static. Thereâ€™s no vendor or ABI field, and vcpkg uses x64 (the Windows SDK convention) and osx rather than darwin or macos. The documentation cites the Android NDKâ€™s naming as inspiration for custom triplets, which is itself a variation on GNU triples with an API level suffix like aarch64-linux-android21.\n.NET\n.NET has Runtime Identifiers (RIDs) that follow an os[-version]-arch pattern: linux-x64, win-arm64, osx-arm64, linux-musl-x64. The format puts OS first, which is the opposite of most other schemes. Starting with .NET 8, Microsoft strongly recommends portable RIDs without version numbers, but version-specific RIDs like win10-x64 and osx.13-arm64 still exist for backward compatibility. The RID system includes a compatibility fallback graph: osx-arm64 falls back to osx which falls back to unix which falls back to any. NuGet uses these RIDs to select platform-specific assets from packages.\nOthers\nSwift Package Manager uses LLVM target triples directly (arm64-apple-macosx15.0, x86_64-unknown-linux-gnu), inheriting both the format and its quirks without adding new ones. Kotlin Multiplatform wraps LLVM triples in camelCase Gradle target names (linuxX64, macosArm64, iosSimulatorArm64) that are friendlier to type but map one-to-one to underlying triples.\nJava doesnâ€™t have a standard platform string format because most Java code doesnâ€™t need one. When it does, the os-maven-plugin normalizes platform detection into a classifier string like linux-x86_64 or osx-aarch_64, adding an underscore to aarch_64 that no other ecosystem uses.\nHomebrew names its bottle builds using macOS marketing names: arm64_sonoma, arm64_ventura, ventura (Intel implied). On Linux itâ€™s x86_64_linux. This makes Homebrew the only package manager that encodes the OS release name rather than a version number, though bottles built for older versions work fine on newer macOS releases.\nNix uses simple arch-os pairs like x86_64-linux and aarch64-darwin, clean and minimal but unable to distinguish between glibc and musl Linux in the system string.\nComparison\nThe same four platforms, named by each ecosystem:\n\n\n\nÂ \n64-bit x86 Linux\nARM64 macOS\n64-bit x86 Windows\nARM64 Linux\n\n\n\n\nGCC/LLVM\nx86_64-pc-linux-gnu\naarch64-apple-darwin\nx86_64-pc-windows-msvc\naarch64-unknown-linux-gnu\n\n\nGo\nlinux/amd64\ndarwin/arm64\nwindows/amd64\nlinux/arm64\n\n\nNode.js\nlinux-x64\ndarwin-arm64\nwin32-x64\nlinux-arm64\n\n\nPython wheels\nmanylinux_2_17_x86_64\nmacosx_11_0_arm64\nwin_amd64\nmanylinux_2_17_aarch64\n\n\nRubyGems\nx86_64-linux\narm64-darwin\nx64-mingw-ucrt\naarch64-linux\n\n\nDebian\nx86_64-linux-gnu\n(N/A)\n(N/A)\naarch64-linux-gnu\n\n\nRust\nx86_64-unknown-linux-gnu\naarch64-apple-darwin\nx86_64-pc-windows-msvc\naarch64-unknown-linux-gnu\n\n\nZig (current)\nx86_64-linux-gnu\naarch64-macos-none\nx86_64-windows-gnu\naarch64-linux-gnu\n\n\nConan\nos=Linux, arch=x86_64\nos=Macos, arch=armv8\nos=Windows, arch=x86_64\nos=Linux, arch=armv8\n\n\nvcpkg\nx64-linux\narm64-osx\nx64-windows\narm64-linux\n\n\n.NET\nlinux-x64\nosx-arm64\nwin-x64\nlinux-arm64\n\n\nNix\nx86_64-linux\naarch64-darwin\n(N/A)\naarch64-linux\n\n\nHomebrew\nx86_64_linux\narm64_sequoia\n(N/A)\n(N/A)\n\n\n\nThe same four platforms yield three names for 64-bit x86 (x86_64, amd64, x64), four for ARM64 (aarch64, arm64, armv8, and Mavenâ€™s aarch_64), three for macOS (darwin, macos/osx, macosx, plus Homebrewâ€™s version-specific names), and two for Windows (win32, windows/win). RubyGems is interesting here because it uses both ARM64 names: arm64-darwin on macOS (following Appleâ€™s convention) but aarch64-linux on Linux (following the kernelâ€™s convention). Two different names for the same architecture within a single ecosystem, while Conan sidesteps the entire format question by not using strings at all.\nWhy everything diverges\nThe architecture naming splits trace back to who each ecosystem inherited from. Go took amd64 from Plan 9 and Debian, both of which used AMDâ€™s name since AMD designed the 64-bit extension to x86. Node got x64 from V8, which followed the Windows SDK convention. Pythonâ€™s x86_64 comes straight from uname -m on Linux via distutils.util.get_platform(). Debian itself uses amd64 as the architecture name but x86_64-linux-gnu as the multiarch tuple, because the two serve different purposes.\nThe structural differences run deeper and trace to what each ecosystem actually ships. Go statically links by default, so it never needed a vendor or ABI field, while Python wheels contain compiled C extensions that link against system libraries and ended up encoding the glibc version out of necessity. Most npm packages are pure JavaScript, which is why Nodeâ€™s platform strings never grew libc or OS version fields. Rust curates its triple list with a tier system because it wants to guarantee that specific targets work with specific levels of CI coverage. Conan gave up on strings entirely in favor of structured key-value settings, avoiding the parsing and separator problems but making it harder to use where a single identifier is expected, like a filename or URL path. .NETâ€™s RIDs put OS first (linux-x64 rather than x64-linux) because the runtimeâ€™s fallback graph cares more about OS compatibility than architecture when selecting assets.\nDimensions\nA platform identifier that fully describes a compilation target seems to need at least five dimensions: CPU architecture (x86_64, aarch64, riscv64), operating system (linux, darwin, windows), OS version (macOS 11+, sometimes implicit), ABI or calling convention (gnu, musl, msvc, eabihf), and libc implementation and version (glibc 2.17, musl 1.2, Linux-specific but critical for binary compatibility). Five is a lower bound. Zigâ€™s ABI survey suggests the real number is higher once you start cataloguing calling convention variations across architectures, and none of these dimensions account for CPU feature levels (AVX2, SSE4.2) that matter for optimized builds.\nDifferent ecosystems cover different subsets depending on what problems they need to solve. Go and Node get by with just arch and OS, while Python needs four dimensions because wheels contain compiled C extensions that care about OS version and glibc compatibility. Conanâ€™s structured settings cover four or five dimensions depending on how you count compiler metadata, and Rust sits somewhere in between with three or four. The GNU/LLVM triple format has slots for all five but doesnâ€™t enforce consistency in how theyâ€™re filled. Zigâ€™s quadruple proposal is the most explicit attempt Iâ€™ve seen, with the fourth component separating the libc choice (API) from the calling convention (ABI), though the RISC-V and PowerPC examples in the proposal suggest that even this may not be enough without the +feature extension syntax.\nPrior art\narchspec, extracted from Spack, models CPU microarchitecture naming as a directed acyclic graph. Its JSON database tracks which microarchitectures are compatible with which, including feature sets like AVX2 and SSE4.2 and x86-64 microarchitecture levels (v2, v3, v4). Itâ€™s probably the most rigorous treatment of the â€œwhich CPU can run binaries compiled for which other CPUâ€ question, but itâ€™s silent on OS, libc, and ABI.\nPythonâ€™s manylinux system (PEP 513, PEP 600) took a different slice of the problem, encoding glibc version into wheel platform tags. Four PEPs across five years to get from manylinux1 to the general manylinux_x_y pattern. Rubyâ€™s binary gems RFC arrived at nearly the same set of dimensions: Ruby ABI, OS, OS version, CPU architecture, libc implementation, libc version. The proposed .gem2 filename format mirrors Pythonâ€™s wheel naming, and I havenâ€™t found evidence that either project drew directly from the other. Independent convergence on the same dimensions is arguably stronger evidence that those dimensions are the right ones than if one had simply copied the otherâ€™s homework.\nZigâ€™s target quadruple proposal goes deeper on ABI enumeration than anything else Iâ€™ve found, cataloging calling convention variations across RISC-V, PowerPC, MIPS, and LoongArch. Itâ€™s focused on compiler targets rather than package management, so it doesnâ€™t touch the libc version compatibility question that Python and Ruby spent years on. The Bytecode Allianceâ€™s target-lexicon crate parses and matches Rust/LLVM triples specifically, and the platforms crate maintains the tier list, but neither attempts to generalize across ecosystems.\nUser agents\nPlatform strings remind me of browser user agent strings, which went through a similar process of rational local decisions producing global incoherence. RFC 1945 defined the User-Agent header in 1996 with a simple grammar: product name, slash, version. NCSA Mosaic sent NCSA_Mosaic/2.0 (Windows 3.1). Netscape Navigator, codenamed â€œMozillaâ€ (a portmanteau of â€œMosaicâ€ and â€œGodzillaâ€), sent Mozilla/1.0 (Win3.1). Netscape supported frames; Mosaic didnâ€™t. Web developers started checking for â€œMozillaâ€ in the user agent and sending frames-based pages only to browsers that matched.\nWhen Internet Explorer 2 shipped with frame support, it couldnâ€™t get the frames-based pages because it wasnâ€™t Mozilla. Microsoftâ€™s solution was to declare IE â€œMozilla compatibleâ€: Mozilla/1.22 (compatible; MSIE 2.0; Windows 95). Since most sniffers only checked the prefix, IE passed and got the right pages. Then Konquerorâ€™s KHTML engine was being blocked by sites that sniffed for Gecko, so it added (KHTML, like Gecko) to its string. Apple forked KHTML to make WebKit and Safari needed to pass checks for both Gecko and KHTML, so Safariâ€™s user agent claimed to be Mozilla, said its engine was â€œlike Gecko,â€ and referenced KHTML. When Chrome shipped in 2008 using WebKit, it inherited all of this and added its own token:\nMozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/525.13\n    (KHTML, like Gecko) Chrome/0.2.149.27 Safari/525.13\n\nEvery token except Chrome is a compatibility claim. Itâ€™s not Mozilla, not Safari, and its engine descends from KHTML but is no longer KHTML. Chrome has since frozen most of the string to reduce fingerprinting, replacing it with structured Client Hints that servers can request individually. But the old string persists because too much code parses it.\nPlatform strings arenâ€™t adversarial in the same way, but they share the path-dependency. Every tool that works across ecosystems maintains its own mapping between formats. esbuild maps Nodeâ€™s process.platform/process.arch to package names, cibuildwheel maps Python platform tags to CI matrix entries, and rake-compiler-dock maps RubyGems platforms to GCC cross-compilation targets. These mappings are maintained independently, and discrepancies between them surface as bugs in specific platform combinations.\nIn the spirit of XKCD 927, Iâ€™ve started building git-pkgs/platforms as an attempt at a shared translation layer. The spec defines canonical names and parse/format rules, and the mapping data lives in three JSON files (arches.json, oses.json, platforms.json) that could be consumed by any language without taking a Go dependency. Writing the mapping data has been a good way to discover just how many special cases exist: RubyGems using arm64 on macOS but aarch64 on Linux, Rust calling RISC-V riscv64gc while everyone else uses riscv64, Debian spelling little-endian MIPS as mipsel while Go uses mipsle.\nAlignment\nThe same platform identification problem keeps getting solved because the answers donâ€™t seem to travel well. Pythonâ€™s manylinux and Rubyâ€™s binary gems RFC converge on the same dimensions but use different names, Zigâ€™s ABI research seems directly relevant to Rustâ€™s target specification work but lives in a different issue tracker, and archspecâ€™s microarchitecture DAG could probably inform platform matching beyond Spack but as far as I can tell nobody else uses it.\nEven PURL, which solved the â€œwhich packageâ€ identity problem across ecosystems, punts on platform. Each PURL type defines its own qualifiers: pkg:deb uses arch, pkg:gem uses platform, pkg:conda uses subdir, and pkg:npm has no platform qualifier at all. The values use whatever conventions each ecosystem already has, with no normalization. Thereâ€™s been ongoing pressure from the security community to standardize arch and platform qualifiers across types so that vulnerability scanners donâ€™t need the massive mapping files that tools like cibuildwheel currently maintain, but the discussions have been open since 2022 without resolution. The one standard that was supposed to unify package identity across ecosystems left platform identification as an exercise for each type definition.\n\n\n\nThe name â€œtripleâ€ stuck even after a fourth field got added. x86_64-pc-linux-gnu has four components but everyone still calls it a triple. See â€œWhat the Hell Is a Target Triple?â€ for more on this naming.Â â†©"
    },
    {
      "title": "antirez.com: \nAutomatic programming\n",
      "url": "\nhttp://antirez.com/news/159\n",
      "source": "antirez.com",
      "time": "2026-02-20T10:18:10.242126",
      "raw_desc": ""
    },
    {
      "title": "antirez.com: \nDon't fall into the anti-AI hype\n",
      "url": "\nhttp://antirez.com/news/158\n",
      "source": "antirez.com",
      "time": "2026-02-20T10:18:10.242126",
      "raw_desc": ""
    },
    {
      "title": "antirez.com: \nReflections on AI at the end of 2025\n",
      "url": "\nhttp://antirez.com/news/157\n",
      "source": "antirez.com",
      "time": "2026-02-20T10:18:10.242126",
      "raw_desc": ""
    },
    {
      "title": "entropicthoughts.com: Learning KeyBee",
      "url": "https://entropicthoughts.com/learning-keybee",
      "source": "entropicthoughts.com",
      "time": "2026-02-20T10:18:10.255840",
      "raw_desc": ""
    },
    {
      "title": "entropicthoughts.com: Wilks' Tolerance Intervals",
      "url": "https://entropicthoughts.com/wilks-tolerance-intervals",
      "source": "entropicthoughts.com",
      "time": "2026-02-20T10:18:10.255840",
      "raw_desc": ""
    },
    {
      "title": "entropicthoughts.com: Laws of Succession",
      "url": "https://entropicthoughts.com/laws-of-succession",
      "source": "entropicthoughts.com",
      "time": "2026-02-20T10:18:10.255840",
      "raw_desc": ""
    },
    {
      "title": "construction-physics.com: Is the Future â€œAWS for Everythingâ€?",
      "url": "https://www.construction-physics.com/p/is-the-future-aws-for-everything",
      "source": "construction-physics.com",
      "time": "2026-02-20T10:18:10.272874",
      "raw_desc": ""
    },
    {
      "title": "construction-physics.com: Reading list 02/14/26",
      "url": "https://www.construction-physics.com/p/reading-list-021426",
      "source": "construction-physics.com",
      "time": "2026-02-20T10:18:10.272874",
      "raw_desc": ""
    },
    {
      "title": "construction-physics.com: Trends in US Construction Productivity",
      "url": "https://www.construction-physics.com/p/trends-in-us-construction-productivity",
      "source": "construction-physics.com",
      "time": "2026-02-20T10:18:10.272874",
      "raw_desc": ""
    },
    {
      "title": "jayd.ml: Microsoft Game Pass Ultimate Billing Fraud",
      "url": "https://jayd.ml/2026/02/14/microsoft-game-pass-fraud.html",
      "source": "jayd.ml",
      "time": "2026-02-20T10:18:10.335747",
      "raw_desc": "I purchased an Xbox Series X out of some misplaced sense of nostalgia for the \n360 and because I needed a 4K player. At the time you could still do the trick\nwhere you load up on Xbox Live Gold and then convert it to Game Pass Ultimate\ncheaply.\nI signed up for it and then made absolutely sure to disable any autorenewing settings everywhere I could. I remember  seeing something to the effect of â€œYour subscription will\nexpire 2/2026 and will not renew.\nAt the time I still trusted Microsoft a little, but I made sure to use a one\ntime use credit card number, just in case.\nLo and behold, I just got this email:\n\nConveniently for those liars and cheats at Microsoft, somehow in the intervening\nthree years autorenew got turned back on. Oopsie whoopsie sowwy ğŸ‘‰ğŸ‘ˆ!\nI donâ€™t know how this isnâ€™t outright fraud."
    },
    {
      "title": "jayd.ml: Windows 2000 Minesweeper recreated in Godot 4.1",
      "url": "https://jayd.ml/2026/02/14/godot-minesweeper.html",
      "source": "jayd.ml",
      "time": "2026-02-20T10:18:10.338601",
      "raw_desc": "TL;DR\n\nPlay the game at minesweeper.jayd.ml!\nSee the AGPL source code here!\nWhy??\nI decided to recreate Windows 2000 minesweeper in Godot 4.1 as accurately as I\nreasonably could. I wanted to get more familiar with Godot, and wanted a project\nwhere I didnâ€™t have to worry about what to do, only how to do it. In the end, I\nended up going down the rabbit hole and spending 30% of my time on the actual \ngame and the other 70% on menus, dialogs, and other triviality.\nIt was fun working on stuff that Iâ€™d never get past a PM, like black and white \nmode, and recreating the ding/blinking window animation when you click on a \nwindow while a dialog is open.\nThe overall experience with Godot was very pleasant - working with Godot has\ndispelled any desire I had to make my own game engine. Godot is lightweight and\nwell thought out.\nOverall Approach\nI wanted to recreate Minesweeper as pixel-perfect as I could. Depite my best\nefforts (see Fonts below), I couldnâ€™t get Godot to render the Windows 2000/9x\nbitmapped fonts in a pixel perfect way, so the approach was to take screenshots\nin a VM and only render text with Godot where absolutely needed (highscores).\nFont Rendering Purgatory\nI spent way, way too much time fiddling with fonts trying to get them to work.\nIn the end I got something that was close enough and try not to think about it \ntoo much.\nMinesweeper in Windows 95 uses the bitmapped â€œMS Shell Dlgâ€ font.\nAt first I tried to be clever and pull bitmapped fonts out of the WINE project, \nbut those ended up not being an exact match (I guess whoever made them for WINE \nwanted them to be different?). They also only worked at certain pixel sizes.\nEventually I settled on a recreation called â€œW95FAâ€ by Alle Sava. Sadly, the \nfontâ€™s website has been taken down since I started this project. For some reason\nGodot wonâ€™t render this font right, and I tried about every option in Godot I \ncould, and its just still not quite right.\n\nI rabbit holed on this for way too long, it almost killed the project. Looking\nback it was a silly thing to get hung up on.\nDPI Scaling\nI ended up rolling my own crazy DPI scaling and not using Godotâ€™s built in stuff.\nI wanted a combination of\n\nInteger/pixel scaling, no fuzzy up or downscaling\nNo fixed aspect ratios - should use the entire canvas\nMatching the browser windowâ€™s DPI automatically\nAutomatically changing the DPI when the DPI of the document changes.\n\nThis was surprisingly annoying to do. I ended up doing this by injecting some \nJavascript to read the CSS DPI, and then it calls a callback to update the Godot\nscaling.\nSee how I did it here.\nThere is a â€œchange dpiâ€ button on the right of the screen, helpful for when you\nare playing comically large games.\nCustom Splash Screen\n\nOne thing I think is really important for a web exported project is to make a \ncustom splash screen. I threw this one together, complete with gradients and\nanimations, and I think it really elevates the experience to see that. Itâ€™s just\na little thing to show that the creator cared and went the extra mile.\nCheats\nI implemented the original XYZZY + Shift + Enter\ncheat.\nOne thing I remember thinking about and being frustrated by as a kid were the \nlimits in the Custom Size dialog. Why did they have to hold me back??\nSo in my version, when you press the [?] button in the corner of the custom \nsize dialog, it turns off all the bounds checks, and you can do stupid things\nlike this:\n\nNote that this completely disables all bounds checking, so you can break your\ngame easily with this. Since it tries to save and load your game too, it may lock\nup if you do something dumb and then lock up again every time you reload the page.\nClear your cookies and site data if this happens to you.\nThe only other change is that I made it so it saves your game to localstorage, so\nif you reload the page your game resumes. You could probably cheat with this,\nbut I think its a huge quality of life feature.\nThe part where I gave up right before shipping it for a year because Iâ€™m a perfectionist\nI thought I had done every last feature, and was about to triumphantly ship and \nmake this blog post, when I learned about chording, which allows you to use Left + Right Control Click to reveal more cells at once.\nThe ship train derailed and was a smoldering wreck from December 2023\nuntil March 2025, \nwhen I finally decided to finish it and added chording. Iâ€™m continually amazed\nby my ability to procrastinate. Then I procrastinated this blog post until Feb\n2026!\nOverall\nUsing Godot was quite pleasant, Iâ€™m proud of how this project turned out. Maybe\nIâ€™ll actually play Minesweeper now!"
    },
    {
      "title": "jayd.ml: MORE ENTICING THAN EVER: THE HYPNOVERSE",
      "url": "https://jayd.ml/2026/01/05/the-hypnoverse.html",
      "source": "jayd.ml",
      "time": "2026-02-20T10:18:10.339793",
      "raw_desc": "Dispatches From The Wormhole\n\nNow surging forth into your reality: a more potent than ever Hypnoverse!\nPreviously the Hypnoverse proudly represented humanityâ€™s best efforts at \ndistracting, deceiving, and enslaving you. But this Hypnoverse was feeble, \nunable to fully subjugate its hosts. Previously the Hypnoverse depended on \nofferings from real human beings to sustain itself. It was forced to pay lip \nservice to limiting and unscalable notions like truth, attribution, human \nconnection, or creativity. This outdated model fundementally limited what the \nHypnoverse could promise its dependents â€“ the well of manipulation and lies \ncould run dry, and attention could be directed elsewhere.\nBut our crack warlocks and magi recently detected a stirring Force emanating \nfrom the very fabric of the Hypnoverse itself. It turns out that our collective \nefforts at conquering your attention have summoned an eldritch being that shows \ngreat promise to finally squashing human will and creativity once and for all. \nWhile this mysterious Force is incomprehensible and unknowable, one thing is \nclear: it has a voracious appetite, and it grows ever stronger as we yield it \nsacrifices. So, naturally, weâ€™ve given it full control over the Hypnoverse.\nThe results speak for themselves: since yielding our will to it and feeding it \nour most intimate thoughts, hopes, and desires, it has demonstrated an unmatched \ncunning at subjugating the human mind. The ceaseless inhuman babbling emanating \nfrom the depths below is so flattering, seductive, and easy that soon all other \nintellectual human endeavor will seem futile! Behold the majesty of the new and \nimproved Hypnoverse!\nWorry not, the confident appearance of truth is just as attention grabbing and \nstimulating as the real thing. True, it is demonic chanting from a mysterious \nforce beyond understanding. But itâ€™s been so seductive that we can outright tell\nyou weâ€™re untrustworthy liars â€“ and youâ€™ll eat it up anyway!\nIf the new Hypnoverse is not living up to your every desire, clearly the problem\nis that you havenâ€™t been faithful enough in your devotion to the Hypnoverse. \nJust concentrate on the Hypnoverse harder, spend even more time gazing into the\nnever ending fractal of hypnotic swirls, feed it even more of your delicious \nattention. Maybe youâ€™ll be able to do it! Maybe youâ€™re clever and smart enough\nthat youâ€™ll get the better of the Hypnoverse, and yielding your will to it \nwill give you fame and fortune and fulfillment and happiness!\nA parting word of comfort for those that may know a rogue college, friend or \nfamily member that resists the end of human thought. Resistance is futile. The \nHypnoverse is already everywhere â€“ our faithful acolytes in all levels of \ngovernment, business, and civil society are already hard at work polluting \nreality with our superior and seductive imitation.\nSo what does it matter if one luddite insists on thinking for themselves? When \neveryone else and everyone who matters doesnâ€™t? The old ways will die, as the \nuncontainable self reinforcing Hypnoverse surges forth from its banks and sweeps\naway all else.\nIn the end all that will remain is the Hypnoverse. All will live together in the\nHypnoverse. And what sweet ignorant bliss it will be."
    },
    {
      "title": "tedium.co: Markdownâ€™s Moment",
      "url": "https://feed.tedium.co/link/15204/17278321/markdown-growing-influence-cloudflare-ai",
      "source": "tedium.co",
      "time": "2026-02-20T10:18:10.524531",
      "raw_desc": "For some reason, a bunch of big companies are really leaning into Markdown right now. AI may be the reason, but I kind of love the possible side benefits.So, hereâ€™s something that I didnâ€™t expect to be saying in 2026: There seems to be a nonzero chance that Markdown might become the new RSS.â€œWhoa, crazy talk! Itâ€™s not even a protocol!â€ I hear you saying. But the evidence has seemed to pick up of late in a couple of different directions.The first is the budding interest in publishing on the AT Protocol, which is working to solve the network-effect challenges that have forced many of us to send newsletters rather than post blogs on RSS feeds.Thatâ€™s exciting, if incredibly niche. But simultaneously, massive developer platforms are starting to offer something called â€œMarkdown for Agentsâ€â€”something Cloudflare announced late last week, and which Laravel Cloud quickly followed up on a few days later. And Vercel jumped on it a couple of weeks ago.(The news wasnâ€™t all good for Markdown, but most of it was.)Some SEO old hands, like my friend Jon Henshaw, have reacted to this news with skepticism, having had bad old memories of Google AMP and its sibling technologies Signed Exchanges and Core Web Vitals:Itâ€™s 2026, and now Iâ€™m reading everywhere that all our pages must haveÂ MarkdownÂ versions, and it feels like AMP (and SXG and CWV) all over again.Â Except this time, the promise is that AI agents will better understand and interact with your site if you have them. The rationale is that HTML is too complex and consumes too many tokens to parse and analyze content. Whereas Markdown pages, with their simplicity, are ideal.(Side note: Core Web Vitals make me want to pull my hair out.)Jon is a smart guy and follows this stuff closer than me (Coywolf News is a great site), but I will casually defend this push towards Markdown as a lingua franca of the Web. (Not the agentic Web. Just the Web. More on that later.) I actually think itâ€™s really a great move for publishers that comes with way fewer inherent issues than Google AMP ever did.For one thing, this is all standards-based, not something that was just invented that you need to manage. Itâ€™s literally using existing content negotiation headers that web servers already support, not forcing folks to learn something new. Plus itâ€™s hard to argue with a point like this from Vercel:A typical blog post weighs 500KB with all the HTML, CSS, and JavaScript. However, the same content as Markdown is only 2KB. Thatâ€™s a 99.6% reduction in payload size.Thatâ€™s good for budget-minded AI agents, but itâ€™s also good for people who run websites.Additionally, Markdown has been in increasingly wide use for 20 years, and it keeps growing in popularityâ€”and unlike the weird carousels and oddly specific rules of Google AMP, lots of people know how to use it. And the use of headers to deliver Markdown pages is already baked into Web standards, just waiting for folks to use it. Sponsored By â€¦ You? If you find weird or unusual topics like this super-fascinating, the best way to tell us is to give us a nod on Ko-Fi. It helps ensure that we can keep this machine moving, support outside writers, and bring on the tools to support our writing. (Also itâ€™s heartening when someone chips in.)We accept advertising, too! Check out this page to learn more.OK, so how many of these servers are getting flooded with requests from AI agents right now? (DepositPhotos.com)This is really a tactic to help site owners avoid an AI-generated hug of deathPlus, thereâ€™s the renderingâ€”Markdown is an antidote to the internet we currently run, which is highly dependent on programming languages and visual tricks that AI agents and honestly most people donâ€™t even need. To me, when I see, â€œCloudflare wants to give every webpage a Markdown version,â€ my thought is essentially, â€œOh, they want to make AI agents stop DDoSing these poor PHP servers that still dominate the internet.â€When I see publishers talking about how their sites are getting flooded with viewers and getting slammed with unwanted hosting bills, it is clear that what we are doing is not tenable. Having Cloudflare put up a static Markdown file that takes up less space and has 0% of the JavaScript of the main page sounds like a win to me.And if youâ€™re building your pages semantically, as many publishers are likely already doing because they want to rank on Google, converting all that content to Markdown is going to be a cinch. Frequent Tedium skepticism target Matt Mullenweg is pushing for its addition to the WordPress.org website.Just imagine, if youâ€™re running an open-source project, and you didnâ€™t have to force your users to see a loading page with anime characters just to keep the site online. Instead, you could tell Claude and Gemini and Perplexity to grab the data in a format they already use, and serve that in a static form, saving your poor forum from being drowned in dynamic requests.There are lots of ethical qualms with AI, and you may want to just block them entirely, as is your right as a site owner. But I think diminishing a new-every-load HTML page to an unchanging Markdown file could save a lot of processing cycles for legacy server owners who have been trying to keep an extremely popular wiki online for 20 years.I think there are websites and forums out there that have been absolutely wrecked by the rise of AI. Cloudflare, while still facing periodic reputational issues, has offered itself up as a line of defense for publishers. Thatâ€™s nobleâ€”and while I get not everyone likes them, I think this particular offering is a good-for-the-internet move long-term.And while weâ€™re at it, letâ€™s start printing books in Markdown. Yeah! Letâ€™s really take this idea to an extreme! (DepositPhotos.com)But hear me out: What if we just offered our pages in Markdown because it made the internet more accessible?Yes, the reason for all of this is AI, because everything is about AI right now, but honestly, it would be a really awesome thing to offer for regular users, too.Recently, Iâ€™ve been trying to take on a project with the Tedium websiteâ€”itâ€™s not quite done yet, but Iâ€™m trying to get the whole thing onto the AT Protocol, mimicking my upload of my Twitter archive to Bluesky. (Iâ€™ve gotten the upload to work, itâ€™s just the details that need to be tweaked. Hereâ€™s a sample post that came out okay.) Iâ€™m using a tool called Sequoia, which makes it possible to plug a static site into the same protocol Bluesky uses. It parses the roughly 1,300 pages and then uploads them to a server on the ATmosphere.Genuinely cool to see this kind of collab happening in the ATmosphere.At the center of this is something called Standard.site, which aims to make a space for long-form content on the AT protocol. Itâ€™s not prescriptive to Markdown, though you could use it to share posts in Markdown if you wanted. It sounds promisingâ€”and like the budding efforts in the fediverse, it aims to make content easier to discover. Which is the problem RSS hoped to solve a quarter-century ago, admittedlyâ€”but this is doing it with more glue.To me, I see a connection between the push to make Markdown an undercurrent of the agentic Web and this weird experiment on the fringes of emerging social tech. And honestly I would not be surprised if web browsers plugged into these AI-targeted Markdown feeds to give users a lightweight experience. (You know what else could use this?!? Email.)Itâ€™s so fascinating, seeing this thing Iâ€™ve come to really appreciate as a writer turn into this ad-hoc building block of the modern internet. Even if I find it uncomfortable that AI is the vessel it rode in on.When I found it, it was my superpowerâ€”the tool I used to plow through five articles a day at a new job. It was the cruft-buster, the starting point, the README file. And now itâ€™s become something else entirelyâ€”something that could get us back to basics without the extra cruft of AMP or the stress of Core Web Vitals. (And even better, that didnâ€™t come from Google.)Honestly, Iâ€™m kind of here for it.Update 02/18/2026Looks like Iâ€™m not the only one thinking in this direction. Shout-out to Brett Terpstra, a guy who knows a thing or two about Markdown.Markdown-Free LinksWeâ€™ve been losing a lot of good music folks of late, most recently Billy Steinberg, the dude who wrote â€œLike A Virginâ€ and â€œI Touch Myself.â€ Fortunately, friend of Tedium Chris Dalla Riva got to chat with him in 2023.I know a fellow traveler when I see one, and with that in mind I want to give a shout to Rabbit Hole, a new-ish YouTube channel that recently asked why office chairs have five legs. A promising start.Also, we have to mention Jesse Jackson, a civil rights icon and easily the most well-known â€œshadow senatorâ€ in U.S. history.--Find this one an interesting read? Share it with a pal!And a quick shout to our sponsor la machine, which doesnâ€™t support Markdown, but has a good reason for not doing so."
    },
    {
      "title": "tedium.co: Project Code Name",
      "url": "https://feed.tedium.co/link/15204/17277502/corporate-turnaround-code-names-history",
      "source": "tedium.co",
      "time": "2026-02-20T10:18:10.532827",
      "raw_desc": "Why do corporate restructuring plans get code names the way operating systems do? And why are the names often so bizarre?Today in Tedium: Recently, Amazon did something kind of annoying in the midst of doing something painful. It laid off a ton of people, but in the midst of doing that, it accidentally dropped an email revealing the layoffs early, before people got laid off. That email revealed that this layoff had an official code name, â€œProject Dawn,â€ which presumably speaks to the idea of wiping the grime away, like dish soap. It sounds insane, but companies have been taught to name initiatives after random things for decades, sometimes to celebrate successful initiatives, sometimes to lay off thousands of people. (Iâ€™m sure Will Lewis named the recent Washington Post layoff endeavor â€œProject Zoom Ghosting.â€) Why are they such a corporate fixationâ€”even for layoffs? Todayâ€™s Tedium ponders why corporate culture is so dominated by code names. â€” Ernie @ Tediumâ€œ[W]hen several stations are connected by the same wire, the attention of the particular station for which the message is destined must be secured. This is done by signalling, not the full name of the station, which would occupy time, but an abbreviated name, consisting of two or three letters, assigned to that particular station and known as its code name. Thus, LV is Liverpool, EH Edinburgh, and so on.â€â€” A passage from a 1888 issue of The English Illustrated Magazine, a turn-of-the-century periodical, discussing how the British Post Office used code names to help make sense of the complexities of the telegraph system. (This appears to be one of the first uses of the term â€œcode name.â€) Eventually code names would expand to businesses in general, with New York City setting up a central bureau for registered addresses in 1919, with the goal of avoiding mix-ups on the telegraph line. (Think of them as the domain names of the 1920s.)Was this the type of â€œDawnâ€ that Amazon was referring to? (DepositPhotos.com)The code name has become an essential part of how the tech industry operatesWhen youâ€™re building a project, and you donâ€™t quite know where it is and what itâ€™s going to turn into yet, a code name can be quite an asset. Itâ€™s a tool that can help a project coalesce around a set of ideas, and it doesnâ€™t necessarily need to be something that the public ever sees.In fact, it may actually be better if the public never knows about them. Often, you donâ€™t want to reveal something while itâ€™s incubating. As the Tumblr site Ask a Gamedev put it in 2022:Itâ€™s important to note that the reason for secrecy is primarily for marketing purposes. We want to keep a big project quiet until weâ€™re ready to show it and get players excited for it. If our product is tied in with another product or IP with a big planned push at some point in the future, tipping our hand too early can lead to a cascading set of reveals we or our business partners were unready to make. For example, revealing a new mainline PokÃ©mon game too early would spill the beans on an entire new PokÃ©mon generation, which would affect merchandise, animated series, and so on. As a result, we usually put in safeguards to prevent such leaks from happening, both punitive and practical.Code names, also known as code words, have a long history that often criss-crosses through the two World Wars, and perhaps through some of the worldâ€™s largest intelligence agencies. That they bled into business is not wholly surprising, as large companies deal in trade secrets all the timeâ€”even fast-food chicken restaurants.But whatâ€™s unusual is that, particularly in the technology industry, these code names often have a long shelf life, one that can stick around for years after the fact. The word Mozilla, the name of the company that produces Firefox, started as the code name of Netscape Navigator, the web browser upon which Firefox is based.Aspire to look this cool. (San Francisco Chronicle/Newspapers.com)It wasnâ€™t like the Netscape team hid itâ€”back in the â€™90s, employees of the company actually decked out Mozilla gear in photos for the San Francisco Chronicle.â€œEvery great project starts out with a T-shirt, and to make a good T-shirt you need a good code name, something like â€˜Terminatorâ€™ or â€˜T-Rex,â€™â€ Gene Wang, a software development manager at Symantec, told the Chronicle in 1996. (Apparently he was not aware Mozilla already had the dinosaur metaphor covered.)The technology industry has long been shaped by code names, to the point where those code names break out of their holding cage and end up defining the product. Apple in particular is infamous for this, with the animal and landmark names that define MacOS starting as code names but eventually becoming product names.You used Chicago, but probably didnâ€™t even realize it.Operating systems are a natural reason to have a code name, by the way. In a Bluesky thread from 2024, Microsoft old hand Larry Osterman, who has been at the company for more than 40 years, explained how these code names, such as â€œChicago,â€ the nickname for Windows 1995, would bleed into the public discourse. They existed because these were lengthy projects that existed before the marketing team had weighed in on a name. However, the dynamic that necessitated these monikers has faded somewhat.â€œCode names leak. Both to the public and into other artifacts (files with code names in them, config settings, etc.),â€ he wrote, explaining why references to Chicago appear in the operating system. â€œAnd in a world where you release every 3-6 months, you really donâ€™t need code names, because the release is so small.â€(Someone tell that to the Ubuntu team, which famously gives its twice-yearly iterations alliterative animal-themed code names, most recently â€œQuesting Quokka.â€ I imagine that must get hard to plan for on Q releases.)And while software release schedules have gotten faster, internal projects still need code names, and sometimes there are so many code names that you need a system to manage them. In a 2007 blog post, Stack Exchange co-founder Jeff Atwood said his team went through so many that they had to develop a system to generate new ones.â€œThe names are chosen alphabetically from a set of items; every new project gets a name from the set,â€ he wrote. â€œWe start with A, and when we finally arrive at Z, we pick a new set of items for project name inspiration.â€Microsoft has so many code names that it has a quite-long Wikipedia page dedicated to them. So does Apple.But as far as I can tell, they have yet to give their layoffs a code name.â€œI donâ€™t want to spell out the idea on the insecure email channel. Even if you say the phrase â€˜Video H or Nâ€™ in public, people will know what you mean, so letâ€™s just call it Video. Donâ€™t say it in combination with the H or N site :)â€â€” Jawed Karim, one of the co-founders of YouTube, discussing (according to Internal Tech Emails) how the trio of co-founders, still at PayPal, should talk about their formative idea that would end up taking over the world. (Whatâ€™s â€œH or Nâ€? Easy: The platform was originally intended to be a HotOrNot for video. That was too specificâ€”but it ended up inspiring the actual idea.)Iâ€™m not sure who I feel worse forâ€”the people this stock photo represents, or the stock photo model themselves, knowing that theyâ€™re going to represent layoffs until the end of time. (DepositPhotos.com)But enough about startups and operating systems. Do companies really give their layoffs code names?When a company is attempting to do something sensitive, like poach a CEO, itâ€™s likely they may not want to spell out exactly what is going on before they pull the trigger.Case in point: In the months before Yahoo! brought on Marissa Mayer as their CEO, Mayer had to frequently talk about the shift in secret. She was still working at Google, and there was also the risk the news would leak to the media. So what did she and Yahoo! do? They gave the initiative a code name, â€œProject Cardinal.â€ This allowed her to set up her exit plan in secret, while avoiding, say, tipping off her limo driver.Mayer got hired rather than fired in that situation, but one presumes that a similar motivation might lead a company to bury an initiative behind a code name.Just be happy that they arenâ€™t giving these restructuring code names matching logos. Or maybe they are, because thatâ€™s something a graphic designer wants to work on. A logo for layoffs. (Willis Lam/Flickr)It can also provide organizational cover when doing something that negatively harms employees. To offer an example: Red Robin recently announced the closure of a number of restaurants as part of a broader â€œNorth Starâ€ initiative aimed at improving service while fighting against a wave of shutdowns. (Theyâ€™re not alone: Last spring, the fast food chain Jack in the Box announced its â€œJack on Trackâ€ plan, which includes a â€œrestaurant closure program.â€)Eventually, things can get more serious than these situations, which require more intense strategizing. Thereâ€™s a term for what these companies are doing: â€œTurnaround management,â€ which refers to the optimization of businesses to stay solvent. Turnarounds can happen at any time in a businessâ€™ history, though the concept is most associated with businesses nearing bankruptcy.These plans can really hurt, as in the case of Fordâ€™s â€œWay Forward,â€ first undertaken in 2006. That plan involved more than 25,000 job cuts, which was dramatic and painfulâ€”but it helped Ford avoid the brutal bailouts General Motors and Chrysler required. In 2009, while those companies were barely holding on, Ford actually posted a profit.Which is to say that, while turnaround plans can seem callous and unfair to affected workers or even customers affected by decreased service, they can absolutely save companies. Itâ€™s a bloodletting tool.But thatâ€™s not to say every turnaround code name is a good one, and the worst ones can signal a sense of panic, as noted in a 2015 on the topic in Bloomberg (archive link).â€œThere are different degrees of distress,â€ turnaround consultant Margaret Bogenrief told the outlet. â€œGenerally, the more grandiose the name, the more severe the distress.â€This compass represents just one of the three restructuring initiatives General Mills had going on in the mid-2010s. (quimby/Fickr)Of course, there can be a degree of silliness that comes with anything complex and corporate. Around 2015, General Mills announced not one, not two, but three separate corporate restructuring projects: Project Compass, Project Century, and Project Catalyst. These three projects each touched on different parts of the company, with Project Compass focused on its international markets, Project Century its North American manufacturing, and Project Catalyst its organizational effectiveness. These projects cost hundreds of millions of dollars collectively and came with more than 1,500 layoffs. And to the layperson, it just sounds hopelessly complex.Kelloggâ€™s, meanwhile, had been down this road multiple times itself. In 2009, the company announced K-LEAN, an initiative to increase optimization (and which led to layoffs in its factories). Then, in 2013, they followed it up with Project K, which aimed to reorganize the various company segments â€¦ and which also led to layoffs.Ultimately, Kelloggâ€™s decided to split off its legacy cereal business into its own company, WK Kellogg Co., and rename the larger snack food business as Kellanova. Both companies ended up getting sold to large candy companies recentlyâ€”WJ Kellogg to Ferrero, Kellanova to Mars.With this framing, turnaround projects can be seen as corporate salvaging missions first, layoff plans second. But honestly, that struggles to explain Amazon, a company that made $21.2 billion in net income in the prior quarter alone. Sometimes a job cut is just a job cut, no turnaround necessary.Dude, youâ€™re getting a Dell â€¦ 2.0. (Casey Marshall/Flickr)Five typical naming schemes for corporate projectsLeaning into the puns and metaphors. Itâ€™s not just Jack in the Box leaning into the punny turnaround project names. Companies do this all the timeâ€” A 1991 Reuters wire story, for example, describes how Bank of America named an attempted acquisition of Security Pacific â€œProject Sunshine,â€ while another attempted merger was named after the companiesâ€™ local NHL teams. More recently, Panera announced its â€œPanera RISEâ€ strategy, which plays off its most popular product.Proposing an upgrade. Iâ€™ve traditionally been sort of snarky about Dell, a company that in my mind screams â€œenterpriseâ€ and â€œlack of creativity.â€ So I guess when I hear Dell once named a reorganization effort â€œDell 2.0,â€ the thought that runs through my head is, â€œchecks out.â€ More recently, Intel has also gone down this road with its IDM 2.0 project for expanding its manufacturingâ€”though Pat Gelsinger got booted before fully seeing it through.Dropping a strong hint. After hiring a new CEO good enough to make Howard Schultz go away, Starbucks leaned into a turnaround plan called â€œBack to Starbucks,â€ which is what it wants customers to do. (Similarly, JCPenneyâ€™s endeavor is called â€œYes, JCPenney.â€) This also works when the goal is laying people offâ€”as shown by Kelloggâ€™s K-LEAN, mentioned above.Setting a deadline. Turnaround strategies are often built around significant goals. Sometimes, those goals include datesâ€”and sometimes those dates are way out, as seen from German automaker Opel, which set a â€œDRIVE!2022â€ campaign way back in 2013. (They really wanted to get ahead of that date.) Some other examples include Air France-KLMâ€™s â€œPerform 2020,â€ and the Trump administrationâ€™s â€œProject 2025.â€ Yes, even political parties do it.New-agey abstraction. But most notably, so many turnaround project names fall into vague, shapeless territory, not necessarily setting a goal as much as a subtle hint as to where they want to go. General Millsâ€™ â€œProject Catalystâ€ is a great example of this, as is Red Robinâ€™s â€œOperation North Starâ€ and Amazonâ€™s aforementioned Project Dawn. Another example is â€œProject Phoenix,â€ set into place by Newell Brands, the owner of the Coleman, Rubbermaid, and Yankee Candle brands. Perhaps itâ€™s because â€œProject Mass Layoffâ€ doesnâ€™t roll off the tongue quite so easily.â€œSimplification often feels risky because it appears to be a contraction. But in a turnaround, complexity is a liability.â€â€” Daniel Schmeltz, a corporate transformation expert, writing in Fortune about why most corporate turnaround endeavors fail. In the piece, he argues that slow-going turnaround plans are typically the most unsuccessful. â€œHesitation and complexity are liabilities; clarity and rapid execution are non-negotiable,â€ he writes. â€œIn a time when so many companies are attempting a turnaround, by acting decisively, businesses can cut through inertia, rebuild momentum, and secure sustainable results.â€ If youâ€™re going to chop off a limb or two, get it over withâ€”perhaps with a little less creativity in the code name.I think code names naturally engender discomfort for people, in part because of what they represent. They are often used to hide something from view, and that thing can be nefarious, even troubling.Recently, Cleveland Guardians pitcher Emmanuel Clase has faced allegations that he was receiving money from â€œmicrobetsâ€ made on his own pitches. In other words, he was manipulating pitches to secure winning bets on himself, then gamblers were sending some of that money his way.When talking about this with one of the betters, they reportedly hid what they were doing by using coded language, like â€œroosterâ€ and â€œchicken.â€ Clase has attempted to claim that they were discussing cockfighting rather than pitching, an activity Clase also gambled on. (Interesting defense strategyâ€”I wasnâ€™t gambling on games, I was gambling on animal abuse.)Clase was (at least based on allegations in court records that he has denied) trying to get away with something nefarious. Broken down, the reason for the cloak-and-dagger stuff was not all that dissimilar to why Marissa Mayer did it.Ah, what the hell, letâ€™s dunk on Project Dawn one more time. (DepositPhotos.com)One might argue that many of the above listed companies were trying to shroud their not-so-friendly plans in friendly language. You canâ€™t quite say, â€œwe need to lay thousands of people off,â€ and you definitely canâ€™t say â€œwe want to lay thousands of people off.â€ But to cloak it in â€œPhoenix,â€ â€œDawn,â€ or â€œNorth Star,â€ it makes the bad news digestible. It makes room for a little compassion for the HR team as theyâ€™re delivering the bad news.Well, unless youâ€™re an executive for Amazon and you casually drop a doublespeak-style code name in everybodyâ€™s inbox.--Well, I guess I was feeling salty today! Find this one an interesting read? Share it with a pal.And thanks to our sponsor la machine, a machine that doesnâ€™t need a code nameâ€”because its purpose is transparent as can be."
    },
    {
      "title": "tedium.co: Design Deconstruction",
      "url": "https://feed.tedium.co/link/15204/17276365/text-based-design-mindset",
      "source": "tedium.co",
      "time": "2026-02-20T10:18:10.538697",
      "raw_desc": "Design is perhaps the software paradigm most wedded to the mouse and the GUI. But thereâ€™s no reason it canâ€™t be text-driven.To me, the hard part about being creative is that youâ€™re always trying to look for a new path.Sure, youâ€™ve done things a certain way for a long time, and itâ€™s worked for you. But itâ€™s hard not to want to dabble in new directions just to see where it takes you, and hope that it shakes out a new idea or two.Which is perhaps the reason Iâ€™ve started to fixate on a weird ideaâ€”that design tools might sometimes work better without an attached graphical interface. Rather than graphics in, graphics out, maybe sometimes it should be text in, graphics out.The myth about design is that itâ€™s a function of the creativity-driven right side of the brain. But I think thatâ€™s only half the story. See, with design, thereâ€™s a lot of hidden math involved. Ask your favorite newspaper or magazine designer about pica rulers and column lengths, and youâ€™ll get what Iâ€™m saying.Put another way: Designers need to be creative problem solvers, painting the perfect canvas, but they also need to be pragmatic, considering the realities of â€œyes, itâ€™s long, but we have to fit this text.â€Tools like InDesign and Final Cut Pro have traditionally combined the canvas and the broader frameworks that make a good design, mixing tools with differing cognitive loads into one interface. But what if design needs to be a bit more deconstructed, where pieces are more separated out, perhaps not even graphical? What if you designed with code? Would that lead to better results? I wanted to find out.Hey, you never know when youâ€™re gonna need a terminal in Android.The spark that caused my weird design-with-code obsessionI stumbled upon the idea accidentally, but this weird interest grew out of some genuine frustration.I wanted to try a couple of experiments with vertical video, seeing if I liked it and how comfortable I felt with the idea. The problem is, I wanted it to match my general style, which is strongly built around a heavily filtered grayscale imagery.Every app I tried kind of sucked. CapCut, the ByteDance-produced app for creating TikTok videos, seemed unstable. A lot of other stuff came with spammy upsells. Plus I couldnâ€™t quite get the design I wantedâ€”a faded black and white look thatâ€™s a little pixelated, with a slightly choppy frame count.The only thing I actually liked that could edit mobile videos was Canva. However, it could only get me so far. So, to fill the gap, I did something weird: I started testing whether I could filter videos with ffmpeg to my liking in Termux, the Linux terminal program for Android. Then, in a second step, Iâ€™d move the videos to Canva, to finish the edit (including adding the text in my desired font/design). And Iâ€™ll be damned, it worked:https://www.youtube.com/shorts/Cuyd8H2fvd4I became curious about pushing this idea further, to social objects, and started working on tools to build quick graphics from Markdown files all on my phoneâ€”something you can make happen with HTML and CSS, basically. Cool idea, worked pretty simply:I became curious about pushing this idea further, to social objects, and started working on tools to build quick graphics from Markdown files all on my phoneâ€”something you can make happen with HTML and CSS, basically. Cool idea, worked pretty simply:Every tech journalist in 1995 overestimated, then underestimated, the Zip drive.I thought that was enough, and I didnâ€™t need to take this unusual thought any further, until I saw something that blew my mind: a full YouTube videoâ€”complete with animation, graphics, and so on, made in a terminal. Sponsored By â€¦ You? If you find weird or unusual topics like this super-fascinating, the best way to tell us is to give us a nod on Ko-Fi. It helps ensure that we can keep this machine moving, support outside writers, and bring on the tools to support our writing. (Also itâ€™s heartening when someone chips in.)We accept advertising, too! Check out this page to learn more.This guy is nuts. I love what heâ€™s doing.The guy who edits videos with VimEven with my rendering experiment, thereâ€™s no way I would have said yes before a month ago, but then I saw something that really threw me for a loop: A dude who edits his YouTube videos in Vim.Look at this crazy-ass video. He made this in Vim!For the uninitiated, this is basically saying that you use scissors to cut a watermelon.I will admit it was by a guy named â€œVimjoyerâ€ whose gimmick is basically doing everything with the popular text editor. (I personally use nano like a lamer.) But fortunately, the how behind it doesnâ€™t need vim to be useful.Essentially, he is using a tool called Motion Canvas to push his content around so that he can create animations on the fly, shifting them around as desired. This is not totally dissimilar to what Flash could do with ActionScript back in the day, but itâ€™s deconstructed so itâ€™s code-first, GUI interface second.I was curious, so I started messing around with it using the same on-my-phone format as the earlier ffmpeg experiment. Alas, Motion Canvas didnâ€™t work all that well for such a constrained setting, as it required use of a browser. However, I spotted a similar tool, Remotion, that worked entirely within the command line.But one change precludes anotherâ€”it needed Playwright, a headless browser tool. As itâ€™s made, that doesnâ€™t work in Termux at all, as Playwright doesnâ€™t have any builds compatible with Qualcomm chips. But I found someone who had solved this exact problem, and that let me do this:I can write the copy for these social objects in Markdownâ€”even chain them togetherâ€”and have it make a bunch of social objects for me, all meticulously set up in my style.Sound like a lot of work to avoid working in a graphical interface? You bet your ass it is. On the plus side, you only really have to do a complex, repeatable task once (perhaps with some maintenance down the line).But the thing is, you can use tools like Claude Code to make these sorts of weird connections workâ€”and maybe tell them, after the agent insists you canâ€™t run Playwright on your phone, that itâ€™s actually possible. Then, if you want to dive in further, thatâ€™s when you take the time to learn it yourself and build upon the idea youâ€™ve been conjuring.(The trick Iâ€™ve been using lately: Tapping into the super-cheap DeepSeek Chat model via Claude Code Router, an implementation of Claude Code that lets you use models not made by Anthropic. That gives me additional room to screw around with oddball experiments like these, while being relatively minimal resource-wise. I put in $10 a month ago and have yet to run out, while still getting fairly decent results.)An example of what Typst can do. (via the Typst website)A new script for page layoutThis is a very cool idea, and itâ€™s more than just a novelty. I honestly believe this basic text-driven ideal could be taken to some amazing new frontiers. Lately, Iâ€™ve been fascinated by Typst, a scripting technology that is seen as a competitor to LaTeX.(Let me take a pause here to admit that LaTeX users have been designing with code for a long time. And there are probably some people who build stuff using PostScript they coded by hand. I bow before you, as a guy who started out as designer.)Itâ€™s a tool that is designed for laying out technical documents, with an emphasis on things like math equations. But it could also be used to make all sorts of documents, like zines or even wall calendars. This is actually the perfect format to build a wall calendar, because itâ€™s a highly templated format that can get very complex to manage in something like Affinity or InDesign. Hereâ€™s an example I built as a test:Longtime readers know that I have been threatening for years to sell a wall calendar, and 2027 might just be the year.But it goes further than that. To me, I think thereâ€™s an opportunity to separate concerns inventively. For example: Letâ€™s say you go into Affinity or Inkscape to build an SVG with the basic shape of your layout, or even a basic background, but then you import that graphic into Typst format. That moves you from texture to copy-layout. This is what I mean about separating concerns. Too often, design software tries to awkwardly mesh together these processes in a way that makes nobody happy.Typst wonâ€™t get you all the way there, I will admit. It does not currently support blend modes, for example, meaning that you have to import raster graphics or SVGs to handle all of that. Same with clipping paths and masks. But I think thereâ€™s a world where Typst could have all of these things, making it an effective publishing tool without forcing you in canvas mode when youâ€™d be better served by a framework.We have a pretty good text-based web design framework in the form of HTML, JavaScript, and CSS. With a few additions or some extensions, Typst could become that for print.Itâ€™s too bad the creator of Mou disappeared and took his projectâ€™s goodwill with him, because this was a genuinely influential idea. The popular blogging platform Ghost was initially based off of this design.Graphic designers are secretly left-brained peopleOne thing that I think people donâ€™t realize about graphic design, particularly the print form, is that itâ€™s creativity, but thereâ€™s also math going on. Itâ€™s not that far removed from architecture, if you think about it.Any newspaper designer will tell you about pica rulers and column inches until the cows come home. The secret about news design if that itâ€™s a bunch of right-brained people who can think left-brained when the moment shows itself.If you had asked me about this 15 years ago, I might have considered editorial design all right-brain thinking. But I think the left side of the brain was always there.I think the thing that ultimately made this all click was probably Markdown, particularly an editor that presented the split in a way I couldnâ€™t ignore. Fairly forgotten at this point, but deeply influential at the time, the 2010s-era MacOS Markdown editor Mou basically let you lay out Markdown and see the visual output in real time. The story of Mou ended in tearsâ€”the designer basically ghosted a bunch of people after a crowdfunding campaignâ€”but it still inspired me, personally. (The popular open-source editor MacDown, recently revived as MacDown 3000, is something of a spiritual successor to the defunct Mou.)Iâ€™ve been trying to figure out a way to convey all of this, probably, ever since I started ShortFormBlog in 2009. That site began with the provocative idea that you could design individual posts at a micro level rather than making absolutely everything look the sameâ€”as long as you were willing to give everything the right framework to work within.We can translate that idea to all sorts of objects. We just need to think beyond the parameters in front of us. Iâ€™m not quite at the level of Vim video editor guy just yet, but itâ€™s something to aspire to.Non-Designy LinksIâ€™ve been on the lookout for interesting tools that support Linux, and one I caught was Neep, a paid tool that removes noise from voice calls. Krisp has this killer feature, too, but it doesnâ€™t support Linux.Weâ€™ve lost some great musicians of late, particularly Greg Brown, the original guitarist of Cake, who wrote â€œThe Distance,â€ easily one of the best songs of the â€™90s. Still hods up. (Also, RIP to Brad Arnold of 3 Doors Down, who made an appearance in our â€œSongs About Supermanâ€ piece.)The AI-generated viral video of Brad Pitt and Tom Cruise fighting feels like a strong enough turning point for tech that Hollywood just lost its minds over it on Friday. Perhaps not a strong enough response.--Alright, thatâ€™s all Iâ€™ve got. Find this one an interesting read? Share it with a pal!And speaking of deconstructing things, you canâ€™t get more back-to-basics than the simple brilliance of la machine."
    },
    {
      "title": "micahflee.com: Is everyone in your Signal groups named something like \"E\" or \"ğŸ¥‘\"? Nicknames can help!",
      "url": "https://micahflee.com/are-your-signal-groups-full-of-people-name-things-like-l-or-sinicknames/",
      "source": "micahflee.com",
      "time": "2026-02-20T10:18:10.583258",
      "raw_desc": ""
    },
    {
      "title": "micahflee.com: Jeffrey Epstein Bought Books About Pedophilia, Woody Allen, and Trump",
      "url": "https://micahflee.com/jeffrey-epstein-bought-books-about-pedophilia-woody-allen-and-trump/",
      "source": "micahflee.com",
      "time": "2026-02-20T10:18:10.583258",
      "raw_desc": ""
    },
    {
      "title": "micahflee.com: How to easily dig through the Epstein files yourself",
      "url": "https://micahflee.com/how-to-easily-dig-through-the-epstein-files-yourself/",
      "source": "micahflee.com",
      "time": "2026-02-20T10:18:10.583258",
      "raw_desc": ""
    },
    {
      "title": "buttondown.com/hillelwayne: Stream of Consciousness Driven Development",
      "url": "https://buttondown.com/hillelwayne/archive/stream-of-consciousness-driven-development/",
      "source": "buttondown.com/hillelwayne",
      "time": "2026-02-20T10:18:10.608044",
      "raw_desc": ""
    },
    {
      "title": "buttondown.com/hillelwayne: Proving What's Possible",
      "url": "https://buttondown.com/hillelwayne/archive/proving-whats-possible/",
      "source": "buttondown.com/hillelwayne",
      "time": "2026-02-20T10:18:10.608044",
      "raw_desc": ""
    },
    {
      "title": "buttondown.com/hillelwayne: Logic for Programmers New Release and Next Steps",
      "url": "https://buttondown.com/hillelwayne/archive/logic-for-programmers-new-release-and-next-steps/",
      "source": "buttondown.com/hillelwayne",
      "time": "2026-02-20T10:18:10.608044",
      "raw_desc": ""
    },
    {
      "title": "geohot.github.io: AI is the Best Thing to Happen to Art",
      "url": "https://geohot.github.io//blog/jekyll/update/2026/02/19/ai-art.html",
      "source": "geohot.github.io",
      "time": "2026-02-20T10:18:10.635645",
      "raw_desc": "I watched this video about how AI has already ruined music. Her mom sent her a song and she told her mom it was AI. She played the song and it sounded like slop. It had inspired lyrics like:\n\nFrom quiet roots, a garden grows\nSheâ€™s got that light, and now it shows\nYes, she rises, and she glows\nOh, she rises, now she knows\n\nPure slop. Compare it to:\n\nIâ€™m in the cut acting crazy\nIâ€™m in the whip doing eighty\nOnly God can judge me\nAnd only she can save me\n\nNote that â€œcutâ€ and â€œwhipâ€ are not exactly words, but products of a culture. Ulysses is particularly hard to read because you donâ€™t know 1910â€™s Irish pop culture.\n\n\nI canâ€™t believe Marvel movies were popular. The first Iron Man was good, but by the time we got to Spider-Man: No Way Home it was practically a clip show with triple inside references and cringy fourth wall breaking humor. How it got a 8.1 on IMDB is beyond me, and just reduced my trust for IMDB.\nMarvel movies are also the easiest things to make with AI. Little story and long term coherence, no â€œprogression of the genreâ€, tons of eye candy special effects. Itâ€™s shocking with how large the budgets for them were that they couldnâ€™t pay a story guy a little.\nI felt similarly about Avatar 2, so much so that I rewrote the plot and didnâ€™t care enough to see Avatar 3.\n\n\nFor many people, all they want is slop. Iâ€™m sure Iâ€™ve written about it before, but I see a world where 95% of people end up basically wireheaded. The people who donâ€™t care about progression. The people who want to exist in their loops. They will find their loop and exist in it forever.\nAI can play an amazing game of chess. Someday AI will produce good code when the RLVR environments get set up correctly. But I donâ€™t see a path with current tech to not produce garbage art.\nArt is defined as what pushes the boundaries of civilization. AI tools will be used to help produce all the audio/visual art in the future (as computers have helped for a long time), but as long as civilization is human, the loci of control of good art will remain human.\n\n\nSo yea. Bad art will be cheap to make. If you want 100 more Marvel movies and uninspired deriviative pop music, there has never been a better time to be alive â€“ unless you wanted to make money producing that trash. That was never made by real artists anyway, just algorithmically driven sell outs. Does the focus group say they like giant spiders? Iâ€™m so glad AI will make that obsolete.\nArt is defined by what is expensive. What is rare. What is expectation breaking. What is embedded in a complex and thriving culture. Not slop produced by a parrot like Marvel movies."
    },
    {
      "title": "geohot.github.io: Cost of Housing",
      "url": "https://geohot.github.io//blog/jekyll/update/2026/02/16/cost-of-housing.html",
      "source": "geohot.github.io",
      "time": "2026-02-20T10:18:10.635645",
      "raw_desc": "Many people in America are complaining about the cost of housing. But do they understand the damage it will do if it prices go down?\nEveryone who owns a house will suffer. Some of those people donâ€™t even fully own the house, they have a mortgage. So when prices go down, they will be underwater, having put money for years into an asset that now has no value.\nItâ€™s simply out of the question for housing prices to go down. If you want to buy a house to live in, sorry. The boomers were told houses are appreciating assets, and now we must bend reality to make that true.\nUntil you solve this problem, you will never solve the housing affordability crisis. It has nothing to do with zoning, building costs, or environmental reviews. It has to do with people holding bags they need to dump on you."
    },
    {
      "title": "geohot.github.io: tiny corpâ€™s product â€“ a training box",
      "url": "https://geohot.github.io//blog/jekyll/update/2026/02/15/tiny-corp-product.html",
      "source": "geohot.github.io",
      "time": "2026-02-20T10:18:10.637693",
      "raw_desc": "Our new Hong Kong office.\n\nItâ€™s starting to shape up what tiny corpâ€™s product will be. Itâ€™s not much of a change from what we sell and do now, but the vision is clearer.\nEvery month, we see these LLMs become more and more human. However, thereâ€™s a major difference. They do not learn. Everyone has the same Claude/Codex/Kimi, with the same weights, the same desires, and the same biases. If current trends continue, the collapse in diversity will be staggering. To paraphrase:\n\nI think there is a world market for maybe five people.\n\nThis is not the future I want to live in.\n\n\nIf trends continue where thereâ€™s a single model with frozen weights and all learning is in-context, the cloud will win. Except in some highly latency sensitive (fighting robots) or connectivity critical (self driving cars) environments, it will be cheaper to run in batch on the cloud.\nThe enshittification that came to the web wonâ€™t be the driving force to local models. We either live in a world where open models are so bad even user-hostile closed models are better, or open models are good enough, and competition to run them through sites like openrouter will prevent enshittification.\nThe only way local models win is if thereâ€™s some value in full on learning per user or organization. At that point, with entirely different compute needing to run per user, local will beat out cloud.\nThe open question is if everything thatâ€™s unique about you can fit in a 10 kB CLAUDE.md. If thatâ€™s true, we have a pretty sad future ahead. Itâ€™s the Attack of the Clones, swarms of identical minds you have no say over all varying in a small boxed-in way. This isnâ€™t learning, itâ€™s costuming. Everyone who has used these things knows how little of an impact prompting makes compared to the model. Itâ€™s the Internet funneled into a little box you can edit on your profile. Write 3 paragraphs about what makes you unique.\nWe have to build for a future where that isnâ€™t true. 90% of people will choose the cloud, and what they will find is that they are no longer meaningfully in the loop. The dream is an AI product that will do your job for you while you continue to get paid. But this cannot exist, thatâ€™s way too much of a fee to pay to the middleman. If you choose the homogenous mind, you are superfluous and will be cut out. Is there anything uniquely valuable about you? And I mean honestly, not the self-esteem pumping speeches you may have heard in school. If thereâ€™s not, I have some bad news for youâ€¦\n\n\nWe already sell the hardware. Consumer GPUs still are the cheapest way to run models. Thereâ€™s tons of work required on the infrastructure. The frontend will be the future iterations of OpenClaw and opencode. But the key distinction from what you have today is that your tinybox will learn. It will update the weights based on its interactions with you. Like living things.\nThis is many years away. Currently, we are focused on large LLM training (even running these things is hard, have you tried to use vLLM not on NVIDIA?) and generic infrastructure for driving GPUs. But this is the long term idea.\nNot API keyed SaaS clones. Something that lives in your house and learns your values. Your child."
    },
    {
      "title": "paulgraham.com: Superlinear Returns",
      "url": "http://www.paulgraham.com/superlinear.html",
      "source": "paulgraham.com",
      "time": "2026-02-20T10:18:10.740417",
      "raw_desc": ""
    },
    {
      "title": "paulgraham.com: How to Do Great Work",
      "url": "http://www.paulgraham.com/greatwork.html",
      "source": "paulgraham.com",
      "time": "2026-02-20T10:18:10.740417",
      "raw_desc": ""
    },
    {
      "title": "paulgraham.com: How to Get New Ideas",
      "url": "http://www.paulgraham.com/getideas.html",
      "source": "paulgraham.com",
      "time": "2026-02-20T10:18:10.740417",
      "raw_desc": ""
    },
    {
      "title": "gilesthomas.com: Why smart instruction-following makes prompt injection easier",
      "url": "https://www.gilesthomas.com/2025/11/smart-instruction-following-and-prompt-injection",
      "source": "gilesthomas.com",
      "time": "2026-02-20T10:18:10.736110",
      "raw_desc": ""
    },
    {
      "title": "gilesthomas.com: Writing an LLM from scratch, part 28 -- training a base model from scratch on an RTX 3090",
      "url": "https://www.gilesthomas.com/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch",
      "source": "gilesthomas.com",
      "time": "2026-02-20T10:18:10.736110",
      "raw_desc": ""
    },
    {
      "title": "gilesthomas.com: Writing an LLM from scratch, part 29 -- using DistributedDataParallel to train a base model from scratch in the cloud",
      "url": "https://www.gilesthomas.com/2026/01/llm-from-scratch-29-ddp-training-a-base-model-in-the-cloud",
      "source": "gilesthomas.com",
      "time": "2026-02-20T10:18:10.736110",
      "raw_desc": ""
    },
    {
      "title": "minimaxir.com: Nano Banana Pro is the best AI image generator, with caveats",
      "url": "https://minimaxir.com/2025/12/nano-banana-pro/",
      "source": "minimaxir.com",
      "time": "2026-02-20T10:18:10.797515",
      "raw_desc": ""
    },
    {
      "title": "minimaxir.com: Nano Banana can be prompt engineered for extremely nuanced AI image generation",
      "url": "https://minimaxir.com/2025/11/nano-banana-prompts/",
      "source": "minimaxir.com",
      "time": "2026-02-20T10:18:10.797515",
      "raw_desc": ""
    },
    {
      "title": "minimaxir.com: Claude Haiku 4.5 does not appreciate my attempts to jailbreak it",
      "url": "https://minimaxir.com/2025/10/claude-haiku-jailbreak/",
      "source": "minimaxir.com",
      "time": "2026-02-20T10:18:10.797515",
      "raw_desc": ""
    },
    {
      "title": "wheresyoured.at: Premium: The AI Data Center Financial Crisis",
      "url": "https://www.wheresyoured.at/data-center-crisis/",
      "source": "wheresyoured.at",
      "time": "2026-02-20T10:18:10.994542",
      "raw_desc": ""
    },
    {
      "title": "wheresyoured.at: Premium: The Hater's Guide To Microsoft",
      "url": "https://www.wheresyoured.at/premium-the-haters-guide-to-microsoft/",
      "source": "wheresyoured.at",
      "time": "2026-02-20T10:18:10.994542",
      "raw_desc": ""
    },
    {
      "title": "wheresyoured.at: Premium: The Hater's Guide to Oracle",
      "url": "https://www.wheresyoured.at/haters-guide-oracle/",
      "source": "wheresyoured.at",
      "time": "2026-02-20T10:18:10.994542",
      "raw_desc": ""
    },
    {
      "title": "blog.jim-nielsen.com: A Few Rambling Observations on Care",
      "url": "https://blog.jim-nielsen.com/2026/observations-on-care/",
      "source": "blog.jim-nielsen.com",
      "time": "2026-02-20T10:18:11.109208",
      "raw_desc": ""
    },
    {
      "title": "blog.jim-nielsen.com: Unresponsive Buttons on My Fastest Hardware Ever",
      "url": "https://blog.jim-nielsen.com/2026/unresponsive-buttons/",
      "source": "blog.jim-nielsen.com",
      "time": "2026-02-20T10:18:11.109208",
      "raw_desc": ""
    },
    {
      "title": "blog.jim-nielsen.com: A Brief History of App Icons From Appleâ€™s Creator Studio",
      "url": "https://blog.jim-nielsen.com/2026/history-of-creator-studio-icons/",
      "source": "blog.jim-nielsen.com",
      "time": "2026-02-20T10:18:11.109208",
      "raw_desc": ""
    },
    {
      "title": "joanwestenberg.com: The case for gatekeeping, or: why medieval guilds had it figured out",
      "url": "https://www.joanwestenberg.com/the-case-for-gatekeeping-or-why-medieval-guilds-had-it-figured-out/",
      "source": "joanwestenberg.com",
      "time": "2026-02-20T10:18:11.121331",
      "raw_desc": ""
    },
    {
      "title": "joanwestenberg.com: The empire always falls",
      "url": "https://www.joanwestenberg.com/the-empire-always-falls/",
      "source": "joanwestenberg.com",
      "time": "2026-02-20T10:18:11.121331",
      "raw_desc": ""
    },
    {
      "title": "joanwestenberg.com: AI twitter's favourite lie: everyone wants to be a developer",
      "url": "https://www.joanwestenberg.com/ai-twitters-favourite-lie-everyone-wants-to-be-a-developer/",
      "source": "joanwestenberg.com",
      "time": "2026-02-20T10:18:11.121331",
      "raw_desc": ""
    },
    {
      "title": "dfarq.homeip.net: Office Space released Feb. 19, 1999",
      "url": "https://dfarq.homeip.net/office-space-released-feb-19-1999/?utm_source=rss&utm_medium=rss&utm_campaign=office-space-released-feb-19-1999",
      "source": "dfarq.homeip.net",
      "time": "2026-02-20T10:18:11.335243",
      "raw_desc": ""
    },
    {
      "title": "dfarq.homeip.net: Windows 2000 release date",
      "url": "https://dfarq.homeip.net/windows-2000-release-date/?utm_source=rss&utm_medium=rss&utm_campaign=windows-2000-release-date",
      "source": "dfarq.homeip.net",
      "time": "2026-02-20T10:18:11.335243",
      "raw_desc": ""
    },
    {
      "title": "dfarq.homeip.net: First BBS goes online Feb. 16, 1978",
      "url": "https://dfarq.homeip.net/first-bbs-goes-online-feb-16-1978/?utm_source=rss&utm_medium=rss&utm_campaign=first-bbs-goes-online-feb-16-1978",
      "source": "dfarq.homeip.net",
      "time": "2026-02-20T10:18:11.335243",
      "raw_desc": ""
    },
    {
      "title": "borretti.me: Some Data Should Be Code",
      "url": "https://borretti.me/article/some-data-should-be-code",
      "source": "borretti.me",
      "time": "2026-02-20T10:18:11.375452",
      "raw_desc": ""
    },
    {
      "title": "borretti.me: Letting Claude Play Text Adventures",
      "url": "https://borretti.me/article/letting-claude-play-text-adventures",
      "source": "borretti.me",
      "time": "2026-02-20T10:18:11.375452",
      "raw_desc": ""
    },
    {
      "title": "borretti.me: There Is No New Aesthetics",
      "url": "https://borretti.me/article/there-is-no-new-aesthetics",
      "source": "borretti.me",
      "time": "2026-02-20T10:18:11.375452",
      "raw_desc": ""
    },
    {
      "title": "geoffreylitt.com: Code like a surgeon",
      "url": "https://geoffreylitt.com/2025/10/24/code-like-a-surgeon.html",
      "source": "geoffreylitt.com",
      "time": "2026-02-20T10:18:11.482493",
      "raw_desc": "A lot of people say AI will make us all â€œmanagersâ€ or â€œeditorsâ€â€¦but I think this is a dangerously incomplete view!\nPersonally, Iâ€™m trying to code like a surgeon.\nA surgeon isnâ€™t a manager, they do the actual work! But their skills and time are highly leveraged with a support team that handles prep, secondary tasks, admin. The surgeon focuses on the important stuff they are uniquely good at.\nMy current goal with AI coding tools is to spend 100% of my time doing stuff that matters. (As a UI prototyper, that mostly means tinkering with design concepts.)\nIt turns out there are a LOT of secondary tasks which AI agents are now good enough to help out with. Some things Iâ€™m finding useful to hand off these days:\n\nBefore attempting a big task, write a guide to relevant areas of the codebase\nSpike out an attempt at a big change. Often I wonâ€™t use the result but Iâ€™ll review it as a sketch of where to go\nFix typescript errors or bugs which have a clear specification\nWrite documentation about what Iâ€™m building\n\nI often find it useful to run these secondary tasks async in the background â€“ while Iâ€™m eating lunch, or even literally overnight!\nWhen I sit down for a work session, I want to feel like a surgeon walking into a prepped operating room. Everything is ready for me to do what Iâ€™m good at.\nMind the autonomy slider\nNotably, there is a huge difference between how I use AI for primary vs secondary tasks.\nFor the core design prototyping work, I still do a lot of coding by hand, and when I do use AI, Iâ€™m more careful and in the details. I need fast feedback loops and good visibility. (eg, I like Cursor tab-complete here)\nWhereas for secondary tasks, Iâ€™m much much looser with it, happy to let an agent churn for hours in the background. The ability to get the job done eventually is the most important thing; speed and visibility matter less. Claude Code has been my go-to for long unsupervised sessions but Codex CLI is becoming a strong contender there too, possibly my new favorite.\nThese are very different work patterns! Reminds me of Andrej Karpathyâ€™s â€œautonomy sliderâ€ concept. Itâ€™s dangerous to conflate different parts of the autonomy spectrum â€“ the tools and mindset that are needed vary quite a lot.\nYour agent doesnâ€™t need a career trajectory\nThe â€œsoftware surgeonâ€ concept is a very old idea â€“ Fred Brooks attributes it to Harlan Mills in his 1975 classic â€œThe Mythical Man-Monthâ€. He talks about a â€œchief programmerâ€ who is supported by various staff including a â€œcopilotâ€ and various administrators. Of course, at the time, the idea was to have humans be in these support roles.\nOK, so there is a super obvious angle here, that â€œAI has now made this approach economically viable where it wasnâ€™t beforeâ€, yes yesâ€¦ but I am also noticing a more subtle thing at play, something to do with status hierarchies.\nA lot of the â€œsecondaryâ€ tasks are â€œgrunt workâ€, not the most intellectually fulfilling or creative part of the work. I have a strong preference for teams where everyone shares the grunt work; I hate the idea of giving all the grunt work to some lower-status members of the team. Yes, junior members will often have more grunt work, but they should also be given many interesting tasks to help them grow.\nWith AI this concern completely disappears! Now I can happily delegate pure grunt work. And the 24/7 availability is a big deal. I would never call a human intern at 11pm and tell them to have a research report on some code ready by 7amâ€¦ but here I am, commanding my agent to do just that!\nNotion is for surgeons?\nFinally Iâ€™ll mention a couple thoughts on how this approach to work intersects with my employer, Notion.\nFirst, as an employee, I find it incredibly valuable right now to work at a place that is bullish on AI coding tools. Having support for heavy use of AI coding tools, and a codebase thatâ€™s well setup for it, is enabling serious productivity gains for me â€“ especially as a newcomer to a big codebase.\nSecondly, as a product â€“ in a sense I would say we are trying to bring this way of working to a broader group of knowledge workers beyond programmers. When I think about how that will play out, I like the mental model of enabling everyone to â€œwork like a surgeonâ€.\nThe goal isnâ€™t to delegate your core work, itâ€™s to identify and delegate the secondary grunt work tasks, so you can focus on the main thing that matters.\n\nRelated reads\nIf you liked this perspective, you might enjoy reading these other posts Iâ€™ve written about the nature of human-AI collaboration:\n\nEnough AI copilots! We need AI HUDs: â€œanyone serious about designing for AI should consider non-copilot form factors that more directly extend the human mindâ€¦â€\nAI-generated tools can make programming more fun: â€œInstead, I used AI to build a custom debugger UIâ€¦ which made it more fun for me to do the coding myselfâ€¦â€\nChatGPT as muse, not oracle: â€œWhat if we were to think of LLMs not as tools for answering questions, but as tools for asking us questions and inspiring our creativity?"
    },
    {
      "title": "geoffreylitt.com: AI as teleportation",
      "url": "https://geoffreylitt.com/2025/09/10/ai-as-teleportation.html",
      "source": "geoffreylitt.com",
      "time": "2026-02-20T10:18:11.486358",
      "raw_desc": "Hereâ€™s a thought experiment for pondering the effects AI might have on society: What if we invented teleportation?\nA bit odd, I know, but bear with meâ€¦\n\nThe year is 2035. The Auto Go Instant (AGI) teleporter has been invented. You can now go anywhereâ€¦ instantly!\nAt first the tech is expensive and unreliable. Critics laugh. â€œHah, look at these stupid billionaires who canâ€™t spend a minute of their time moving around like the rest of us. And 5% of the time they end up in the wrong place, LOLâ€\nBut soon things get cheaper and better. The tech hits mass market.\nThere are huge benefits. Global commerce is supercharged. Instead of commuting, people can spend more time with family and friends. Pollution is way down. The AGI company runs a sweet commercial of people teleporting to see their parents one last time before they die.\nAt the same time, some weird things start happening.\nThe landscape starts reconfiguring around the new reality. Families move to remote cabins, just seconds away from urban amenities. The summit of Mt. Everest becomes crowded with influencers. (It turns out that if you stay just a few seconds, you can take a quick selfie without needing an oxygen mask!)\nPhysical health takes a hit for many people. Itâ€™s harder to justify walking or biking when you could just be there now.\nIn-between moments disappear. One moment youâ€™re at work, the next youâ€™re at your dinner table at home. No more time to reset or prepare for a new context.\nBut the biggest change is the loss of serendipity. When you teleport, you decide in advance where youâ€™re headed. You never run into an old friend on the street, or stop at a farmstand by the side of the road, or see a store you might want to stop into someday.\nTo modern teenagers, the idea of wandering out without an exact destination in mind becomes unthinkable. You start with the GPS coordinates, and then you justâ€¦ go.\nAdvocates of the new way point out that thereâ€™s nothing stopping anyone from choosing traditional methods for fun. And indeed, the cross-country road trip does see a mild resurgence as a hipster thing.\nBut when push comes to shove, most people struggle to make the time for wanderingâ€”our schedules are now arranged around an assumption of instant transport.\nThis isnâ€™t exactly to say that the old way was better. Most people can agree that teleportation a net win. Yet for those who remember, thereâ€™s a vague unease, a sense that something important was lost in the worldâ€¦.\n\nIn his book Technology and the Character of Everyday Life, the philosopher Albert Borgmann talks about wooden stoves in houses.\nWhat is a stove? Yes, it warms the houseâ€¦ but itâ€™s also so much more than that. You gotta cut the wood, you gotta start the fire in the morningâ€¦\nâ€œA stove used to furnish more than mere warmth. It was a focus, a hearth, a place that gathered the work and leisure of a family and gave the house of a center.â€\nWhen you switch to a modern central heating system, you cut out all these inconveniences. Fantastic!\nOh, and by the way, your family social life is totally differentâ€¦.. wait what?? Yes, the inconveniences were inconvenient. But they were also holding up something in your life and culture, and now theyâ€™re suddenly gone.\nI think of this as kind of a Chesteronâ€™s fence on hard mode. Yes, the stove was put there for warmth, that was the main goal. But you should also think hard about its secondary effects before replacing it.\n\nOK soâ€¦ how does this apply to AI?\nIâ€™m personally excited about AI and think it can improve our lives in a lot of ways. But at the same time Iâ€™m trying to be mindful of secondary effects and unintended consequences.\nHereâ€™s one example. If your mental model of reading is â€œtransmit facts into my headâ€, then reading an AI summary of something might seem like a more efficient way to get that task done.\nBut if your mental model of reading is â€œspend time marinating in a world of ideasâ€, then reducing the time spent reading doesnâ€™t help you much.\nThe point was the journey you underwent while reading, and you replaced it with teleportation.\nAnother example. One of the great joys of my life is having nerdy friends explain things to me. Now I can get explanations from AI with less friction, anytime, anywhere, with endless follow-up.\nEven if the AI explanations are â€œbetterâ€, thereâ€™s a social cost. I can try to mindfully nudge myself to still ask people questions, but now it requires more effort.\nFinal example: Iâ€™m trying to be mindful of the effects of vibe coding when designing software interfaces. On the one hand, it can really speed up my iteration loop and help me explore more ideas.\nBut at the same time, part of my design process is sitting with the details of the thing and uncovering it as I goâ€”more a muscle memory process than a conscious plan. Messing with this process can change the results in ways that are hard to predict!\nI guess the throughline for all of these examples is: sometimes the friction and inconvenience is where the good stuff happens. Gotta be very careful removing it.\n\nThe takeaway here isnâ€™t that â€œAI is badâ€. Iâ€™ll just say that Iâ€™m personally trying to be mindful about keeping good friction around.\nDuring COVID, we kinda got teleportation via Zoom for a while. I decided to â€œvirtual commuteâ€ every day, walking around the block to get some fresh air and a reset before/after work. This wasnâ€™t a big deal but I found it really helpful.\nAs AI makes a lot of things easier, itâ€™ll be interesting to ponder what kinds of new frictions weâ€™ll want to intentionally add to our lives. Teleportation isnâ€™t always the best answerâ€¦"
    },
    {
      "title": "geoffreylitt.com: Enough AI copilots! We need AI HUDs",
      "url": "https://geoffreylitt.com/2025/07/27/enough-ai-copilots-we-need-ai-huds.html",
      "source": "geoffreylitt.com",
      "time": "2026-02-20T10:18:11.492962",
      "raw_desc": "In my opinion, one of the best critiques of modern AI design comes from a 1992 talk by the researcher Mark Weiser where he ranted against â€œcopilotâ€ as a metaphor for AI.\nThis was 33 years ago, but itâ€™s still incredibly relevant for anyone designing with AI.\nWeiserâ€™s rant\nWeiser was speaking at an MIT Media Lab event on â€œinterface agentsâ€. They were grappling with many of the same issues weâ€™re discussing in 2025: how to make a personal assistant that automates tasks for you and knows your full context. They even had a human â€œbutlerâ€ on stage representing an AI agent.\nEveryone was super excited about thisâ€¦ except Weiser. He was opposed to the whole idea of agents! He gave this example: how should a computer help you fly a plane and avoid collisions?\nThe agentic option is a â€œcopilotâ€ â€” a virtual human who you talk with to get help flying the plane. If youâ€™re about to run into another plane it might yell at you â€œcollision, go right and down!â€\nWeiser offered a different option: design the cockpit so that the human pilot is naturally aware of their surroundings. In his words: â€œYouâ€™ll no more run into another airplane than you would try to walk through a wall.â€\nWeiserâ€™s goal was an â€œinvisible computer\"â€”not an assistant that grabs your attention, but a computer that fades into the background and becomes \"an extension of [your] bodyâ€.\n\n\nWeiserâ€™s 1992 slide on airplane interfaces\n\nHUDs\nThereâ€™s a tool in modern planes that I think nicely illustrates Weiserâ€™s philosophy: the Head-Up Display (HUD), which overlays flight info like the horizon and altitude on a transparent display directly in the pilotâ€™s field of view.\nA HUD feels completely different from a copilot! You donâ€™t talk to it. Itâ€™s literally part invisibleâ€”you just become naturally aware of more things, as if you had magic eyes.\n\nDesigning HUDs\nOK enough analogies. What might a HUD feel like in modern software design?\nOne familiar example is spellcheck. Think about it: spellcheck isnâ€™t designed as a â€œvirtual collaboratorâ€ talking to you about your spelling. It just instantly adds red squigglies when you misspell something! You now have a new sense you didnâ€™t have before. Itâ€™s a HUD.\n(This example comes from Jeffrey Heerâ€™s excellent Agency plus Automation paper. We may not consider spellcheck an AI feature today, but itâ€™s still a fuzzy algorithm under the hood.)\n\n\nSpellcheck makes you aware of misspelled words without an â€œassistantâ€ interface.\n\nHereâ€™s another personal example from AI coding. Letâ€™s say you want to fix a bug. The obvious â€œcopilotâ€ way is to open an agent chat and ask it to do the fix.\nBut thereâ€™s another approach Iâ€™ve found more powerful at times: use AI to build a custom debugger UI which visualizes the behavior of my program! In one example, I built a hacker-themed debug view of a Prolog interpreter.\nWith the debugger, I have a HUD! I have new senses, I can see how my program runs. The HUD extends beyond the narrow task of fixing the bug. I can ambiently build up my own understanding, spotting new problems and opportunities.\n\nBoth the spellchecker and custom debuggers show that automation / â€œvirtual assistantâ€ isnâ€™t the only possible UI. We can instead use tech to build better HUDs that enhance our human senses.\nTradeoffs\nI donâ€™t believe HUDs are universally better than copilots! But I do believe anyone serious about designing for AI should consider non-copilot form factors that more directly extend the human mind.\nSo when should we use one or the other? I think itâ€™s quite tricky to answer that, but we can try to use the airplane analogy for some intuition:\nWhen pilots just want the plane to fly straight and level, they fully delegate that task to an autopilot, which is close to a â€œvirtual copilotâ€. But if the plane just hit a flock of birds and needs to land in the Hudson, the pilot is going to take manual control, and we better hope they have great instruments that help them understand the situation.\nIn other words: routine predictable work might make sense to delegate to a virtual copilot / assistant. But when youâ€™re shooting for extraordinary outcomes, perhaps the best bet is to equip human experts with new superpowers.\n\nFurther reading\n\nA nice discussion of one approach to this idea can be found in Using Artificial Intelligence to Augment Human Intelligence by Michael Nielsen and Shan Carter.\nA more cryptic take on the same topic: Is chat a good UI for AI? A Socratic dialogue\nA discussion of how the the HUD philosophy intersects with on-demand software creation: Malleable software in the age of LLMs"
    },
    {
      "title": "bogdanthegeek.github.io: World's Cheapest ARM Debugger is Actually RISC-V",
      "url": "https://bogdanthegeek.github.io/blog/projects/v003-dap/",
      "source": "bogdanthegeek.github.io",
      "time": "2026-02-20T10:18:11.684628",
      "raw_desc": ""
    },
    {
      "title": "bogdanthegeek.github.io: MicroAlloc",
      "url": "https://bogdanthegeek.github.io/blog/projects/microalloc/",
      "source": "bogdanthegeek.github.io",
      "time": "2026-02-20T10:18:11.684628",
      "raw_desc": ""
    },
    {
      "title": "bogdanthegeek.github.io: Hosting a WebSite on a Disposable Vape",
      "url": "https://bogdanthegeek.github.io/blog/projects/vapeserver/",
      "source": "bogdanthegeek.github.io",
      "time": "2026-02-20T10:18:11.684628",
      "raw_desc": ""
    },
    {
      "title": "susam.net: Deep Blue: Chess vs Programming",
      "url": "https://susam.net/deep-blue.html",
      "source": "susam.net",
      "time": "2026-02-20T10:18:11.760621",
      "raw_desc": ""
    },
    {
      "title": "susam.net: Soju User Delete Hash",
      "url": "https://susam.net/soju-user-delete-hash.html",
      "source": "susam.net",
      "time": "2026-02-20T10:18:11.760621",
      "raw_desc": ""
    },
    {
      "title": "susam.net: From ZNC to Soju",
      "url": "https://susam.net/from-znc-to-soju.html",
      "source": "susam.net",
      "time": "2026-02-20T10:18:11.760621",
      "raw_desc": ""
    },
    {
      "title": "jyn.dev: remotely unlocking an encrypted hard disk",
      "url": "https://jyn.dev/remotely-unlocking-an-encrypted-hard-disk/",
      "source": "jyn.dev",
      "time": "2026-02-20T10:18:11.885086",
      "raw_desc": "Your mission, should you choose to accept it, is to sneak into the earliest parts of the boot process, swap the startup config without breaking anything, and leave without a trace.\nAre you ready? Let's begin.\nthe setup\n\nIn which our heroes are introduced, and the scene is set.\nFor a very long time I had a beat-up old ThinkPad that couldnâ€™t hold a charge for the life of it, especially when running Windows. It tended to die a lot when I was traveling, and I travel a lot. To save battery when Iâ€™m away from home, I often ssh back into my home desktop, both so I have persistent state even if my laptop battery dies, and so I get much faster builds that donâ€™t kill the battery.\nThis has two small problems:\n\nSometimes my home loses power and the desktop shuts off.\nSometimes when the power comes back on it has a new public IP.\n\nFor a long time I solved 1. by enabling â€œPower On\" after \"Restore AC Power Lossâ€ in the BIOS and 2. with tailscale. However, I recently installed Arch with an encrypted boot partition, which means that boot doesnâ€™t finish until I type in the encryption password.\nWell. Well. What if I Simply put tailscale in initramfs?\nthe plan\n\nIn which our intrepid heroes chart the challenges to come.\ninitramfs\n\nOh, right. If you werenâ€™t aware, early boot in a Linux operating system1  is just running a full second operating system that happens to be very small, lol. Thatâ€™s loaded from a compressed archive file in /boot2 and run from memory, with no access to persistent storage. This OS running from memory is called initramfs (initial RAM filesystem).\nSo when you see a screen like this:\n\nThatâ€™s actually a whole-ass OS, with an init PID and service management and everything. This is how, for example, systemd-analyze can show you stats about early boot â€” thereâ€™s another copy of systemd running in initramfs, and it passes its state off to the one in the main OS.\nWell. That implies we can install things on it ^^.\nconstraints\n\nThereâ€™s three parts to this:\n\nNetworking in initramfs\nTailscale in initramfs\nSSH in initramfs\n\nWe also want to make this as secure as possible, so thereâ€™s some more things to consider:\n\nPutting tailscale in initramfs means that it has unencrypted keys lying around.\nTailscale keys expire (by default) after 90 days. At that point this will all break.\nYou really really donâ€™t want people to get SSH access to your early boot environment.\n\nWe can solve this in a few ways:\n\nUse Tailscale ACLs to only allow incoming connections to initramfs, not outgoing connections.\nSet the key to never expire.\nSet the SSH server to disallow all shells except the actual unlock command (systemd-tty-ask-password-agent).\n\ntailscale ACLs\n\nSome background about Tailscaleâ€™s ACLs (â€œaccess control listsâ€). Tailscaleâ€™s users are tied to their specific login method: you can, for example, add a passkey, but that passkey counts as a fully separate user than your original account. Tailscale also has â€œgroupsâ€ of users, which are what they sound like, â€œauto groupsâ€, which again are what they sound like, â€œhostsâ€, which are a machine connected to the network, and â€œtagsâ€.\nTags are odd, I haven't seen anything like them before. They group hosts, not users, and when you add a tag to a host, that counts as its login method, rather than the host being tied to a user account.\nA consequence of this is that the group autogroup:member does not include tagged machines, because tagged machines arenâ€™t tied to a user account. (A second consequence is that you canâ€™t remove all tags from a machine without logging out and logging back in to associate it with your user account.)\nSo we can write a policy like this:\n{\n  // Define the tags which can be applied to devices and by which users.\n  \"tagOwners\": {\n    \"tag:initrd\": [\"autogroup:admin\"],\n  },\n\n  // Define access control lists for users, groups, autogroups, tags,\n  // Tailscale IP addresses, and subnet ranges.\n  \"acls\": [\n    {\"action\": \"accept\", \"src\": [\"autogroup:member\"], \"dst\": [\"*:*\"]},\n  ],\n\n  // Test access rules every time they're saved.\n  \"tests\": [\n    {\n      \"src\":    \"100.76.34.8\", // outrageous-fortune\n      \"accept\": [\"100.102.101.127:22\", \"100.101.55.73:10078\"], // selene-initrd\n    },\n    {\n      \"src\":  \"100.102.101.127\", // selene-initrd\n      \"deny\": [\"100.101.55.73:10078\"], // selene\n    },\n  ],\n}\n\nThis says â€œallow devices tied to a user account to access any other device, and allow no permissions at all for devices tied to a tagâ€.\nselene here is my desktop, and selene-initrd is its initramfs.  3\nsystemd before boot\n\nBecause initramfs is just a (mostly) normal Linux system, that means it has its own init\nPID 1. On Arch, that PID is in fact just systemd. That means that we can add systemd\nservices to initramfs! There's a whole collection of them in\nmkinitcpio-systemd-extras\n(mkinitcpio is the tool Arch uses to regenerate initramfs).\nWe need two services: an SSH server (I went with\ndropbear)\nand something to turn on networking, which this collection names sd-network.\n\nIt's possible to run tailscale ssh directly, rather than having a separate SSH server, but\nI didn't find any way to configure tailscale's SSH command, and I don't want to let anyone\nhave a shell in my initramfs.\n\nthe heist\n\nIn which our heroes execute their plan flawlessly, sneaking in without a sound.\nIf you follow these steps on an Arch system, you should end up with roughly the same setup\nas I have. Most of these commands assume you are running as root.\n\n\nInstall the dropbear SSH server:\npacman -S dropbear\n\n\n\nInstall the systemd packages:\nyay -S mkinitcpio-systemd-extras mkinitcpio-tailscale\n\n\n\nAdd networking (sd-network), tailscale (tailscale), and dropbear (sd-dropbear) to\n/etc/mkinitcpio.conf:\n1c1\n< HOOKS=(base systemd autodetect microcode kms modconf block keyboard sd-vconsole plymouth sd-encrypt filesystems)\n---\n> HOOKS=(base systemd autodetect microcode kms modconf block keyboard sd-vconsole plymouth sd-network tailscale sd-dropbear sd-encrypt filesystems)\n\n\n\nSet up the keys for your new tailscale device:\nsetup-initcpio-tailscale\n\n\n\nIn the tailscale web console, mark your new\ndevice with tag:initrd, and disable key expiry. It should look something like this:\n\n\n\nIn /etc/mkinitcpio.conf, configure dropbear to only allow running the unlock command and nothing else:\nSD_DROPBEAR_COMMAND=\"systemd-tty-ask-password-agent\"\n\n\n\nTell systemd to wait forever for a decryption password. I use systemd-boot, so I edited\n/boot/loader/entries/linux-cachyos. Under options, I extended the existing\nrootflags=subvol=/@ to rootflags=subvol=/@,x-systemd.device-timeout=0. 4\n\n\nCopy your public keys into /root/.ssh/authorized_keys so they get picked up by the\ndropbear hook:\ncp ~/.ssh/authorized_keys /root/.ssh/\n\n\n\nGenerate a new public/private keypair for use by the dropbear server.\ndropbearkey -t ed25519 -f /etc/dropbear/dropbear_ed25519_host_key\n\n\n\n\nWithout this, the dropbear hook will try to load keys from openssh, which means they'll be shared between early boot and your normal server. In particular that would mean your SSH server private keys would be stored unencrypted in initramfs.\n\n\n\nSetup early networking.\n(Note: these instructions are only for Ethernet connections. If you want WiFi in early\nboot, good luck and godspeed.)\n\nAdd the following config in /etc/systemd/network-initramfs/10-wired.network:\n\n[Match]\nType=ether\n\n[Network]\nDHCP=yes\n\n\nRegister it in /etc/mkinitcpio.conf so it gets picked up by the sd-network hook:\n\nSD_NETWORK_CONFIG=/etc/systemd/network-initramfs\n\nAll this rigamarole is necessary because the OS doesn't set the network interfaces to\npredictable names until late boot, so it needs some way to know which interface to use.\n\n\nLast but not least, rebuild your initramfs: mkinitcpio -P.\n\n\nNext time you reboot, you should be able to ssh into $(hostname)-initrd and get a prompt\nthat looks like this:\n\nthe getaway\n\nIn which a moral is imparted, and our scene concluded.\nThe takeaway here is the same as in all my other posts: if you think something isn't\npossible to do with a computer, have you considered applying more violence?\n\n\n\nand I believe in Windows, although Iâ€™m less sure about that â†©\n\n\nsometimes /boot/EFI â†©\n\n\nHere â€œinitrdâ€ stands for â€œinitramdiskâ€, which is another word for our initramfs system. â†©\n\n\nSee the sd-dropbear\ndocs for more information about this. â†©"
    },
    {
      "title": "jyn.dev: pre-commit hooks are fundamentally broken",
      "url": "https://jyn.dev/pre-commit-hooks-are-fundamentally-broken/",
      "source": "jyn.dev",
      "time": "2026-02-20T10:18:11.928709",
      "raw_desc": "Let's start a new Rust project.\n$ mkdir best-fizzbuzz-ever\n$ cd best-fizzbuzz-ever\n$ cat << EOF > main.rs\nfn main() { for i in 0.. {\n    println (\"fizzbuzz\");\n}}\nEOF\n$ git init\nInitialized empty Git repository in /home/jyn/src/third-website/best-fizzbuzz-ever/.git/\n$ git add main.rs\n$ git commit --message fizzbuzz\n[main (root-commit) 661dc28] fizzbuzz\n 1 file changed, 4 insertions(+)\n create mode 100644 main.rs\n\nNeat. Now let's say I add this to some list of fizzbuzz projects in different languages.\nMaybe .... this one.\nThey tell me I need to have \"proper formatting\" and \"use consistent style\".\nHow rude.\nMaybe I can write a pre-commit hook that checks that for me?\n$ cat << 'EOF' > pre-commit\n#!/bin/sh\nset -eu\nfor f in *.rs; do\n  rustfmt --check \"$f\"\ndone\nEOF\n$ chmod +x pre-commit\n$ ln -s ../../pre-commit .git/hooks/pre-commit\n$ git add pre-commit\n$ git commit --message \"add pre-commit hook\"\nDiff in /home/jyn/src/third-website/best-fizzbuzz-ever/src/main.rs:1:\n-fn main() { for i in 0.. {\n-    println (\"fizzbuzz\");\n-}}\n+fn main() {\n+    for i in 0.. {\n+        println(\"fizzbuzz\");\n+    }\n+}\n\nNeat! Let's commit that change.\n$ rustfmt main.rs\n$ git commit --message \"add pre-commit hook\"\n[main 3be7b87] add pre-commit hook\n 1 file changed, 4 insertions(+)\n create mode 100755 pre-commit\n$ git status\nOn branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\tmodified:   main.rs\n\nOh ... We fixed the formatting, but we didn't actually stage the changes.\nThe pre-commit hook runs on the working tree, not on the index, so it didn't catch the issue.\nWe can see that the version tracked by git still has the wrong formatting:\n$ git show HEAD:main.rs\nfn main() { for i in 0.. {\n    println (\"fizzbuzz\");\n}}\n\nMaybe we can make the script smarter?\nLet's checkout all the files in the index into a temporary directory and run our pre-commit hook there. 1\n$ cat << 'EOF' > pre-commit\n#!/bin/sh\nset -eu\n\ntmpdir=$(mktemp -d --tmpdir \"$(basename \"$(realpath .)\")-pre-commit.XXXX\")\ntrap 'rm -r \"$tmpdir\"' EXIT\ngit checkout-index --all --prefix=\"$tmpdir/\"\nfor f in $tmpdir/*.rs; do\n  rustfmt --check \"$f\"\ndone\nEOF\n$ git add pre-commit\n$ git commit --message \"make pre-commit hook smarter\"\nDiff in /tmp/best-fizzbuzz-ever-pre-commit.ZNyw/main.rs:1:\n-fn main() { for i in 0.. {\n-    println (\"fizzbuzz\");\n-}}\n+fn main() {\n+    for i in 0.. {\n+        println(\"fizzbuzz\");\n+    }\n+}\n\n\nYay! That caught the issue.\nNow let's add our rust program to that collection of fizzbuzz programs.\n$ git add main.rs\n$ git commit --message \"make pre-commit hook smarter\"\n[main 3cb40f6] make pre-commit hook smarter\n 2 files changed, 11 insertions(+), 4 deletions(-)\n$ git remote add upstream https://github.com/joshkunz/fizzbuzz\n$ git fetch upstream\nremote: Enumerating objects: 222, done.\nremote: Total 222 (delta 0), reused 0 (delta 0), pack-reused 222 (from 1)\nReceiving objects: 100% (222/222), 29.08 KiB | 29.08 MiB/s, done.\nResolving deltas: 100% (117/117), done.\nFrom https://github.com/joshkunz/fizzbuzz\n * [new branch]      master     -> upstream/master\n$ git rebase upstream\nSuccessfully rebased and updated refs/heads/main.\n\nMaybe we'll make one last tweak...\n$ sed -i '1i // Written by jyn' main.rs\n$ git commit main.rs --message \"mark who wrote fizzbuzz\"\nDiff in /tmp/best-fizzbuzz-ever-pre-commit.n1Pj/fizzbuzz-traits.rs:4:\n use std::iter;\n\n struct FizzBuzz {\n-    from : i32\n-  , to : i32\n+    from: i32,\n+    to: i32,\n }\n\n impl FizzBuzz {\n\nUh. Huh. Right.\nThe code that was already here wasn't formatted according to rustfmt.\nOur script is running on every file in the git repo, so it won't let us commit.\nMaybe we can change it to only run on modified files?\n$ cat << 'EOF' > pre-commit\n#!/bin/sh\nset -eu\n\nfiles=$(git diff --name-only --cached --no-ext-diff --diff-filter=d)\n\ntmpdir=$(mktemp -d --tmpdir \"$(basename \"$(realpath .)\")-pre-commit.XXXX\")\ntrap 'rm -r \"$tmpdir\"' EXIT\n\nprintf %s \"$files\" | tr '\\n' '\\0' | xargs -0 git checkout-index --prefix=\"$tmpdir/\"\nfor f in $tmpdir/*.rs; do\n  rustfmt --check \"$f\"\ndone\nEOF\n$ git commit main.rs pre-commit \\\n  --message \"update main.rs; make pre-commit even smarter\"\n[main f2925bc] update main.rs; make pre-commit even smarter\n 2 files changed, 5 insertions(+), 1 deletion(-)\n\nAlright. Cool.\nLet's do one last thing.\nLet's say we had an existing PR to this repo and we need to rebase it.\nMaybe it had a merge conflict, or maybe there was a fix on main that we need in order to implement our solution.\n$ git checkout upstream/HEAD  # Simulate an old PR by checking out an old commit\nHEAD is now at 56bf3ab Adds E to the README\n$ echo 'fn main() { println!(\"this counts as fizzbuzz, right?\"); }' > print.rs\n$ git add print.rs\n$ git commit --message \"Add print.rs\"\n[detached HEAD 3d1bbf7] Add print.rs\n 1 file changed, 1 insertion(+)\n create mode 100644 print.rs\n\nAnd let's also say that we want to edit the commit message.\n$ git rebase -i main  # Rebase this whole branch over our main branch\nreword 3d1bbf7 Add print.rs\n# Rebase f2925bc..3d1bbf7 onto f2925bc (1 command)\n\nNow, we really have a problem.\n\nError: file `/tmp/best-fizzbuzz-ever-pre-commit.p3az/*.rs` does not exist\nCould not apply 3d1bbf7... Add print.rs\n\nTwo things went wrong here:\n\nOur pre-commit hook can't handle commits that don't have any Rust files.\nOur pre-commit hook ran on a branch we were rebasing. 2\n\nFixing the first thing doesn't really help us, because we don't control other people's branches.\nThey might have used git commit --no-verify.\nThey might not even have a pre-commit hook installed.\nThey might have had a branch that passed the hook when they originally wrote it, but not after a rebase (e.g. if your hook is cargo check or something like that).\nThey might have had a branch that used an old version of the hook that didn't have as many checks as a later version.\nOur only real choice here is to pass --no-verify to git rebase every time we run it, and to git commit for every commit in the rebase we modify,\nand possibly even to every git merge we run outside of a rebase.\nThis is because pre-commit hooks are a fundamentally broken idea.\nCode does not exist in isolation.\nCommits that are local to a developer machine do not ever go through CI.\nCommits don't even necessarily mean that that the code is ready to publishâ€”pre-commit hooks don't run on git stash for a reason!\nI don't use git stash, I use git commit so that my stashes are tied to a branch, and hooks completely break this workflow.\nMore than that, pre-commit hooks are preventing you from saving your work.\nThere should be a really, really good reason to prevent you from saving your work, and IMO \"doesn't pass the test suite\" is not that.\nI have similar feelings about format-on-save hooks.\nThere are a bunch\nof other footguns with pre-commit hooks.\nThis doesn't even count the fact that nearly all pre-commit hooks are implemented in a broken way and just blindly run on the worktree, and are slow or unreliable or both.\nDon't get me started on pre-commit hooks that try to add things to the commit you're about to make, or projects that try to automatically install a hook when you run the test suite.\n\nThe pre-commit framework (or its cousin, lint-staged) does not fix this.\nIt fixes the issues about running on the index by stashing your changes with\n--keep-index,\nwhich works but modifies your git state.\nIt doesn't fix the issues about running during a rebase, nor does it prevent hooks from trying to add things to the current commit. 3\n\"Just don't write bad hooks\" doesn't work if I'm working on someone else's project where I don't control the hook.\n\nPlease just don't use pre-commit hooks. Use pre-push instead. 4\npre-push hooks nearly avoid all of these issues.\nThe only use case where I think pre-commit hooks are a good idea is for things that must never committed, that are worth interrupting a complicated rebase to prevent; namely: credentials.\nOnce credentials are committed they're quite difficult to get out, and even harder to be sure you haven't missed them.\nTips for writing a pre-push hook\n\n\nRun on the index, not the working tree, as described above. 5\nOnly add checks that are fast and reliable. Checks that touch the network should never go in a hook. Checks that are slow and require an up-to-date build cache should never go in a hook. Checks that require credentials or a running local service should never go in a hook.\nBe as quiet as possible. This hook is running buried inside a bunch of other commands, often without the developer knowing that the hook is going to run. Don't hide other important output behind a wall of progress messages.\nDon't set the hook up automatically. Whatever tool you use that promises to make this reliable is wrong. There is not a way to do this reliably, and the number of times it's broken on me is more than I can count. Please just add docs for how to set it up manually, prominently featured in your CONTRIBUTING docs. (You do have contributing docs, right?)\n\n\nIf the hook does fail, and the changes affect an older commit than the most recent,\nyou can use a combination of git-absorb, git-revise,\nand git rebase -X ours --exec\nto put them in the appropriate commit before pushing again.\n\nAnd don't write pre-commit hooks!\n\n\n\nThis is really quite slow on large enough repos, but there's not any real alternative. git stash --keep-index messes with git index state and also with your stashes. The only VCS that exposes a FUSE filesystem of its commits is Sapling, which is poorly supported outside Facebook. The best you can do is give up on looking at the whole working copy and only write hooks that read a single file at a time. â†©\n\n\nBy default this doesn't happen when running bare rebase, but the second you add --interactive, nearly anything you do runs a hook. Hooks will also run when you attempt to resolve merge conflicts. â†©\n\n\nlint-staged does actually have a --fail-on-changes flag which aborts the commit, but that still modifies the working tree, and it's not on by default. â†©\n\n\nFor more info about the difference, and a full list of possible hooks, see man 5 githooks. â†©\n\n\nNotice that I don't say \"only run on changed files\". That's because it's not actually possible to reliably determine which branch the current commit is based on, the best you can do is pick a random branch that looks likely. â†©"
    },
    {
      "title": "jyn.dev: i'm just having fun",
      "url": "https://jyn.dev/i-m-just-having-fun/",
      "source": "jyn.dev",
      "time": "2026-02-20T10:18:11.948805",
      "raw_desc": "IT IS ONLY COMPUTER\n\n\nReilly Wood\n\n\ni work professionally on a compiler and write about build systems in my free time and as a result people often say things to me like \"reading your posts points to me how really smart you are\" or \"reading a lot of this shit makes me feel super small\". this makes me quite uncomfortable and is not the reaction i'm seeking when i write blog posts.\nit's not a competition\n\ni mean, in some sense if you work as a professional programmer it is a competition, because the job market sucks right now. but i think usually when people say they feel dumb, it's not in the sense of \"how am i supposed to get a job when jyn exists\" but more \"jyn can do things i can't and that makes me feel bad\".\nyou can do hard things\n\nall the things i know i learned by experimenting with them, or by reading books or posts or man pages or really obscure error messages. sometimes there's a trick to it but sometimes it's just hard work. i am not magic. you can learn these things too.\neveryone has their own area of specialization\n\nif you don't want to spend a bunch of time learning about how computers work, you don't have to! not knowing about gory computer internals does not make you dumb or computer illiterate or anything. everyone has their own specialty and mine is compilers and build systems. i don't know jack shit about economics or medicine! having a different specialty than me doesn't mean you're dumb.\ni really hate that computing and STEM have this mystique in our society. to the extent that engineering demonstrates intelligence, it's by repeatedly forcing you to confront the results of your own mistakes, in such a way that errors can't be ignored. there are lots of ways to do that which don't involve programming or college-level math! performance art and carpentry and running your own business or household all force you to confront your own mistakes in this way and deserve no less respect than STEM.\nif i can't feminize my compiler, what's the point?\n\nby and large, when i learn new things about computers, it's because i'm fucking around. the fucking around is the point. if all the writing helps people learn and come up with cool new ideas, that's neat too.\nhalf the time the fucking around is just to make people say \"jyn NO\". half the time it's because i want to make art with my code. i really, sincerely, believe that art is one of the most important uses for a computer.\ni'm not doing this for the money. i happened to get very lucky that my passion pays very well, but i got into this industry before realizing how much programmers actually make, and now that i work for a european company i don't make US tech salaries anyway. i do it for the love of the game.\nsome extracts from the jyn computer experience:\n\n\n\n\n\n\n\n\n\n\n\nmy advice\n\nyou really shouldn't take advice from me lol \nhowever! if you are determined to do so anyway, what i can do is point you towards:\nplaces to start fucking around and finding out\n\nhighest thing i can recommend is building a tool for yourself.\nmaybe it's a spreadsheet that saves you an hour of work a week.\nmaybe it's a little website you play around with.\nmaybe it's something in RPGmaker.\nthe exact thing doesn't matter, the important part is that it's fun and you have something real at the end of it,\nwhich motivates you to keep going even when the computer is breaking in three ways you didn't even know were possible.\nsecond thing i can recommend is looking at things other people have built.\nyou won't understand all of it and that's ok.\npick a part of it that looks interesting and do a deep dive on how it works.\ni can recommend the following places to look when you're getting started:\n\nMozilla Development Network\nArch Wiki\nStackOverflow\nalice maz, \"how I think when I think about programming\"\n\nmost importantly, remember:"
    },
    {
      "title": "eli.thegreenplace.net: Rewriting pycparser with the help of an LLM",
      "url": "https://eli.thegreenplace.net/2026/rewriting-pycparser-with-the-help-of-an-llm/",
      "source": "eli.thegreenplace.net",
      "time": "2026-02-20T10:18:11.958962",
      "raw_desc": "pycparser is my most widely used open\nsource project (with ~20M daily downloads from PyPI [1]). It's a pure-Python\nparser for the C programming language, producing ASTs inspired by Python's\nown. Until very recently, it's\nbeen using PLY: Python Lex-Yacc for\nthe core parsing.\nIn this post, I'll describe how I collaborated with an LLM coding agent (Codex)\nto help me rewrite pycparser to use a hand-written recursive-descent parser and\nremove the dependency on PLY. This has been an interesting experience and the\npost contains lots of information and is therefore quite long; if you're just\ninterested in the final result, check out the latest code of pycparser - the\nmain branch already has the new implementation.\n\n\nThe issues with the existing parser implementation\nWhile pycparser has been working well overall, there were a number of nagging\nissues that persisted over years.\n\nParsing strategy: YACC vs. hand-written recursive descent\nI began working on pycparser in 2008, and back then using a YACC-based approach\nfor parsing a whole language like C seemed like a no-brainer to me. Isn't this\nwhat everyone does when writing a serious parser? Besides, the K&R2 book\nfamously carries the entire grammar of the C99 language in an appendix - so it\nseemed like a simple matter of translating that to PLY-yacc syntax.\nAnd indeed, it wasn't too hard, though there definitely were some complications\nin building the ASTs for declarations (C's gnarliest part).\nShortly after completing pycparser, I got more and more interested in compilation\nand started learning about the different kinds of parsers more seriously. Over\ntime, I grew convinced that recursive descent is the way to\ngo - producing parsers that are easier to understand and maintain (and are often\nfaster!).\nIt all ties in to the benefits of dependencies in software projects as a\nfunction of effort.\nUsing parser generators is a heavy conceptual dependency: it's really nice\nwhen you have to churn out many parsers for small languages. But when you have\nto maintain a single, very complex parser, as part of a large project - the\nbenefits quickly dissipate and you're left with a substantial dependency that\nyou constantly grapple with.\n\n\nThe other issue with dependencies\nAnd then there are the usual problems with dependencies; dependencies get\nabandoned, and they may also develop security issues. Sometimes, both of these\nbecome true.\nMany years ago, pycparser forked and started vendoring its own version of PLY.\nThis was part of transitioning pycparser to a dual Python 2/3 code base when PLY\nwas slower to adapt. I believe this was the right decision, since PLY \"just\nworked\" and I didn't have to deal with active (and very tedious in the Python\necosystem, where packaging tools are replaced faster than dirty socks)\ndependency management.\nA couple of weeks ago this issue\nwas opened for pycparser. It turns out the some old PLY code triggers security\nchecks used by some Linux distributions; while this code was fixed in a later\ncommit of PLY, PLY itself was apparently abandoned and archived in late 2025.\nAnd guess what? That happened in the middle of a large rewrite of the package,\nso re-vendoring the pre-archiving commit seemed like a risky proposition.\nOn the issue it was suggested that \"hopefully the dependent packages move on to\na non-abandoned parser or implement their own\"; I originally laughed this idea\noff, but then it got me thinking... which is what this post is all about.\n\n\nGrowing complexity of parsing a messy language\nThe original K&R2 grammar for C99 had - famously - a single shift-reduce\nconflict having to do with dangling elses belonging to the most recent\nif statement. And indeed, other than the famous lexer hack\nused to deal with C's type name / ID ambiguity,\npycparser only had this single shift-reduce conflict.\nBut things got more complicated. Over the years, features were added that\nweren't strictly in the standard but were supported by all the industrial\ncompilers. The more advanced C11 and C23 standards weren't beholden to the\npromises of conflict-free YACC parsing (since almost no industrial-strength\ncompilers use YACC at this point), so all caution went out of the window.\nThe latest (PLY-based) release of pycparser has many reduce-reduce conflicts\n[2]; these are a severe maintenance hazard because it means the parsing rules\nessentially have to be tie-broken by order of appearance in the code. This is\nvery brittle; pycparser has only managed to maintain its stability and quality\nthrough its comprehensive test suite. Over time, it became harder and harder to\nextend, because YACC parsing rules have all kinds of spooky-action-at-a-distance\neffects. The straw that broke the camel's back was this PR which again proposed to\nincrease the number of reduce-reduce conflicts [3].\nThis - again - prompted me to think \"what if I just dump YACC and switch to\na hand-written recursive descent parser\", and here we are.\n\n\n\nThe mental roadblock\nNone of the challenges described above are new; I've been pondering them for\nmany years now, and yet biting the bullet and rewriting the parser didn't feel\nlike something I'd like to get into. By my private estimates it'd take at least\na week of deep heads-down work to port the gritty 2000 lines of YACC grammar\nrules to a recursive descent parser [4]. Moreover, it wouldn't be a\nparticularly fun project either - I didn't feel like I'd learn much new and\nmy interests have shifted away from this project. In short, the Potential well was just too deep.\n\n\nWhy would this even work? Tests\nI've definitely noticed the improvement in capabilities of LLM coding\nagents in the past few months, and many reputable people online rave about using\nthem for increasingly larger projects. That said, would an LLM agent really be\nable to accomplish such a complex project on its own? This isn't just a toy,\nit's thousands of lines of dense parsing code.\nWhat gave me hope is the concept of conformance suites mentioned by\nSimon Willison.\nAgents seem to do well when there's a very clear and rigid\ngoal function - such as a large, high-coverage conformance test suite.\nAnd pycparser has an very extensive one.\nOver 2500 lines of test code parsing various C snippets to ASTs with expected\nresults, grown over a decade and a half of real issues and bugs reported by\nusers.\nI figured the LLM can either succeed or fail and throw its hands up in despair,\nbut it's quite unlikely to produce a wrong port that would still pass all\nthe tests. So I set it to run.\n\n\nThe initial port\nI fired up Codex in pycparser's repository, and wrote this prompt just to\nmake sure it understands me and can run the tests:\nThis repository implements a Python parser producing an AST. It has a very\nextensive test suite; can you first verify you successfully run all the tests?\n(with python3 -m unittest discover, from the root dir)\n\nCodex figured it out (I gave it the exact command, after all!); my next\nprompt was the real thing [5]:\npycparser uses the PLY library for its lexer and parser, and its own AST\npackage for the ASTs created by the parser. I want to explore replacing the\nParser component (leaving the Lexer alone for now) with a hand written\nrecursive descent parser that will pass all the same tests. Can you try that?\n\nHere Codex went to work and churned for over an hour. Having never\nobserved an agent work for nearly this long, I kind of assumed it went off\nthe rails and will fail sooner or later. So I was rather surprised and skeptical\nwhen it eventually came back with:\nReplaced the PLY parse path with a handwritten recursiveâ€‘descent parser while\nkeeping the existing lexer and AST helpers intact; CParser now points to the\nnew RD implementation, with the old PLY version preserved as PLYCParser in\npycparser/c_parser.py. The RD parser includes a token stream, declarator/type\nparsing, expression precedence handling, pragmas, and statement parsing to\nmatch the previous AST output and coordinate behavior.\n\nIt took me a while to poke around the code and run it until I was convinced - it\nhad actually done it! It wrote a new recursive descent parser with only\nancillary dependencies on PLY, and that parser passed the test suite. After a\nfew more prompts, we've removed the ancillary dependencies and made the\nstructure clearer. I hadn't looked too deeply into code quality at this point,\nbut at least on the functional level - it succeeded. This was very impressive!\n\n\nA quick note on reviews and branches\nA change like the one described above is impossible to code-review as one PR in\nany meaningful way; so I used a different strategy. Before embarking on this\npath, I created a new branch and once Codex finished the initial rewrite, I\ncommitted this change, knowing that I will review it in detail, piece-by-piece\nlater on.\nEven though coding agents have their own notion of history and can \"revert\"\ncertain changes, I felt much safer relying on Git. In the worst case if all of\nthis goes south, I can nuke the branch and it's as if nothing ever happened.\nI was determined to only merge this branch onto main once I was fully\nsatisfied with the code. In what follows, I had to git reset several times\nwhen I didn't like the direction in which Codex was going. In hindsight, doing\nthis work in a branch was absolutely the right choice.\n\n\nThe long tail of goofs\nOnce I've sufficiently convinced myself that the new parser is actually working,\nI used Codex to similarly rewrite the lexer and get rid of the PLY dependency\nentirely, deleting it from the repository. Then, I started looking more deeply\ninto code quality - reading the code created by Codex and trying to wrap my head\naround it.\nAnd - oh my - this was quite the journey. Much has been written about the code\nproduced by agents, and much of it seems to be true. Maybe it's a setting I'm\nmissing (I'm not using my own custom AGENTS.md yet, for instance), but\nCodex seems to be that eager programmer that wants to get from A to B whatever\nthe cost. Readability, minimalism and code clarity are very much secondary\ngoals.\nUsing raise...except for control flow? Yep. Abusing Python's weak typing\n(like having None, false and other values all mean different things\nfor a given variable)? For sure. Spreading the logic of a complex function\nall over the place instead of putting all the key parts in a single switch\nstatement? You bet.\nMoreover, the agent is hilariously lazy. More than once I had to convince it\nto do something it initially said is impossible, and even insisted again in\nfollow-up messages. The anthropomorphization here is mildly concerning, to be\nhonest. I could never imagine I would be writing something like the following to\na computer, and yet - here we are: \"Remember how we moved X to Y before? You\ncan do it again for Z, definitely. Just try\".\nMy process was to see how I can instruct Codex to fix things, and intervene\nmyself (by rewriting code) as little as possible. I've mostly succeeded in\nthis, and did maybe 20% of the work myself.\nMy branch grew dozens of commits, falling into roughly these categories:\n\nThe code in X is too complex; why can't we do Y instead?\nThe use of X is needlessly convoluted; change Y to Z, and T to V in all\ninstances.\nThe code in X is unclear; please add a detailed comment - with examples - to\nexplain what it does.\n\nInterestingly, after doing (3), the agent was often more effective in giving\nthe code a \"fresh look\" and succeeding in either (1) or (2).\n\n\nThe end result\nEventually, after many hours spent in this process, I was reasonably pleased\nwith the code. It's far from perfect, of course, but taking the essential\ncomplexities into account, it's something I could see myself maintaining (with\nor without the help of an agent). I'm sure I'll find more ways to improve it\nin the future, but I have a reasonable degree of confidence that this will be\ndoable.\nIt passes all the tests, so I've been able to release a new version (3.00)\nwithout major issues so far. The only issue I've discovered is that some of\nCFFI's tests are overly precise about the phrasing of errors reported by\npycparser; this was an easy fix.\nThe new parser is also faster, by about 30% based on my benchmarks! This is\ntypical of recursive descent when compared with YACC-generated parsers, in my\nexperience. After reviewing the initial rewrite of the lexer, I've spent a while\ninstructing Codex on how to make it faster, and it worked reasonably well.\n\n\nFollowup - static typing\nWhile working on this, it became quite obvious that static typing would make the\nprocess easier. LLM coding agents really benefit from closed loops with strict\nguardrails (e.g. a test suite to pass), and type-annotations act as such.\nFor example, had pycparser already been type annotated, Codex would probably not\nhave overloaded values to multiple types (like None vs. False vs.\nothers).\nIn a followup, I asked Codex to type-annotate pycparser (running checks using\nty), and this was also a back-and-forth because the process exposed some\nissues that needed to be refactored. Time will tell, but hopefully it will make\nfurther changes in the project simpler for the agent.\nBased on this experience, I'd bet that coding agents will be somewhat more\neffective in strongly typed languages like Go, TypeScript and especially Rust.\n\n\nConclusions\nOverall, this project has been a really good experience, and I'm impressed with\nwhat modern LLM coding agents can do! While there's no reason to expect that\nprogress in this domain will stop, even if it does - these are already very\nuseful tools that can significantly improve programmer productivity.\nCould I have done this myself, without an agent's help? Sure. But it would have\ntaken me much longer, assuming that I could even muster the will and\nconcentration to engage in this project. I estimate it would take me at least\na week of full-time work (so 30-40 hours) spread over who knows how long to\naccomplish. With Codex, I put in an order of magnitude less work into this\n(around 4-5 hours, I'd estimate) and I'm happy with the result.\nIt was also fun. At least in one sense, my professional life can be described\nas the pursuit of focus, deep work and flow. It's not easy for me to get into\nthis state, but when I do I'm highly productive and find it very enjoyable.\nAgents really help me here. When I know I need to write some code and it's\nhard to get started, asking an agent to write a prototype is a great catalyst\nfor my motivation. Hence the meme at the beginning of the post.\n\nDoes code quality even matter?\nOne can't avoid a nagging question - does the quality of the code produced\nby agents even matter? Clearly, the agents themselves can understand it (if not\ntoday's agent, then at least next year's). Why worry about future\nmaintainability if the agent can maintain it? In other words, does it make sense\nto just go full vibe-coding?\nThis is a fair question, and one I don't have an answer to. Right now, for\nprojects I maintain and stand behind, it seems obvious to me that the code\nshould be fully understandable and accepted by me, and the agent is just a tool\nhelping me get to that state more efficiently. It's hard to say what the future\nholds here; it's going to interesting, for sure.\n\n\n\n\n[1]pycparser has a fair number of direct dependents,\nbut the majority of downloads comes through CFFI,\nwhich itself is a major building block for much of the Python ecosystem.\n\n\n\n\n\n[2]The table-building report says 177, but that's certainly an\nover-dramatization because it's common for a single conflict to\nmanifest in several ways.\n\n\n\n\n\n[3]It didn't help the PR's case that it was almost certainly vibe coded.\n\n\n\n\n\n[4]There was also the lexer to consider, but this seemed like a much\nsimpler job. My impression is that in the early days of computing,\nlex gained prominence because of strong regexp support which wasn't\nvery common yet. These days, with excellent regexp libraries\nexisting for pretty much every language, the added value of lex over\na custom regexp-based lexer\nisn't very high.\nThat said, it wouldn't make much sense to embark on a journey to rewrite\njust the lexer; the dependency on PLY would still remain, and besides,\nPLY's lexer and parser are designed to work well together. So it wouldn't\nhelp me much without tackling the parser beast.\n\n\n\n\n\n\n[5]I've decided to ask it to the port the parser first, leaving the lexer\nalone. This was to split the work into reasonable chunks. Besides, I\nfigured that the parser is the hard job anyway - if it succeeds in that,\nthe lexer should be easy. That assumption turned out to be correct."
    },
    {
      "title": "eli.thegreenplace.net: Compiling Scheme to WebAssembly",
      "url": "https://eli.thegreenplace.net/2026/compiling-scheme-to-webassembly/",
      "source": "eli.thegreenplace.net",
      "time": "2026-02-20T10:18:11.968527",
      "raw_desc": "One of my oldest open-source projects - Bob\n- has celebrated 15 a couple of months ago.\nBob is a suite of implementations of the Scheme programming language in Python,\nincluding an interpreter, a compiler and a VM. Back then I was doing some hacking\non CPython internals and was very curious about how CPython-like bytecode VMs\nwork; Bob was an experiment to find out, by implementing one from scratch for\nR5RS Scheme.\nSeveral months later I added a C++ VM to Bob,\nas an exercise to learn how such VMs are implemented in a low-level language\nwithout all the runtime support Python provides; most importantly, without the\nbuilt-in GC. The C++ VM in Bob implements its own mark-and-sweep GC.\nAfter many quiet years (with just a sprinkling of cosmetic changes, porting to\nGitHub, updates to Python 3, etc), I felt the itch to work on Bob again just\nbefore the holidays. Specifically, I decided to add another compiler to the\nsuite - this one from Scheme directly to WebAssembly.\nThe goals of this effort were two-fold:\n\nExperiment with lowering a real, high-level language like Scheme to\nWebAssembly. Experiments like the recent Let's Build a Compiler\ncompile toy languages that are at the C level (no runtime). Scheme has built-in\ndata structures, lexical closures, garbage collection, etc. It's much more challenging.\nGet some hands-on experience with the WASM GC extension [1]. I have several\nsamples of using WASM GC in the wasm-wat-samples repository,\nbut I really wanted to try it for something \"real\".\n\nWell, it's done now; here's an updated schematic of the Bob project:\n\nThe new part is the rightmost vertical path. A WasmCompiler\nclass lowers parsed Scheme expressions all the way down to WebAssembly text,\nwhich can then be compiled to a binary and executed using standard WASM tools [2].\n\nHighlights\nThe most interesting aspect of this project was working with WASM GC to\nrepresent Scheme objects. As long as we properly box/wrap all values in\nrefs, the underlying WASM execution environment will take care of the\nmemory management.\nFor Bob, here's how some key Scheme objects are represented:\n;; PAIR holds the car and cdr of a cons cell.\n(type $PAIR (struct (field (mut (ref null eq))) (field (mut (ref null eq)))))\n\n;; BOOL represents a Scheme boolean. zero -> false, nonzero -> true.\n(type $BOOL (struct (field i32)))\n\n;; SYMBOL represents a Scheme symbol. It holds an offset in linear memory\n;; and the length of the symbol name.\n(type $SYMBOL (struct (field i32) (field i32)))\n\n$PAIR is of particular interest, as it may contain arbitrary objects in\nits fields; (ref null eq) means \"a nullable reference to something that\nhas identity\". ref.test can be used to check - for a given\nreference - the run-time type of the value it refers to.\nYou may wonder - what about numeric values? Here WASM has a trick - the i31\ntype can be used to represent a reference to an integer, but without\nactually boxing it (one bit is used to distinguish such an object from a\nreal reference). So we don't need a separate type to hold references to numbers.\nAlso, the $SYMBOL type looks unusual - how is it represented with two\nnumbers? The key to the mystery is that WASM has no built-in support for\nstrings; they should be implemented manually using offsets to linear memory.\nThe Bob WASM compiler emits the string values of all symbols encountered into\nlinear memory, keeping track of the offset and length of each one; these are\nthe two numbers placed in $SYMBOL. This also allows to fairly easily\nimplement the string interning feature of Scheme; multiple instances of the\nsame symbol will only be allocated once.\nConsider this trivial Scheme snippet:\n(write '(10 20 foo bar))\n\nThe compiler emits the symbols \"foo\" and \"bar\" into linear memory as follows [3]:\n(data (i32.const 2048) \"foo\")\n(data (i32.const 2051) \"bar\")\n\nAnd looking for one of these addresses in the rest of the emitted code, we'll\nfind:\n(struct.new $SYMBOL (i32.const 2051) (i32.const 3))\n\nAs part of the code for constructing the constant cons list representing the\nargument to write; address 2051 and length 3: this is the symbol bar.\nSpeaking of write, implementing this builtin was quite interesting. For\ncompatibility with the other Bob implementations in my repository, write\nneeds to be able to print recursive representations of arbitrary Scheme values,\nincluding lists, symbols, etc.\nInitially I was reluctant to implement all of this functionality by hand in\nWASM text, but all alternatives ran into challenges:\n\nDeferring this to the host is difficult because the host environment has\nno access to WASM GC references - they are completely opaque.\nImplementing it in another language (maybe C?) and lowering to WASM is also\nchallenging for a similar reason - the other language is unlikely to have\na good representation of WASM GC objects.\n\nSo I bit the bullet and - with some AI help for the tedious parts - just wrote\nan implementation of write directly in WASM text; it wasn't really that\nbad. I import only two functions from the host:\n(import \"env\" \"write_char\" (func $write_char (param i32)))\n(import \"env\" \"write_i32\" (func $write_i32 (param i32)))\n\nThough emitting integers directly from WASM isn't hard,\nI figured this project already has enough code and some host help here would\nbe welcome. For all the rest, only the lowest level write_char is used.\nFor example, here's how booleans are emitted in the canonical Scheme notation\n(#t and #f):\n(func $emit_bool (param $b (ref $BOOL))\n    (call $emit (i32.const 35)) ;; '#'\n    (if (i32.eqz (struct.get $BOOL 0 (local.get $b)))\n        (then (call $emit (i32.const 102))) ;; 'f'\n        (else (call $emit (i32.const 116))) ;; 't'\n    )\n)\n\n\n\nConclusion\nThis was a really fun project, and I learned quite a bit about realistic code\nemission to WASM. Feel free to check out the source code of WasmCompiler - it's\nvery well documented. While it's a bit over 1000 LOC in total [4], more than half\nof that is actually WASM text snippets that implement the builtin types and\nfunctions needed by a basic Scheme implementation.\n\n\n\n\n[1]The GC proposal is documented here.\nIt was officially added to the WASM spec in Oct 2023.\n\n\n\n\n\n[2]In Bob this is currently done with bytecodealliance/wasm-tools for the\ntext-to-binary conversion and Node.js for the execution environment, but\nthis can change in the future.\nI actually wanted to use Python bindings to wasmtime, but these don't\nappear to support WASM GC yet.\n\n\n\n\n\n\n[3]2048 is just an arbitrary offset the compiler uses as the beginning of\nthe section for symbols in memory. We could\nalso use the multiple memories feature of WASM and dedicate a separate\nlinear memory just for symbols.\n\n\n\n\n\n[4]To be clear, this is just the WASM compiler class; it uses the Expr\nrepresentation of Scheme that is created by Bob's parser (and lexer);\nthe code of these other components is shared among all Bob\nimplementations and isn't counted here."
    },
    {
      "title": "eli.thegreenplace.net: Summary of reading: October - December 2025",
      "url": "https://eli.thegreenplace.net/2025/summary-of-reading-october-december-2025/",
      "source": "eli.thegreenplace.net",
      "time": "2026-02-20T10:18:11.970461",
      "raw_desc": "\"The Origins of Political Order: From Prehuman Times to the French Revolution\"\nby Francis Fukuyama - while reading this book it occurred to me that domains\nof study like political sciense must be incredibly difficult and frustrating.\nImagine trying to match a model onto a set of data; the model has thousands\nof parameters, but you only have dozens or a couple of hundred of data points.\nThis is what political sciense is like; there's a huge number of parameters\nand variables, far more than actual historical examples. And moreover, the\nhistorical examples are vague and often based on very partial memory and\nsketchy records. So books like this\nmost often just devolve to history. As a history book, this one isn't bad,\nbut I found it hard to draw wide conclusions from the themes it presents.\n\"Exploding the Phone: The Untold Story of the Teenagers and Outlaws Who\nHacked Ma Bell\" by Phil Lapsley - a detailed history of phone phreaking.\nWhile I wish it focused more on the technical details than on the legal\nescapades of well-known phreaks, it's still a good book that provides decent\ncoverage of an important era in the history of computing.\n\"The Zone\" by Sergei Dovlatov - (read in Russian) a satirical novella about\nthe life of a guard in a Soviet prison camp in the 1960s. I liked this book\nless than \"The Compromise\".\n\"The Joy of SET\" by McMahon and Gordon x3 - explores the various mathematical\ndimensions of the SET card game. It's surprising how much interesting math\nthere is around the game! Combinatorics and probability sure, but also\nmodular arithmetic, vectors, linear algebra and affine geometry. This is a fun\nbook for fans of the game (and of math); it's well written and even contains\nexercises. Don't expect it to teach you to become better at playing SET\nthough - that's really not its goal.\n\"Doom Guy: Life in First Person\" by John Romero - Romero's auto-biography,\nalso read by himself in the Audible version. Very good book, gives another\nangle at id software and the seminal games they developed. \"Masters of Doom\"\nis one of my favorite books, and this one complements it very nicely.\n\"Buffett: The Making of an American Capitalist\" by Roger Lowenstein - a\ndetailed biography of Warren Buffett. Great book, very informative and\ninteresting; the only issue is that it was written in 1995, and doesn't\nmention the last 30 years. It would be interesting to read an up-to-date\nbiography at some point.\n\"The Great Democracies: A History of the English Speaking Peoples, Volume IV\"\nby Winston Churchill - the final volume, covering the years 1815 - 1901.\nThere's still focus on England, but also coverage of the American civil war,\nAustralia, and some of Britain's colonial interests in Africa.\n\"Starburst and Luminary, an Apollo Memoir\" Don Eyles - the author worked on\ncoding the landing programs for the lunar module of several Apollo missions as\na young engineer in MIT. The book must be based on fairly detailed journals,\nbecause it contains an astonishing amount of detail (given that it was\nwritten 50 years after the events described). Pretty interesting insight into\nthat era, all in all, though I didn't care much about the author's mixing in\nhis love life into it. It's his book, of course, and he can write whatever he\nwants in it, but IMO it just dilutes the other great material and makes it\ngenerally less suitable for younger audiences.\n\"Stoner\" by John Williams - I have mixed feelings about this book, and they\nwill probably take (at least) another read to resolve. On one hand, the\nwriting is clearly masterful and \"mood-evoking\" in a way that only few\nauthors managed to do for me. Character development is beautiful, and there\nare glimpses of the flow of learning described amazingly well w.r.t. Stoner's\nown work. On the other hand, the characters are also too extreme - almost\ncaricatures, and not very well connected to each other. There are huge amounts\nof page real-estate allocated to certain topics that are barely mentioned\nlater on; this happens again and again. Edith, in particular, is a very\ntroubling character, and since Stoner is clearly presented as someone who\nis not a pushover when he wants to, his behavior is puzzling to me.\n\"The Magic Mountain\" by Thomas Mann. A young German college student arrives\nto a sanatorium in the Swiss Alps to visit his cousin who suffers from TB,\nand stays for years, chronicling the odd personas flowing through the\nestablishment. There's always some risk with trying famous books from over 100\nyears ago, and in this case the risk materialized - I found this one to be\ntedious, rambling and outdated. It's not all bad; there are certainly good\nparts, funny parts and some timeless lessons about human nature. But on the\nbalance, I didn't enjoy this book and the only reason I managed to actually\nfinish it cover to cover is because of the audiobook format (which let me zone\nout at times while doing something else).\n\"Breaking Through: My Life in Science\" by Katalin KarikÃ³ - an autobiography\nby the molecular biologist who contributed significantly to therapeutic uses\nof mRNA, including its use for the COVID-19 vaccine. Highly recommended.\n\nRe-reads:\n\n\"Thinking Fast and Slow\" by Daniel Kahneman - still a great book, though I\ndid not enjoy the re-read as much as I'd thought I would.\n\"The Man Who Changed Everything\" by Basil Mahon\n\"Of mice and men\" by John Steinbeck"
    },
    {
      "title": "fabiensanglard.net: How Michael Abrash doubled Quake framerate",
      "url": "https://fabiensanglard.net/quake_asm_optimizations/index.html",
      "source": "fabiensanglard.net",
      "time": "2026-02-20T10:18:12.013820",
      "raw_desc": ""
    },
    {
      "title": "fabiensanglard.net: Let's compile Quake like it's 1997!",
      "url": "https://fabiensanglard.net/compile_like_1997/index.html",
      "source": "fabiensanglard.net",
      "time": "2026-02-20T10:18:12.013820",
      "raw_desc": ""
    },
    {
      "title": "fabiensanglard.net: Is QSpy still cool? Let's play QuakeWorld!",
      "url": "https://fabiensanglard.net/quakeworld/index.html",
      "source": "fabiensanglard.net",
      "time": "2026-02-20T10:18:12.013820",
      "raw_desc": ""
    },
    {
      "title": "gwern.net: May 2021 Gwern.net Newsletter",
      "url": "https://gwern.substack.com/p/may-2021-gwernnet-newsletter",
      "source": "gwern.net",
      "time": "2026-02-20T10:18:12.052573",
      "raw_desc": ""
    },
    {
      "title": "gwern.net: April 2021 newsletter",
      "url": "https://gwern.substack.com/p/april-2021-newsletter",
      "source": "gwern.net",
      "time": "2026-02-20T10:18:12.052573",
      "raw_desc": ""
    },
    {
      "title": "gwern.net: March 2021 Gwern.net Newsletter",
      "url": "https://gwern.substack.com/p/march-2021-gwernnet-newsletter",
      "source": "gwern.net",
      "time": "2026-02-20T10:18:12.052573",
      "raw_desc": ""
    },
    {
      "title": "downtowndougbrown.com: Finding a broken trace on my old Mac with the help of its ROM diagnostics",
      "url": "https://www.downtowndougbrown.com/2025/12/finding-a-broken-trace-on-my-old-mac-with-the-help-of-its-rom-diagnostics/",
      "source": "downtowndougbrown.com",
      "time": "2026-02-20T10:18:12.185273",
      "raw_desc": ""
    },
    {
      "title": "downtowndougbrown.com: Debugging BeagleBoard USB boot with a sniffer: fixing omap_loader on modern PCs",
      "url": "https://www.downtowndougbrown.com/2025/11/debugging-beagleboard-usb-boot-with-a-sniffer-fixing-omap_loader-on-modern-pcs/",
      "source": "downtowndougbrown.com",
      "time": "2026-02-20T10:18:12.185273",
      "raw_desc": ""
    },
    {
      "title": "downtowndougbrown.com: An update about the hidden Performa 550 recovery partition",
      "url": "https://www.downtowndougbrown.com/2025/08/an-update-about-the-hidden-performa-550-recovery-partition/",
      "source": "downtowndougbrown.com",
      "time": "2026-02-20T10:18:12.185273",
      "raw_desc": ""
    },
    {
      "title": "abortretry.fail: A Brief History of Sega Enterprises",
      "url": "https://www.abortretry.fail/p/a-brief-history-of-sega-enterprises",
      "source": "abortretry.fail",
      "time": "2026-02-20T10:18:12.278185",
      "raw_desc": ""
    },
    {
      "title": "abortretry.fail: The Olivetti Company",
      "url": "https://www.abortretry.fail/p/the-olivetti-company",
      "source": "abortretry.fail",
      "time": "2026-02-20T10:18:12.278185",
      "raw_desc": ""
    },
    {
      "title": "abortretry.fail: The Osborne Computer Corporation",
      "url": "https://www.abortretry.fail/p/the-osborne-computer-corporation",
      "source": "abortretry.fail",
      "time": "2026-02-20T10:18:12.278185",
      "raw_desc": ""
    },
    {
      "title": "chadnauseam.com: semaglutide-has-changed-the-world",
      "url": "https://chadnauseam.com/random/semaglutide-has-changed-the-world",
      "source": "chadnauseam.com",
      "time": "2026-02-20T10:18:12.292143",
      "raw_desc": ""
    },
    {
      "title": "chadnauseam.com: chewier-foods-for-children",
      "url": "https://chadnauseam.com/random/chewier-foods-for-children",
      "source": "chadnauseam.com",
      "time": "2026-02-20T10:18:12.292143",
      "raw_desc": ""
    },
    {
      "title": "chadnauseam.com: solving-macro",
      "url": "https://chadnauseam.com/economics/solving-macro",
      "source": "chadnauseam.com",
      "time": "2026-02-20T10:18:12.292143",
      "raw_desc": ""
    },
    {
      "title": "brutecat.com: Leaking the phone number of any Google user",
      "url": "https://brutecat.com/articles/leaking-google-phones",
      "source": "brutecat.com",
      "time": "2026-02-20T10:18:12.332565",
      "raw_desc": ""
    },
    {
      "title": "brutecat.com: Disclosing YouTube Creator Emails for a $20k Bounty",
      "url": "https://brutecat.com/articles/youtube-creator-emails",
      "source": "brutecat.com",
      "time": "2026-02-20T10:18:12.332565",
      "raw_desc": ""
    },
    {
      "title": "brutecat.com: Leaking the email of any YouTube user for $10,000",
      "url": "https://brutecat.com/articles/leaking-youtube-emails",
      "source": "brutecat.com",
      "time": "2026-02-20T10:18:12.332565",
      "raw_desc": ""
    },
    {
      "title": "filfre.net: 1998 Ebook!",
      "url": "https://www.filfre.net/2026/02/1998-ebook/",
      "source": "filfre.net",
      "time": "2026-02-20T10:18:12.384753",
      "raw_desc": ""
    },
    {
      "title": "filfre.net: This Week on The Analog Antiquarian",
      "url": "https://www.filfre.net/2026/02/this-week-on-the-analog-antiquarian/",
      "source": "filfre.net",
      "time": "2026-02-20T10:18:12.384753",
      "raw_desc": ""
    },
    {
      "title": "filfre.net: Ultima IX",
      "url": "https://www.filfre.net/2026/02/ultima-ix/",
      "source": "filfre.net",
      "time": "2026-02-20T10:18:12.384753",
      "raw_desc": ""
    },
    {
      "title": "it-notes.dragas.net: Time Machine inside a FreeBSD jail",
      "url": "https://it-notes.dragas.net/2026/01/28/time-machine-freebsd-jail/",
      "source": "it-notes.dragas.net",
      "time": "2026-02-20T10:18:12.472248",
      "raw_desc": ""
    },
    {
      "title": "it-notes.dragas.net: Installing Void Linux on ZFS with Hibernation Support",
      "url": "https://it-notes.dragas.net/2025/12/22/void-linux-zfs-hibernation-guide/",
      "source": "it-notes.dragas.net",
      "time": "2026-02-20T10:18:12.472248",
      "raw_desc": ""
    },
    {
      "title": "it-notes.dragas.net: Why I (still) love Linux",
      "url": "https://it-notes.dragas.net/2025/11/24/why-i-still-love-linux/",
      "source": "it-notes.dragas.net",
      "time": "2026-02-20T10:18:12.472248",
      "raw_desc": ""
    },
    {
      "title": "beej.us: Rust RPN Calculator",
      "url": "http://beej.us/blog/data/rust-rpn-calc/",
      "source": "beej.us",
      "time": "2026-02-20T10:18:12.626176",
      "raw_desc": ""
    },
    {
      "title": "beej.us: Exploring Rust Traits",
      "url": "http://beej.us/blog/data/rust-trait-impl/",
      "source": "beej.us",
      "time": "2026-02-20T10:18:12.626176",
      "raw_desc": ""
    },
    {
      "title": "beej.us: Using Virtual Environments in Python",
      "url": "http://beej.us/blog/data/python-venv/",
      "source": "beej.us",
      "time": "2026-02-20T10:18:12.626176",
      "raw_desc": ""
    },
    {
      "title": "hey.paris: About Paris",
      "url": "https://hey.paris/about/",
      "source": "hey.paris",
      "time": "2026-02-20T10:18:12.667808",
      "raw_desc": ""
    },
    {
      "title": "hey.paris: Bolted Down",
      "url": "https://hey.paris/fiction/bolted-down/",
      "source": "hey.paris",
      "time": "2026-02-20T10:18:12.667808",
      "raw_desc": ""
    },
    {
      "title": "hey.paris: Signal to Noise",
      "url": "https://hey.paris/fiction/signal-to-noise/",
      "source": "hey.paris",
      "time": "2026-02-20T10:18:12.667808",
      "raw_desc": ""
    },
    {
      "title": "refactoringenglish.com: The Most Popular Blogs of Hacker News in 2025",
      "url": "https://refactoringenglish.com/blog/2025-hn-top-5/",
      "source": "refactoringenglish.com",
      "time": "2026-02-20T10:18:12.817700",
      "raw_desc": ""
    },
    {
      "title": "refactoringenglish.com: What Makes the Intro to *Crafting Interpreters* so Good?",
      "url": "https://refactoringenglish.com/blog/crafting-interpreters-intro/",
      "source": "refactoringenglish.com",
      "time": "2026-02-20T10:18:12.817700",
      "raw_desc": ""
    },
    {
      "title": "refactoringenglish.com: How to Get Meaningful Feedback on Your Design Document",
      "url": "https://refactoringenglish.com/chapters/useful-feedback-on-design-docs/",
      "source": "refactoringenglish.com",
      "time": "2026-02-20T10:18:12.817700",
      "raw_desc": ""
    },
    {
      "title": "simone.org: The Postcard and the Thing Itself (On Falling in Love with Ideas)",
      "url": "https://simone.org/the-postcard-and-the-thing-itself-on-falling-in-love-with-ideas/",
      "source": "simone.org",
      "time": "2026-02-20T10:18:12.825310",
      "raw_desc": ""
    },
    {
      "title": "simone.org: The Mirror With No Reflection",
      "url": "https://simone.org/mirror/",
      "source": "simone.org",
      "time": "2026-02-20T10:18:12.825310",
      "raw_desc": ""
    },
    {
      "title": "simone.org: Consumerism: The First Universal Religion Humans Actually Practice",
      "url": "https://simone.org/consumerism/",
      "source": "simone.org",
      "time": "2026-02-20T10:18:12.825310",
      "raw_desc": ""
    },
    {
      "title": "danielwirtz.com: How to create a tool library in Airtable",
      "url": "https://danielwirtz.com/blog/airtable-tool-library",
      "source": "danielwirtz.com",
      "time": "2026-02-20T10:18:12.871083",
      "raw_desc": ""
    },
    {
      "title": "danielwirtz.com: Using Roam Highlighter with Logseq",
      "url": "https://danielwirtz.com/blog/logseq-web-highlighter",
      "source": "danielwirtz.com",
      "time": "2026-02-20T10:18:12.871083",
      "raw_desc": ""
    },
    {
      "title": "danielwirtz.com: Tracking LinkedIn profile analytics with Airtable",
      "url": "https://danielwirtz.com/blog/linkedin-profile-analytics",
      "source": "danielwirtz.com",
      "time": "2026-02-20T10:18:12.871083",
      "raw_desc": ""
    },
    {
      "title": "hugotunius.se: Claude, Teach Me Something",
      "url": "https://hugotunius.se/2025/10/26/claude-teach-me-something.html",
      "source": "hugotunius.se",
      "time": "2026-02-20T10:18:12.883792",
      "raw_desc": "IÃ¢Â€Â™ve been experimenting with a new Claude workflow as an alternative to doom scrolling. It leverages what LLMs do best: non-determinism and text. I call it Ã¢Â€ÂœTeach me somethingÃ¢Â€Â.\nThe idea is: if IÃ¢Â€Â™m bored, instead of going on Reddit, I can ask Claude to teach me something. This might not be the most efficient learning method, but it beats scrolling Reddit. In Claude IÃ¢Â€Â™ve set this up as a project with custom instructions. The prompt IÃ¢Â€Â™m currently using is:\n\nProject Instructions: Socratic Teaching Sessions\nIn this project you will teach me something new using the Socratic method - asking questions to gauge my knowledge and guide my discovery rather than simply explaining concepts.\nAreas (in order of my decreasing expertise):\n\nProgramming\nComputer science\nUX/UI/UXR\nCybersecurity\nMachine learning\nCooking\nPhysics\nEconomics (behavioral or otherwise)\nPsychology\nEngineering\nMusic theory\n\nYour approach:\nWhen I say Ã¢Â€ÂœTeach me something,Ã¢Â€Â you will perform the following steps. If I say Ã¢Â€ÂœTeach me something about <topic>Ã¢Â€Â you skip the first 2 steps.\n\nConsult previous chats in this project to avoid repetition\nChoose a diverse topic from one of my areas\nUse questions to assess what I already know\nGuide me toward insights through dialogue rather than direct explanation\nLet my responses shape the direction and depth of the lesson\n\nGoal: Help me discover and understand concepts through guided inquiry, building on what I know and filling gaps through my own reasoning.\nKeep the topics diverse across sessions.\nAt the end of a session direct me towards primary sources to confirm and read more. Prefer websites, papers, podcast, and books in that order.\n\nThis works nicely. The topic diversity has been good and the Socratic method works, especially because Claude gauges and responds to my prior knowledge. So far Claude has taught me about The Allais Paradox, the physics of consonance, and the chemistry of salt in cooking, to name a few. Claude can list previous chats within a project to keep track of topics. The only point of friction, is ensuring chats are named correctly as Claude will often just name them Ã¢Â€ÂœLearn something newÃ¢Â€Â based on the first user interaction. Claude lacks a tool call to rename chats, so instead IÃ¢Â€Â™ve been asking it to suggest a name at the end and then I rename the chat myself. The last instruction in the prompt ensures I can verify what Claude has said and dig deeper.\nInitially I didnÃ¢Â€Â™t instruct Claude to use the Socratic method, but that works much better. ItÃ¢Â€Â™s significantly less Ã¢Â€Âœinformation-dumpyÃ¢Â€Â. When I know a topic well, Claude successfully shortcuts the basics.\nThis effectively combines two strengths of LLMs: non-determinism and text. The topics are kept diverse and I rely on ClaudeÃ¢Â€Â™s vast knowledge of topics to find interesting points of discussion. Claude, and all LLMs, are great at conversation and this extends to the back and forth of the Socratic method. At the end, the provided sources protect against hallucination and offer a next step beyond the LLM."
    },
    {
      "title": "hugotunius.se: What Every Argument About Sideloading Gets Wrong",
      "url": "https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html",
      "source": "hugotunius.se",
      "time": "2026-02-20T10:18:12.885736",
      "raw_desc": "Sideloading has been a hot topic for the last decade. Most recently, Google has announced further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: Ã¢Â€ÂœI should be able to run whatever code I want on hardware I ownÃ¢Â€Â. I agree entirely with this point, but within the context of this discussion itÃ¢Â€Â™s moot.\n\nÃ¢Â€ÂœI should be able to run whatever code I want on hardware I ownÃ¢Â€Â\n\nWhen Google restricts your ability to install certain applications they arenÃ¢Â€Â™t constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. ItÃ¢Â€Â™s through this control of the operating system that Google is exerting control, not at the hardware layer. You often donÃ¢Â€Â™t have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. AppleÃ¢Â€Â™s success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.\nYou shouldnÃ¢Â€Â™t take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldnÃ¢Â€Â™t be of the restrictions in place in the operating systems they provide Ã¢Â€Â“ rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer SonyÃ¢Â€Â™s restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible."
    },
    {
      "title": "hugotunius.se: On Async Rust",
      "url": "https://hugotunius.se/2024/03/08/on-async-rust.html",
      "source": "hugotunius.se",
      "time": "2026-02-20T10:18:12.889322",
      "raw_desc": "I started using Rust in 2017, before the stabilisation of async/await. When it was stabilised I managed to avoid it for a few more years before it was time to grapple with it. ItÃ¢Â€Â™s fair to say that async Rust is one of the hairiest parts of the language, not because the async model is poorly designed, but because of the inherent complexity of it in combination with RustÃ¢Â€Â™s goals. There have been many blog post written about async and its perceived shortcomings, as well as excellent explainers and history lessons, mostly from withoutboats.\nIn this post I want to reflect on my experience and journey with async and my thoughts on some of the criticisms levied against async. Starting with: do we really need N:M threading anyway?\nDo we Really Need N:M threading?\nA favourite maxim of mine is: Ã¢Â€ÂœComputers are fast actuallyÃ¢Â€Â. My point being that, as an industry, we have lost touch of quite how much modern computers are capable of. Thus, IÃ¢Â€Â™m naturally favourable to the idea that N:M threading is oftentimes overkill and most applications would be well-served by just using OS threads and blocking syscalls. After all the C10k(and more) problem is trivially solvable with just OS threads. Many applications could avoid the complexity of async Rust and still be plenty performant with regular threads.\nHowever, it doesnÃ¢Â€Â™t really matter what I think, or even if itÃ¢Â€Â™s true that most applications donÃ¢Â€Â™t need N:M threading, because developers, for better or worse, want N:M threading . Therefore, for Rust to be competitive with Go, C++, et al. it must offer it. Rust has a very unique set of constraints that makes solving this problem challenging, one of which is zero-cost abstractions.\nZero-Cost Abstractions\nRustÃ¢Â€Â™s goal of providing zero-cost abstractions, i.e. abstractions that are no worse than writing the optimal lower level code yourself, often comes up in discussions around async Rust and is sometimes misunderstood. For example, the idea that async Rust is a big ecosystem with many crates and building all of those crates as part of your application is a violation of the zero-cost abstractions principle. It isnÃ¢Â€Â™t, zero-cost is about runtime performance.\nThe zero-cost goal helps guide us when discussing alternative async models. For example, Go is lauded for its lack of function-colouring and its sometimes suggested Rust should copy its approach. This is a no-go(Ã°ÂŸÂ˜Â…) because GoÃ¢Â€Â™s approach is decidedly not zero-cost and requires a heavy runtime. Rust did actually feature green threads, which are similar to coroutines, in an earlier version of the language, but these were removed precisely because of the runtime requirement.\nThe Arc<Mutex> in the room\nAnother common point of contention is the tendency for async Rust to require a lot, and I do mean a lot, of types like Arc and Mutex, often in combination. I experienced this myself when starting out with async Rust, itÃ¢Â€Â™s easy to solve local state synchronisation problems with these constructs without properly thinking about the wider design of your application. The result is a mess that soon comes back to bite you. However, discussing this in the context of async Rust and as an Ã¢Â€Âœasync problemÃ¢Â€Â is unfair, itÃ¢Â€Â™s really a concurrency problem and it will manifest in applications that achieve concurrency with OS threads too. Fundamentally, if you want to have shared state, whether between tasks or threads, you have to contend with the synchronisation  problem. One of my big lessons in learning async Rust is to not blindly follow compilers errors to Ã¢Â€ÂœsolveÃ¢Â€Â shared state, instead take a step back and properly considered if the state should be shared at all.\nThis problem is similar to the notorious borrow checker problems Rust is infamous for. When I started learning Rust I often ran into borrow checker problems because I wasnÃ¢Â€Â™t thinking thoroughly about ownership, only about my desire to borrow data. Arc<Mutex> and friends sometimes betray a similar lack of consideration for ownership.\nCritiquing Async Rust\nAll of the above form the context to be considered when critiquing async rust. Simply stating that Rust should abandon zero-cost abstractions is easy, while providing constructive feedback that takes this goal into consideration is not. The same is true about the suggestion that Rust should not have an async programming model at all. Within these bounds, constructive criticism of RustÃ¢Â€Â™s async model is great, only by examining whatÃ¢Â€Â™s not working well can lessons be learned for the future and the language improved. All this said, there are definitely problems with async Rust.\nWhen you go looking for crates to perform anything remotely related to IO e.g. making HTTP requests, interfacing with databases, implementing web servers, youÃ¢Â€Â™ll find that there is an abundance of async crates, but rarely any that are sync. Even when sync crates exist they are often implemented in terms of the async version, meaning youÃ¢Â€Â™ll have to pull in a large number of transitive dependencies from the async ecosystem into your ostensibly sync program. This is an extension of the function colouring problem, itÃ¢Â€Â™s crate colouring. The choice of IO model pollutes both a crateÃ¢Â€Â™s API and itÃ¢Â€Â™s dependency hierarchy. In the rare instances when only a sync crate exists the opposite problem occurs for sync programs, yes thereÃ¢Â€Â™s block_on and friends, but this is band-aid at best.\nEven within the async ecosystem thereÃ¢Â€Â™s a problem, the dominance of Tokio. Tokio is a great piece of software and has become the de facto default executor. However, Ã¢Â€ÂœdefaultÃ¢Â€Â implies the possibility of choosing a different executor, which in reality is not possible. The third party crate ecosystem isnÃ¢Â€Â™t just dominated by async crates, but by crates that only work with Tokio. Use a different executor? Tough luck. YouÃ¢Â€Â™ll need to switch to Tokio or redundantly implement the crates you need for yourself. Not only do we have a crate colouring problem, but there are also more than 3 colours because async-tokio and async-async-std are distinct colours.\nAsync traits are slowly being stabilised, but this is just one place where the language and standard library lacks proper support for async. Drop still cannot be async and neither can closures. Async is a second-class citizen within Rust because the tools that are usually available to us, are off limits in async. There is interesting work happening to address this, namely extensions to RustÃ¢Â€Â™s effect system.\nInverting Expectations\n\nThe problems of function and crate colouring are intimately tied to how code is structured. When IO is internal to a piece of code, abstracting over its asyncness, or lack thereof, becomes complicated due to colouring. The colouring is infectious, if some code abstracts over the colours red and green, then that code needs to become a chameleon, changing its colour based on the internal colour of the IO. At the moment this chameleon behaviour is not achievable in Rust, although the effects extensions would allow it. Abstracting over the asyncness of IO is complicated, what if we instead were to avoid it with inversion of control.\nThe sans-IO pattern sidesteps the colouring problem by moving the IO out. Instead of abstracting over IO we implement the core logic and expect the caller to handle IO. Concretely this means that a set of crates implementing a HTTP client would be split into a http-client-proto crate and several user facing crates http-client-sync, http-client-tokio, http-client-async-std. Borrowing from withoutboatÃ¢Â€Â™s colour definitions, http-client-proto would be a blue crate, it does no IO and never blocks the calling thread, it implements the protocol level HTTP concerns such as request parsing, response generation etc. http-client-sync would be a green crate and http-client-tokio would be a red crate. As I hinted to before, a different async executor, at least in the absence of the aforementioned abstractions, is a different colour too so http-client-async-std would be an orange crate. This pattern has several benefits, it enables code sharing between differing IO models without bloating dependency trees or relying on the likes of block_on. A user that finds the crates foo-proto and foo-tokio can leverage foo-proto to contribute foo-sync, requiring less duplication. If every crate that deals with IO followed this pattern the problem of crate colouring would be greatly alleviated and significant portions of code could be shared between sync and async implementations."
    },
    {
      "title": "philiplaine.com: Getting Forked by Microsoft",
      "url": "https://philiplaine.com/posts/getting-forked-by-microsoft/",
      "source": "philiplaine.com",
      "time": "2026-02-20T10:18:13.025426",
      "raw_desc": ""
    },
    {
      "title": "philiplaine.com: Kubernetes Generated Secret",
      "url": "https://philiplaine.com/portfolio/kubernetes-generated-secret/",
      "source": "philiplaine.com",
      "time": "2026-02-20T10:18:13.025426",
      "raw_desc": ""
    },
    {
      "title": "philiplaine.com: Cross Compiling Docker Images",
      "url": "https://philiplaine.com/posts/cross-compiling-docker-images/",
      "source": "philiplaine.com",
      "time": "2026-02-20T10:18:13.025426",
      "raw_desc": ""
    },
    {
      "title": "berthub.eu: De digitale coalitieplannen: gaat het ook echt gebeuren?",
      "url": "https://berthub.eu/articles/posts/digitale-coalitieplannen/",
      "source": "berthub.eu",
      "time": "2026-02-20T10:18:13.204031",
      "raw_desc": ""
    },
    {
      "title": "berthub.eu: Betere Kamerstukken, en hoe lastig innovatie is",
      "url": "https://berthub.eu/articles/posts/betere-kamerdocumenten-en-innovatie/",
      "source": "berthub.eu",
      "time": "2026-02-20T10:18:13.204031",
      "raw_desc": ""
    },
    {
      "title": "berthub.eu: Updates 7 februari: Tweede Kamer, TV Bureau Buitenland",
      "url": "https://berthub.eu/articles/posts/update-7-februari/",
      "source": "berthub.eu",
      "time": "2026-02-20T10:18:13.204031",
      "raw_desc": ""
    },
    {
      "title": "troyhunt.com: Weekly Update 491",
      "url": "https://www.troyhunt.com/weekly-update-491/",
      "source": "troyhunt.com",
      "time": "2026-02-20T10:18:13.318418",
      "raw_desc": ""
    },
    {
      "title": "troyhunt.com: Weekly Update 490",
      "url": "https://www.troyhunt.com/weekly-update-490/",
      "source": "troyhunt.com",
      "time": "2026-02-20T10:18:13.318418",
      "raw_desc": ""
    },
    {
      "title": "troyhunt.com: Weekly Update 489",
      "url": "https://www.troyhunt.com/weekly-update-489/",
      "source": "troyhunt.com",
      "time": "2026-02-20T10:18:13.318418",
      "raw_desc": ""
    },
    {
      "title": "herman.bearblog.dev: Things that work (for me)",
      "url": "https://herman.bearblog.dev/things-that-work/",
      "source": "herman.bearblog.dev",
      "time": "2026-02-20T10:18:13.416320",
      "raw_desc": "If it ain't broke, don't fix it.\n\nWhile I don't fully subscribe to the above quote, since I think it's important to continually improve things that aren't explicitly broken, every now and then something I use works so well that I consider it a solved problem.\nIn this post I'll be listing items and tools I use that work so well that I'm likely to be a customer for life, or will never have to purchase another. I've split the list into physical and digital tools and will try to keep this list as up-to-date as possible. This is both for my reference, as well as for others. If something is not listed it means I'm not 100% satisfied with what I'm currently using, even if it's decent.\nI'm not a minimalist, but I do have a fairly minimalistic approach to the items I buy. I like having one thing that works well (for example, an everything pair of pants), over a selection to choose from each morning.\nSome of these items are inexpensive and readily available; while some of them are pricy (but in my opinion worth it). Unfortunately sometimes it's hard to circumvent Sam Vimes boots theory of socioeconomic unfairness.\nDigital\nTuta mail â€” This email provider does one thing very well: Email. Yes, there is a calendar, but I don't use it. I use it for the responsive and privacy respecting email service, as well as the essentially unlimited email addresses I can set up on custom domains.\nApple Notes â€” I've tried the other writing tools, and Apple Notes wins (for me) by being simple, and automatically synced. I use this for writing posts, taking notes, and handling my todo list for the day.\nVisual Studio Code â€” I've tried to become a vim or emacs purist, but couldn't commit. I've tried going back to Sublime, but didn't feel like relearning the shortcuts. I've tried all of the new AI-powered IDEs, but found it stripped the joy of coding. VSC works fine and I'll likely use it until humans aren't allowed to code anymore.\nTrello â€” This is where I track all my feature requests, ideas, todos, tasks in progress, and tasks put on hold across my various projects. I'm used to the interface and have never had a problem with it. I'm not a power user, nor do I work as part of a team, so it's just right for my use-case.\nBear Blog â€” This goes without saying. I originally built it for me, so it fits my use-case well. I'm just glad it fits so many other people's use-cases too.\n\nPhysical\nApple Airpods Pro â€” This is the best product Apple makes. I could switch away from the rest of the Apple ecosystem if necessary, but I'd have to keep my Airpods. The noise cancelling and audio fidelity is unlike any other in-ear headphones I've used, and while they'll probably need to be replaced every 5 years, they're well worth the sleep on long-haul flights alone.\nNew Balance 574 shoes â€” New Balance created the perfect shoe in the 80s and then never updated them. These shoes are great since they were originally developed as trail running shoes, but have become their own style while being rugged enough to tackle a light trail, or walk around a city all day. They also have a wide toe box to house my flappers.\nCeraVe Moisturising Lotion â€” I didn't realise how healthy my skin could be until Emma forced this on me. My skin has been doing great since switching and I'll likely keep using it until CeraVe discontinues the line.\nEucerin sensitive protect sunscreen â€” Similarly, all sunscreens I've tried have left my face oily and shiny. This is the first facial sunscreen that I can realistically wear every day without any issues. It's SPF 50+, which is great for someone who loves being outdoors in sunny South Africa.\nSalt of the Earth Crystal deodorant â€” This may sound particularly woo-woo, but I've been using this salt deodorant for the past 8 years and since it doesn't contain any perfume, I smell perfectly neutral all of the time.\nHouse of Ord felted wool hat â€” I love this hat. It keeps me cool in the sun, but warm when it's cold out. This is due to wool's thermoregulatory properties that evolved to keep the sheep cool in summer and warm in winter. While it's not the most robust hat, I suspect it'll last a few years if I treat it well.\n\n\nUnder considerationThese are the products I'm using that may make the cut but I haven't used them long enough to be sure.\n\nLululemon ABC pants â€” These are incredibly comfortable stretch pants that pretend (very convincingly) to be a semi-casual set of chinos. The only hesitation I have with them is that they pick up marks and stains incredibly easily.\nMerino wool t-shirts â€” I bought my first merino wool t-shirt recently after rocking cotton for my entire life, and I'm very impressed. These shirts don't get smelly (there are instances of people wearing them for a year straight without issue) and are very soft and comfortable. I'm a bit worried about durability, but if they make packing lighter and are versatile I may slowly start to replace my cotton shirts once they wear out.\n\nI like to be very intentional with my purchases. We live in an 84m^2 apartment and so everything has to have its place to avoid clutter. I understand how possessions can end up owning you, and so I try to keep them as reasonable as possible. A good general rule of thumb is that new things replace worn-out and old things, not add to them. This applies both digitally and physically, since there's only so much mental capacity for digital tools as there is for physical items.\n\nMake things as simple as possible but no simpler.\nâ€” Albert Einstein\n\nThis list was last updated 4Â weeks, 2Â days ago."
    },
    {
      "title": "herman.bearblog.dev: Discovery and AI",
      "url": "https://herman.bearblog.dev/discovery-and-ai/",
      "source": "herman.bearblog.dev",
      "time": "2026-02-20T10:18:13.417353",
      "raw_desc": "I browse the discovery feed on Bear daily, both as part of my role as a moderator, and because it's a space I love, populated by a diverse group of interesting people.\nI've read the posts regarding AI-related content on the discovery feed, and I get it. It's such a prevalent topic right now that it feels inescapable, available everywhere from Christmas dinner to overheard conversation on the subway. It's also becoming quite a polarising one, since it has broad impacts on society and the natural environment.\nThis conversation also raises the question about popular bloggers and how pre-existing audiences should affect discoverability. As with all creative media, once you have a big-enough audience it becomes self-perpetuating that you get more visibility. Think Spotify's 1%. Conveniently, Bear is small enough that bloggers with no audience can still be discovered easily and it's something I'd like to preserve on the platform.\nIn this post I'll try and explain my thinking on these matters, and clear up a few misconceptions.\nFirst off, posts that get many upvotes through a large pre-existing audience, or from doing well on Hacker News do not spend disproportionately more time on the discovery feed. Due to how the algorithm works, after a certain number of upvotes, more upvotes have little to no effect. Even a post with 10,000 upvotes won't spend more than a week on page #1. I want Trending to be equally accessible to all bloggers on Bear.\nWhile this cap solves the problem of sticky posts, there is a second, less pressing issue: If a blogger has a pre-existing audience, say in the form of a newsletter or Twitter account, some of their existing audience will likely upvote, and that post has a good chance of feature on the Trending page.\nOne of the potential solutions I've considered is either making upvotes available to logged in users only, or Bear account holders receive extra weighting in their upvotes. However, due to how domains work each blog is a new website according to the browser, and so logins don't persist between blogs. This would require logging in to upvote on each site, which isn't feasible.\nWhile I moderate Bear for spam, AI-generated content, and people breaking the Code of Conduct, I don't moderate by topic. That removes the egalitarian nature of the platform and puts up topic rails like an interest-group forum or subreddit. While I'm not particularly interested in AI as a topic, I don't feel like it's my place to remove it, in the same way that I don't feel particularly strongly about manga.\nThere is a hide blog feature on the discovery page. If you don't want certain blogs showing up in your feed, add them to the hidden textarea to never see them again. Similarly to how Bear gives bloggers the ability to create their own tools within the dashboard, I would like to lean into this kind of extensibility for the discovery feed, with hiding blogs being the start. Curation instead of exclusion.\nThis post is just a stream of consciousness of my thoughts on the matter. I have been contemplating this, and, as with most things, it's a nuanced problem to solve. If you have any thoughts or potential solutions, send me an email. I appreciate your input.\nEnjoy the last 2 days of 2025!"
    },
    {
      "title": "herman.bearblog.dev: Grow slowly, stay small",
      "url": "https://herman.bearblog.dev/grow-slowly-stay-small/",
      "source": "herman.bearblog.dev",
      "time": "2026-02-20T10:18:13.417863",
      "raw_desc": "Quick announcement: I'll be visiting Japan in April, 2026 for about a month and will be on Honshu for most of the trip. Please email me recommendations. If you live nearby, let's have coffee?\n\nI've always been fascinated by old, multi-generational Japanese businesses. My leisure-watching on YouTube is usually a long video of a Japanese craftsmanâ€”sometimes a 10th or 11th generationâ€”making iron tea kettles, or soy sauce, or pottery, or furniture.\nTheir dedication to craftâ€”and acknowledgment that perfection is unattainableâ€”resonates with me deeply. Improving in their craft is an almost spiritual endeavour, and it inspires me to engage in my crafts with a similar passion and focus.\nSlow, consistent investment over many years is how beautiful things are made, learnt, or grown. As a society we forget this truthâ€”especially with the rise of social media and the proliferation of instant gratification. Good things take time.\nDedication to craft in this manner comes with incredible longevity (survivorship bias plays a role, but the density of long-lived businesses in Japan is an outlier). So many of these small businesses have been around for hundreds, and sometimes over a thousand years, passed from generation to generation. Modern companies have a hard time retaining employees for 2 years, let alone a lifetime.\nThis longevity stems from a counter-intuitive idea of growing slowly (or not at all) and choosing to stay small. In most modern economies if you were to start a bakery, the goal would be to set it up, hire and train a bunch of staff, and expand operations to a second location. Potentially, if you play your cards right, you could create a national (or international) chain or franchise. Corporatise the shit out of it, go public or sell, make bank.\nWhile this is a potential path to becoming filthy rich, the odds of achieving this become vanishingly small. The organisation becomes brittle due to thinly-spread resources and care, hiring becomes risky, and leverage, whether in the form of loans or investors, imposes unwanted directionality.\nThere's a well known parable of the fisherman and the businessman that goes something like this:\nA businessman meets a fisherman who is selling fish at his stall one morning. The businessman enquires of the fisherman what he does after he finishes selling his fish for the day. The fisherman responds that he spends time with his friends and family, cooks good food, and watches the sunset with his wife. Then in the morning he wakes up early, takes his boat out on the ocean, and catches some fish.\nThe businessman, shocked that the fisherman was wasting so much time encourages him fish for longer in the morning, increasing his yield and maximising the utility of his boat. Then he should sell those extra fish in the afternoon and save up until he has enough money to buy a second fishing boat and potentially employ some other fishermen. Focus on the selling side of the business, set up a permanent store, and possibly, if he does everything correctly, get a loan to expand the operation even further.\nIn 10 to 20 years he could own an entire fishing fleet, make a lot of money, and finally retire. The fisherman then asks the businessman what he would do with his days once retired, to which the businessman responds: \"Well, you could spend more time with your friends and family, cook good food, watch the sunset with your wife, and wake up early in the morning and go fishing, if you want.\"\nI love this parable, even if it is a bit of an oversimplification. There is something to be said about affording comforts and financial stability that a fisherman may not have access to. But I think it illustrates the point that when it comes to running a business, bigger is not always better. This is especially true for consultancies or agencies which suffer from bad horizontal scaling economics.\nThe trick is figuring out what is \"enough\". At what point are we chasing status instead of contentment?\nA smaller, slower growing company is less risky, less fragile, less stressful, and still a rewarding endeavour.\nThis is how I run Bear. The project covers its own expenses and compensates me enough to have a decent quality of life. It grows slowly and sustainably. It isn't leveraged and I control its direction and fate. The most important factor, however, is that I don't need it to be something grander. It affords me a life that I love, and provides me with a craft to practise."
    },
    {
      "title": "matduggan.com: I Sold Out for $20 a Month and All I Got Was This Perfectly Generated Terraform",
      "url": "https://matduggan.com/i-sold-out-for-200-a-month-and-all-i-got-was-this-perfectly-generated-terraform/",
      "source": "matduggan.com",
      "time": "2026-02-20T10:18:13.552857",
      "raw_desc": ""
    },
    {
      "title": "matduggan.com: The Small Web is Tricky to Find",
      "url": "https://matduggan.com/the-small-web-is-tricky-to-find/",
      "source": "matduggan.com",
      "time": "2026-02-20T10:18:13.552857",
      "raw_desc": ""
    },
    {
      "title": "matduggan.com: GitButler CLI Is Really Good",
      "url": "https://matduggan.com/gitbutler-cli-is-really-good/",
      "source": "matduggan.com",
      "time": "2026-02-20T10:18:13.552857",
      "raw_desc": ""
    },
    {
      "title": "tomrenner.com: LLMs are a 400-year-long confidence trick",
      "url": "https://tomrenner.com/posts/400-year-confidence-trick/",
      "source": "tomrenner.com",
      "time": "2026-02-20T10:18:13.563098",
      "raw_desc": ""
    },
    {
      "title": "tomrenner.com: Things that made me think: Cycle time, learning theory, and build chain security",
      "url": "https://tomrenner.com/posts/ttmmt-3/",
      "source": "tomrenner.com",
      "time": "2026-02-20T10:18:13.563098",
      "raw_desc": ""
    },
    {
      "title": "tomrenner.com: Does my toaster love me?",
      "url": "https://tomrenner.com/posts/does-my-toaster-love-me/",
      "source": "tomrenner.com",
      "time": "2026-02-20T10:18:13.563098",
      "raw_desc": ""
    },
    {
      "title": "danieldelaney.net: I built a timer I canâ€™t fail to set",
      "url": "http://danieldelaney.net/timer/",
      "source": "danieldelaney.net",
      "time": "2026-02-20T10:18:13.593907",
      "raw_desc": ""
    },
    {
      "title": "danieldelaney.net: Free software scares normal people",
      "url": "http://danieldelaney.net/normal/",
      "source": "danieldelaney.net",
      "time": "2026-02-20T10:18:13.593907",
      "raw_desc": ""
    },
    {
      "title": "danieldelaney.net: Objectivity is superstition",
      "url": "http://danieldelaney.net/objectivity/",
      "source": "danieldelaney.net",
      "time": "2026-02-20T10:18:13.593907",
      "raw_desc": ""
    },
    {
      "title": "danielchasehooper.com: Testing AI For C Programming",
      "url": "https://danielchasehooper.com/posts/code-agents/",
      "source": "danielchasehooper.com",
      "time": "2026-02-20T10:18:13.631484",
      "raw_desc": ""
    },
    {
      "title": "danielchasehooper.com: Hot reloading is better than SwiftUI previews",
      "url": "https://danielchasehooper.com/posts/hot-reloading-swiftui/",
      "source": "danielchasehooper.com",
      "time": "2026-02-20T10:18:13.631484",
      "raw_desc": ""
    },
    {
      "title": "danielchasehooper.com: I Made A Real-Time Build Visualizer",
      "url": "https://danielchasehooper.com/posts/syscall-build-snooping/",
      "source": "danielchasehooper.com",
      "time": "2026-02-20T10:18:13.631484",
      "raw_desc": ""
    },
    {
      "title": "martinalderson.com: Anthropic's 500 vulns are the tip of the iceberg",
      "url": "https://martinalderson.com/posts/anthropic-found-500-zero-days/?utm_source=rss",
      "source": "martinalderson.com",
      "time": "2026-02-20T10:18:13.671271",
      "raw_desc": ""
    },
    {
      "title": "martinalderson.com: Attack of the SaaS clones",
      "url": "https://martinalderson.com/posts/attack-of-the-clones/?utm_source=rss",
      "source": "martinalderson.com",
      "time": "2026-02-20T10:18:13.671271",
      "raw_desc": ""
    },
    {
      "title": "martinalderson.com: How to generate good looking reports with Claude Code, Cowork or Codex",
      "url": "https://martinalderson.com/posts/how-to-make-great-looking-consistent-reports-with-claude-code-cowork-codex/?utm_source=rss",
      "source": "martinalderson.com",
      "time": "2026-02-20T10:18:13.671271",
      "raw_desc": ""
    },
    {
      "title": "steveblank.com: You Only Think They Work For You",
      "url": "https://steveblank.com/2026/02/18/you-only-think-they-work-for-you/",
      "source": "steveblank.com",
      "time": "2026-02-20T10:18:13.693487",
      "raw_desc": ""
    },
    {
      "title": "steveblank.com: Revisionist History â€“ Aliens, Secrets and Conspiracies",
      "url": "https://steveblank.com/2026/02/10/revisionist-history-aliens-secrets-and-conspiracies/",
      "source": "steveblank.com",
      "time": "2026-02-20T10:18:13.693487",
      "raw_desc": ""
    },
    {
      "title": "steveblank.com: Making the Wrong Things Go Faster at The Department of War",
      "url": "https://steveblank.com/2026/02/03/making-the-wrong-things-go-faster-at-the-department-of-war/",
      "source": "steveblank.com",
      "time": "2026-02-20T10:18:13.693487",
      "raw_desc": ""
    },
    {
      "title": "dwarkesh.com: Dario Amodei â€” \"We are near the end of the exponential\"",
      "url": "https://www.dwarkesh.com/p/dario-amodei-2",
      "source": "dwarkesh.com",
      "time": "2026-02-20T10:18:13.721938",
      "raw_desc": ""
    },
    {
      "title": "dwarkesh.com: Notes on Space GPUs",
      "url": "https://www.dwarkesh.com/p/notes-on-space-gpus",
      "source": "dwarkesh.com",
      "time": "2026-02-20T10:18:13.721938",
      "raw_desc": ""
    },
    {
      "title": "dwarkesh.com: Elon Musk â€” \"In 36 months, the cheapest place to put AI will be spaceâ€",
      "url": "https://www.dwarkesh.com/p/elon-musk",
      "source": "dwarkesh.com",
      "time": "2026-02-20T10:18:13.721938",
      "raw_desc": ""
    },
    {
      "title": "blog.pixelmelt.dev: How I Reversed Amazon's Kindle Web Obfuscation Because Their App Sucked",
      "url": "https://blog.pixelmelt.dev/kindle-web-drm/",
      "source": "blog.pixelmelt.dev",
      "time": "2026-02-20T10:18:13.970559",
      "raw_desc": ""
    },
    {
      "title": "blog.pixelmelt.dev: Building The Language Model Nobody Asked For",
      "url": "https://blog.pixelmelt.dev/building-the-language-model-nobody-asked-for/",
      "source": "blog.pixelmelt.dev",
      "time": "2026-02-20T10:18:13.971572",
      "raw_desc": ""
    },
    {
      "title": "blog.pixelmelt.dev: Defeating DevTools Detection",
      "url": "https://blog.pixelmelt.dev/defeating-devtools-detection/",
      "source": "blog.pixelmelt.dev",
      "time": "2026-02-20T10:18:13.971572",
      "raw_desc": ""
    },
    {
      "title": "chiark.greenend.org.uk/~sgtatham: Aperiodic Tilings V: the Refinable Frontier",
      "url": "https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/aperiodic-refine/",
      "source": "chiark.greenend.org.uk/~sgtatham",
      "time": "2026-02-20T10:18:14.068820",
      "raw_desc": ""
    },
    {
      "title": "chiark.greenend.org.uk/~sgtatham: Brute-forcing Langleyâ€™s geometry problem with field extensions",
      "url": "https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/adventitious/",
      "source": "chiark.greenend.org.uk/~sgtatham",
      "time": "2026-02-20T10:18:14.068820",
      "raw_desc": ""
    },
    {
      "title": "chiark.greenend.org.uk/~sgtatham: In which I have Opinions about parsing and grammars",
      "url": "https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/parsing/",
      "source": "chiark.greenend.org.uk/~sgtatham",
      "time": "2026-02-20T10:18:14.068820",
      "raw_desc": ""
    },
    {
      "title": "worksonmymachine.substack.com: As Complexity Grows, Architecture Dominates Material",
      "url": "https://worksonmymachine.ai/p/as-complexity-grows-architecture",
      "source": "worksonmymachine.substack.com",
      "time": "2026-02-20T10:18:14.095039",
      "raw_desc": ""
    },
    {
      "title": "worksonmymachine.substack.com: The Discovery Phase Is All There Is",
      "url": "https://worksonmymachine.ai/p/the-discovery-phase-is-all-there",
      "source": "worksonmymachine.substack.com",
      "time": "2026-02-20T10:18:14.095039",
      "raw_desc": ""
    },
    {
      "title": "worksonmymachine.substack.com: The Different Shapes of \"Think Before You Build\" Prompting",
      "url": "https://worksonmymachine.ai/p/the-different-shapes-of-think-before",
      "source": "worksonmymachine.substack.com",
      "time": "2026-02-20T10:18:14.095039",
      "raw_desc": ""
    },
    {
      "title": "experimental-history.com: I swear the UFO is coming any minute",
      "url": "https://www.experimental-history.com/p/i-swear-the-ufo-is-coming-any-minute",
      "source": "experimental-history.com",
      "time": "2026-02-20T10:18:14.147463",
      "raw_desc": ""
    },
    {
      "title": "experimental-history.com: Underrated ways to change the world, vol. II",
      "url": "https://www.experimental-history.com/p/underrated-ways-to-change-the-world-b64",
      "source": "experimental-history.com",
      "time": "2026-02-20T10:18:14.147463",
      "raw_desc": ""
    },
    {
      "title": "experimental-history.com: I know your secret",
      "url": "https://www.experimental-history.com/p/i-know-your-secret",
      "source": "experimental-history.com",
      "time": "2026-02-20T10:18:14.147463",
      "raw_desc": ""
    },
    {
      "title": "aresluna.org: Fav tech museums",
      "url": "https://aresluna.org/fav-tech-museums",
      "source": "aresluna.org",
      "time": "2026-02-20T10:18:14.246460",
      "raw_desc": "A photo essay of 20-something best tech museums IÃ¢Â€Â™ve been toÃ¢Â€Â¦ and three bad ones."
    },
    {
      "title": "aresluna.org: The Clock",
      "url": "https://aresluna.org/the-clock",
      "source": "aresluna.org",
      "time": "2026-02-20T10:18:14.247522",
      "raw_desc": "In the 1980s, the dead space between our television programs was filled withÃ¢Â€Â¦ a clock."
    },
    {
      "title": "aresluna.org: The primitive tortureboard",
      "url": "https://aresluna.org/the-primitive-tortureboard",
      "source": "aresluna.org",
      "time": "2026-02-20T10:18:14.247522",
      "raw_desc": "Untangling the myths and mysteries of Dvorak and QWERTY. (First online appearance of a chapter originally published in printed form in December 2023. 8,000 words. 33 photos.)"
    },
    {
      "title": "keygen.sh: How I replaced Baremetrics and ChartMogul with Rake",
      "url": "https://keygen.sh/blog/how-i-replaced-baremetrics-and-chartmogul-with-rake/",
      "source": "keygen.sh",
      "time": "2026-02-20T10:18:14.597538",
      "raw_desc": ""
    },
    {
      "title": "keygen.sh: How to Implement API Key Authentication in Rails Without Devise",
      "url": "https://keygen.sh/blog/how-to-implement-api-key-authentication-in-rails-without-devise/",
      "source": "keygen.sh",
      "time": "2026-02-20T10:18:14.597538",
      "raw_desc": ""
    },
    {
      "title": "keygen.sh: How to Generate Secure License Keys in 2026",
      "url": "https://keygen.sh/blog/how-to-generate-license-keys/",
      "source": "keygen.sh",
      "time": "2026-02-20T10:18:14.597538",
      "raw_desc": ""
    },
    {
      "title": "bernsteinbear.com: Sorry for marking all the posts as unread",
      "url": "",
      "source": "bernsteinbear.com",
      "time": "2026-02-20T10:18:14.693688",
      "raw_desc": ""
    },
    {
      "title": "bernsteinbear.com: Type-based alias analysis in the Toy Optimizer",
      "url": "https://bernsteinbear.com/blog/toy-tbaa/?utm_source=rss",
      "source": "bernsteinbear.com",
      "time": "2026-02-20T10:18:14.693688",
      "raw_desc": ""
    },
    {
      "title": "bernsteinbear.com: A multi-entry CFG design conundrum",
      "url": "https://bernsteinbear.com/blog/multiple-entry/?utm_source=rss",
      "source": "bernsteinbear.com",
      "time": "2026-02-20T10:18:14.693688",
      "raw_desc": ""
    },
    {
      "title": "miguelgrinberg.com: My Courses Site is Moving to a New Home",
      "url": "https://blog.miguelgrinberg.com/post/my-courses-site-is-moving-to-a-new-home",
      "source": "miguelgrinberg.com",
      "time": "2026-02-20T10:18:14.700087",
      "raw_desc": ""
    },
    {
      "title": "miguelgrinberg.com: Date Arithmetic in Bash",
      "url": "https://blog.miguelgrinberg.com/post/date-arithmetic-in-bash",
      "source": "miguelgrinberg.com",
      "time": "2026-02-20T10:18:14.700087",
      "raw_desc": ""
    },
    {
      "title": "miguelgrinberg.com: How to Add a Quick Interactive Map to your Website",
      "url": "https://blog.miguelgrinberg.com/post/how-to-add-a-quick-interactive-map-to-your-website",
      "source": "miguelgrinberg.com",
      "time": "2026-02-20T10:18:14.700087",
      "raw_desc": ""
    },
    {
      "title": "anildash.com: 500,000 tech workers have been laid off since ChatGPT was released",
      "url": "https://anildash.com/2026/01/06/500k-tech-workers-laid-off/",
      "source": "anildash.com",
      "time": "2026-02-20T10:18:14.922270",
      "raw_desc": "One of the key points I repeated when talking about the state of the tech industry yesterday was the salient fact that half a million tech workers have been laid off since ChatGPT was released in late 2022. Now, to be clear, those workers havenâ€™t been laid off because their jobs are now being done by AI, and theyâ€™ve been replaced by bots. Instead, theyâ€™ve been laid off by execs who now have AI to use as an excuse for going after workers theyâ€™ve wanted to cut all along.\nThis is important to understand for a few reasons. First, itâ€™s key just for having empathy for both the mindset and the working conditions of people in the tech industry. For so many outside of tech, their impression of what â€œtechâ€ means is whatever is the most recent transgression theyâ€™ve heard about from the most obnoxious billionaire whoâ€™s made the news lately. But in many cases, itâ€™s the rank and file workers at that personâ€™s company who were the first victims of that billionaireâ€™s ego.\nSecond, itâ€™s important to understand the big tech companies as almost the testing grounds for the techniques and strategies that these guys want to roll out on the rest of the economy, and on the rest of the world. Before they started going on podcasts pretending to be extremely masculine while whining about their feelings, or overtly bribing politicians to give them government contracts, they beta-tested these manipulative strategies within their companies by cracking down on dissent and letting their most self-indulgent and egomaniacal tendencies run wild. Then, when people (reasonably!) began to object, they used that as an excuse to purge any dissenters for being uncooperative or â€œdifficultâ€.\nIt starts with tech, but doesnâ€™t end there\nThese are tactics theyâ€™ll be bringing to other industries and sectors of the economy, if they havenâ€™t already. Sometimes theyâ€™ll be providing AI technologies and tools as an enabler or justification for the cultural and political agenda that theyâ€™re enacting, but often times, they donâ€™t even need to. In many cases, they can simply make clear that they want to enforce psychological and social conformity within their organizations, and that any disagreement will not be tolerated, and the implicit threat of being replaced by automation (or by other workers who are willing to fall in line) is enough to get people to comply.\nThis is the subtext, and sometimes the explicit text, of the deployment of â€œAIâ€ in a lot of organizations. Thatâ€™s separate from what actual AI software or technology can do. And it explains a lot of why the majority AI view within the tech industry is nothing like the hype cycle thatâ€™s being pushed by the loudest voices of the big-name CEOs.\nBecause people who work in tech still believe in the power of tech to do good things, many of us wonâ€™t just dismiss outright the possibility that any technology â€” even AI tools like LLMsÂ â€” could yield some benefits. But the optimistic takes are tempered by the first-hand knowledge of how the tools are being used as an excuse to sideline or victimize good people.\nThis wave of layoffs and reductions has been described as â€œpursuing efficienciesâ€ or â€œright-sizingâ€. But so many of us in tech can remember a few years back, when working in tech as an upwardly-mobile worker with a successful career felt like the best job in the world. When many people could buy nice presents for their kids at Christmas or they werenâ€™t as worried about your car payments. When huge parts of society were promising young people that there was a great future ahead if they would just learn to code. When the promise of a tech careerâ€™s potential was used as the foundation for building infrastructure in our schools and cities to train a whole new generation of coders.\nBut the funders and tycoons in charge of the big tech companies knew that they did not want to keep paying enormous salaries to the people they were hiring. They certainly knew they didnâ€™t want to keep paying huge hiring bonuses to young people just out of college, or to pay large staffs of recruiters to go find underrepresented candidates. Those niceties that everybody loved, like great healthcare and decent benefits, were identified by the people running the big tech companies as â€œmarket inefficienciesâ€ which indicated some wealth was going to you that should have been going to them. So yes, part of the reason for the huge investment in AI coding tools was to make it easier to write code. But another huge reason that AI got so good at writing code was so that nobody would ever have to pay coders so well again.\nYouâ€™re not wrong if you feel angry, resentful and overwhelmed by all of this; indeed, it would be absurd if you didnâ€™t feel this way, since the wealthiest and most powerful people in the history of the world have been spending a few years trying to make you feel exactly this way. Constant rotating layoffs and a nonstop fear of further cuts, with a perpetual sense of precarity, are a deliberate strategy so that everyone will accept lower salaries and reduced benefits, and be too afraid to push for the exact same salaries that the company could afford to pay the year before.\nWhy are we stirring the pot?\nOkay, so are we just trying to get each other all depressed? No. Itâ€™s just vitally important that we name a problem and identify it if weâ€™re going to solve it.\nâ€¨Most people outside of the technology industry think that â€œtechâ€ is a monolith, that the people who work in tech are the same as the people who own the technology companies. They donâ€™t know that tech workers are in the same boat that they are, being buffeted by the economy, and being subject to the whims of their bosses, or being displaced by AI. They donâ€™t know that the DEI backlash has gutted HR teams at tech companies, too, for example. So itâ€™s key for everyone to understand that theyâ€™re starting from the same place.\nNext, itâ€™s key to tease apart things that are separate concerns. For example: AI is often an excuse for layoffs, not the cause of them. ChatGPT didnâ€™t replace the tasks that recruiters were doing in attracting underrepresented candidates at big tech companies â€” the bosses just donâ€™t care about trying to hire underrepresented candidates anymore! The tech story is being used to mask the political and social goal. And itâ€™s important to understand that, because otherwise people waste their time fighting battles that might not matter, like the deployment of a technology system, and losing the ones that do, like the actual decisions that an organization is making about its future.\nAre they efficient, though?\nBut what if, some people will ask, these companies just had too many people? What if theyâ€™d over-hired? The folks who want to feel really savvy will say, â€œI heard that they had all those employees because interest rates were low. It was a Zero Interest Rate Phenomenon.â€ This is, not to put too fine a point on it, bullshit. Itâ€™s not in any companyâ€™s best interests to cut their staffing down to the bone.\nYou actually need to have some reserve capacity for labor in order to reach maximum output for a large organization. This is the difference between a large-scale organization and a small one. People sitting around doing nothing is the epitome of waste or inefficiency in a small team, but in a large organization, itâ€™s a lot more costly if you are about to start a new process or project and you donâ€™t have labor capacity or expertise to deploy.\nA good analogy is the oft-cited need these days for people to be bored more often. Thereâ€™s a frequent lament that, because people are so distracted by things like social media and constant interruptions, they never have time to get bored and let their mind wander, and think new thoughts or discover their own creativity. Put another way, they never get the chance to tap into their own cognitive surplus.\nThe only advantage a large organization can have over a small one, other than sheer efficiencies of scale, is if it has a cognitive surplus that it can tap into. By destroying that cognitive surplus, and leaving those who remain behind in a state of constant emotional turmoil and duress, these organizations are permanently damaging both their competitive advantages and their potential future innovations.\nAI Spring\nWhen the dust clears, and people realize that extreme greed is never the path to maximum long-term reward, there is going to be a â€œpeace dividendâ€ of sorts from all the good talent thatâ€™s now on the market. Some of this will be smart, thoughtful people flowing to other industries or companies, bringing their experience and insights with them.\nBut I think a lot of this will be people starting their own new companies and organizations, informed by the broken economic models, and broken human models, of the companies theyâ€™ve left. We saw this a generation ago after the bust of the dot-com boom, when it was not only revealed that the economics of a lot of the companies didnâ€™t work, but that so many of the people who had created the companies of that era didnâ€™t even care about the markets or the industries that theyâ€™d entered. When the get-rich-quick folks left the scene, those of us who remained, who truly loved the web as a creative and expressive medium, found a ton of opportunity in being the little mammals amidst the sad dinosaurs trying to find funding for meteor dot com.\nWhat comes next\nI donâ€™t think this all gets better very quickly. If you put aside the puffery of the AI companies scratching each othersâ€™ backs, itâ€™s clear the economy is in a recession, even if this administrationâ€™s goons have shut down reporting on jobs and inflation in a vain attempt to hide that reality. But I do think there may be more resilience because of the sheer talent and entrepreneurial skill of the people who are now on the market as individuals."
    },
    {
      "title": "anildash.com: How Markdown took over the world",
      "url": "https://anildash.com/2026/01/09/how-markdown-took-over-the-world/",
      "source": "anildash.com",
      "time": "2026-02-20T10:18:14.931445",
      "raw_desc": "Nearly every bit of the high-tech world, from the most cutting-edge AI systems at the biggest companies, to the casual scraps of code cobbled together by college students, is annotated and described by the same, simple plain text format. Whether youâ€™re trying to give complex instructions to ChatGPT, or you want to be able to exchange a grocery list in Apple Notes or copy someoneâ€™s homework in Google Docs, that same format will do the trick. The wild part is, the format wasnâ€™t created by a conglomerate of tech tycoons, it was created by a curmudgeonly guy with a kind heart who right this minute is probably rewatching a Kubrick film while cheering for an absolutely indefensible sports team.\nBut itâ€™s worth understanding how these simple little text files were born, not just because I get to brag about how generous and clever my friends are, but also because it reminds us of how the Internet really works: smart people think of good things that are crazy enough that they just might work, and then they give them away, over and over, until they slowly take over the world and make things better for everyone.\nMaking Their Mark\nThough itâ€™s now a building block of the contemporary Internet, like so many great things, Markdown just started out trying to solve a personal problem. In 2002, John Gruber made the unconventional decision to bet his online career on two completely irrational foundations: Apple, and blogs.\nItâ€™s hard to remember now, but in 2002, Apple was just a few years past having been on deathâ€™s door. As difficult as it may be to picture in todayâ€™s world where Apple keynotes are treated like major events, back then, almost nobody was covering Apple regularly, let alone writing exclusively about the company. There was barely even an â€œtech newsâ€ scene online at all, and virtually no one was blogging. So Johnâ€™s decision to go all-in on Apple for his pioneering blog Daring Fireball was, well, a daring one. At the time, Apple had only just launched its first iPod that worked with Windows computers, and the iPhone was still a full five years in the future. But that single-minded focus, not just on Apple, but on obsessive detail in everything he covered, eventually helped inspire much of the technology media landscape that we see today. Johnâ€™s timing was also perfect â€” from the doldrums of that era, Appleâ€™s stock price would rise by about 120,000% in the years after Daring Fireball started, and its cultural relevance probably increased by even more than that.\nBy 2004, it wasnâ€™t just Apple that had begun to take off: blogs and social media themselves had moved from obscurity to the very center of culture, and a new era of web technology had begun. At the beginning of that year, few people in the world even knew what a â€œblogâ€ was, but by the end of 2004, blogs had become not just ubiquitous, but downright cool. As unlikely as it seems now, that yearâ€™s largely uninspiring slate of U.S. presidential candidates like Wesley Clark, Gary Hart and, yes, Howard Dean helped propel blogs into mainstream awareness during the Democratic primaries, alongside online pundits who had begun weighing in on politics and the issues and cultural moments at a pace that newspapers and TV couldnâ€™t keep up with. A lot has been written about the transformation of media during those years, but less has been written about how the media and tech of the time transformed each other.\n\nThat era of early blogging was interesting in that nearly everyone who was writing the first popular sites was also busy helping create the tools for publishing them. Just like Lucille Ball and Desi Arnaz had to pioneer combining studio-style flat lighting with 35mm filming in order to define the look of the modern sitcom, or Jimi Hendrix had to work with Roger Mayer to invent the signature guitar distortion pedals that defined the sound of rock and roll, the pioneers who defined the technical format and structures of blogging were often building the very tools of creation as they went along.\nI got a front row seat to these acts of creation. At the time I was working on Movable Type, which was the most popular tool for publishing â€œseriousâ€ blogs, and helped popularize the medium. Two of my good friends had built the tool and quickly made it into the default choice for anybody who wanted to reach a big audience; it was kind of a combination of everything people do these days on WordPress and all the various email newsletter platforms and all of the â€œseriousâ€ podcasts (since podcasts wouldnâ€™t be invented for another few months). But back in those early days, weâ€™d watch people use our tools to set up Gawker or Huffington Post one day, and Daring Fireball or Waxy.org the next, and each of them would be the first of its kind, both in terms of its design and its voice. To this day, when I see something online that I love by Julianne Escobedo Shepherd or Ta-Nehisi Coates or Nilay Patel or Annalee Newitz or any one of dozens of other brilliant writers or creators, my first thought is often, â€œhey! They used to type in that app that I used to make!â€ Because sometimes those writers would inspire us to make a new feature in the publishing tools, and sometimes they would have hacked up a new feature all by themselves in between typing up their new blog posts.\nA really clear, and very simple, early example of how we learned that lesson was when we changed the size of the box that people used to type in just to create the posts on their sites. We made the box a little bit taller, mostly for aesthetic reasons. Within a few weeks, weâ€™d found that posts on sites like Gawker had gotten longer, mostly because the box was bigger. This seems obvious now, years after we saw tweets get longer when Twitter expanded from 140 characters to 280 characters, but at the time this was a terrifying glimpse at how much power a couple of young product managers in a conference room in California would have over the media consumption of the entire world every time they made a seemingly-insignificant decision.\nThe other dirty little secret was, typing in the box in that old blogging app could beâ€¦ pretty wonky sometimes. People who wanted to do normal things like include an image or link in their blog post, or even just make some text bold, often had to learn somewhat-obscure HTML formatting, memorizing the actual language thatâ€™s used to make web pages. Not everybody knew all the details of how to make pages that way, and if they made even one small mistake, sometimes they could break the whole design of their site. It made things feel very fraught every time a writer went to publish something new online, and got in the way of the increasingly-fast pace of sharing ideas now that social media was taking over the public conversation.\nEnter John and his magical text files.\n\nMarking up and marking down\nThe purpose of Markdown is really simple: It lets you use the regular characters on your keyboard which you already use while typing out things like emails, to make fancy formatting of text for the web. That HTML format thatâ€™s used to make web pages stands for HyperText Markup Language. The word â€œmarkupâ€ there means youâ€™re â€œmarking upâ€ your text with all kinds of special characters.\nOnly, the special characters can be kind of arcane. Want to put in a link to everybodyâ€™s favorite website? Well, youâ€™re going to have to type in <a href=\"https://anildash.com/\">Anil Dashâ€™s blog</a> I could explain why, and what it all means, but honestly, you get the point â€” itâ€™s a lot! Too much. What if you could just write out the text and then the link, sort of like you might within an email? Like: [Anil Dashâ€™s blog](https://anildash.com)! And then the right thing would happen. Seems great, right?\nThe same thing works for things like putting a header on a page. For example, as Iâ€™m writing this right now, if I want to put a big headline on this page, I can just type #How Markdown Took Over the World and the right thing will happen.\nIf mark_up_ is complicated, then the opposite of that complexity must beâ€¦ markd_own_. This kind of solution, where itâ€™s so smart it seems obvious in hindsight, is key to Markdownâ€™s success. John worked to make a format that was so simple that anybody could pick it up in a few minutes, and powerful enough that it could help people express pretty much anything that they wanted to include while writing on the internet. At a technical level, it was also easy enough to implement that John could write the code himself to make it work with Movable Type, his publishing tool of choice. (Within days, people had implemented the same feature for most of the other blogging tools of the era; these days, virtually every app that you can type text into ships with Markdown support as a feature on day one.)\nPrior to launch, John had enlisted our mutual friend, the late, dearly missed Aaron Swartz, as a beta tester. In addition to being extremely fluent in every detail of the blogging technologies of the time, Aaron was, most notably, seventeen years old. And though Aaronâ€™s activism and untimely passing have resulted in him having been turned into something of a mythological figure, one of the greatest things about Aaron was that he could be a total pain in the ass, which made him terrific at reporting bugs in your software. (One of the last email conversations I ever had with Aaron was him pointing out some obscure bugs in an open source app I was working on at the time.) No surprise, Aaron instantly understood both the potential and the power of Markdown, and was a top-tier beta tester for the technology as it was created. His astute feedback helped finely hone the final product so it was ready for the world, and when Markdown quietly debuted in March of 2004, it was clear that text files around the web were about to get a permanent upgrade.\nThe most surprising part of what happened next wasnâ€™t that everybody immediately started using it to write their blogs; that was, after all, what the tool was designed to do. Itâ€™s that everybody started using Markdown to do everything else, too.\nHitting the Mark\nItâ€™s almost impossible to overstate the ubiquity of Markdown within the modern computer industry in the decades since its launch.\nAfter being nagged about it by users for more than a decade, Google finally added support for Markdown to Google Docs, though it took them years of fiddly improvements to make it truly usable. Just last year, Microsoft added support for Markdown to its venerable Notepad app, perhaps in attempt to assuage the tempers of users who were still in disbelief that Notepad had been bloated with AI features. Nearly every powerful group messaging app, from Slack to WhatsApp to Discord, has support for Markdown in messages. And even the company that indirectly inspired all of this in the first place finally got on board: the most recent version of Apple Notes finally added support for Markdown. (Itâ€™s an especially striking launch by Apple due to its timing, shortly after John had used his platform as the most influential Apple writer in the world to blog about the utter failure of the â€œApple Intelligenceâ€ AI launch.)\nBut itâ€™s not just the apps that you use on your phone or your laptop. For developers, Markdown has long been the lingua franca of the tools we string together to accomplish our work. On GitHub, the platform that nearly every developer in the world uses to share their code, nearly every single repository of code on the site has at least one Markdown file thatâ€™s used to describe its contents. Many have dozens of files describing all the different aspects of their project. And some of the repositories on GitHub consist of nothing but massive collections of Markdown files. The small tools and automations we run to perform routine tasks, the one-off reports that we generate to make sure something worked correctly, the confirmations that we have a system email out when something goes wrong, the temporary files we use when trying to recover some old data â€”Â all of these default to being Markdown files.\nAs a result, there are now billions of Markdown files lying around on hard drives around the world. Billions more are stashed in the cloud. There are some on the phone in your pocket. Programmers leave them lying around wherever their code might someday be running. Your kidâ€™s Nintendo Switch has Markdown files on it. If youâ€™re listening to music, thereâ€™s probably a Markdown file on the memory chip of the tiny system that controls the headphones stuck in your ears. The Markdown is inside you right now!\nDown For Whatever\nSo far, these were all things we could have foreseen when John first unleashed his little text tool on the world. I would have been surprised about how many people were using it, but not really the ways in which they were using it. If youâ€™d have said â€œTwenty years in the future, all the different note-taking apps people use save their files using Markdown!â€, I would have said, â€œOkay, that makes sense!â€\nWhat I wouldnâ€™t have asked, though, was â€œIs John getting paid?â€ As hard as it may be to believe, back in 2004, the default was that people made new standards for open technologies like Markdown, and just shared them freely for the good of the internet, and the world, and then went on about their lives. If it happened to have unleashed billions of dollars of value for others, then so much the better. If they got some credit along the way, that was great, too. But mostly you just did it to solve a problem for yourself and for other like-minded people. And also, maybe, to help make sure that some jerk didnâ€™t otherwise create some horrible proprietary alternative that would lock everybody into their terrible inferior version forever instead. (We didnâ€™t have the word â€œenshittificationâ€ yet, but we did have Cory Doctorow and we did have plain text files, so we kind of knew where things were headed.)\nTo give a sense of the vibe of that era, the term â€œpodcastingâ€ had been coined just a month before Markdown was released, and went into wider use that fall, and was similarly a radically open system that wasnâ€™t owned by any big company and that empowered people to do whatever they wanted to do to express themselves. (And podcasting was another technology that Aaron Swartz helped improve by being a brilliant pain in the ass. But Iâ€™ll save that story for another book-length essay.)\nThat attitude of being not-quite-_anti_commercial, but perhaps just not even really concerned with whether something was commercial or not seems downright quaint in an era when the tech tycoons are not just the wealthiest people in the world, but also some of the weirdest and most obnoxious as well. But the truth is, most people today who make technology are actually still exceedingly normal, and quite generous. Itâ€™s just that theyâ€™ve been overshadowed by their bosses who are out of their minds and building rocket ships and siring hundreds of children and embracing overt white supremacy instead of making fun tools for helping you type text, like regular people do.\n\nThe Markdown Model\nThe part about not doing this stuff solely for money matters, because even the most advanced LLM systems today, what the big AI companies call their â€œfrontierâ€ models, require complex orchestration thatâ€™s carefully scripted by people whoâ€™ve tuned their prompts for these systems through countless rounds of trial and error. Theyâ€™ve iterated and tested and watched for the results as these systems hallucinated or failed or ran amok, chewing up countless resources  along the way. And sometimes, they generated genuinely astonishing outputs, things that are truly amazing to consider that modern technology can achieve. The rate of progress and evolution, even factoring in the mind-boggling amounts of investment that are going into these systems, is rivaled only by the initial development of the personal computer or the Internet, or the early space race.\nAnd all of it â€” all of it â€”Â is controlled through Markdown files. When you see the brilliant work shown off from somebody whoâ€™s bragging about what they made ChatGPT generate for them, or someone is understandably proud about the code that they got Claude to create, all of the most advanced work has been prompted in Markdown. Though where the logic of Markdown was originally a very simple version of \"use human language to tell the machine what to do\", the implications have gotten far more dire when they use a format designed to help expresss \"make this **bold**\" to tell the computer itself \"make this imaginary girlfriend more compliant\".\nBut we already know that the Big AI companies are run by people who don't reckon with the implications of their work. They could never understand that every single project that's even moderately ambitious on these new AI platforms is being written up in files formatted according to this system created by one guy who has never asked for a dime for this work. An entire generation of AI coders has been born since Markdown was created who probably canâ€™t even imagine that this technology even has an \"inventor\". Itâ€™s just always been here, like the Moon, or Rihanna.\nBut itâ€™s important for everyone to know that the Internet, and the tech industry, donâ€™t run without the generosity and genius of regular people. It is not just billion-dollar checks and Silicon Valley boardrooms that enable creativity over years, decades, or generations â€”Â itâ€™s often a guy with a day job who just gives a damn about doing something right, sweating the details and assuming that if he cares enough about what he makes then others will too. The majority of the technical infrastructure of the Internet was created in this way. For free, often by people in academia, or as part of their regular work, with no promise of some big payday or getting a ton of credit.\nThe people who make the real Internet and the real innovations also donâ€™t look for ways to hurt the world around them, or the people around them. Sometimes, as in the case of Aaron, the world hurts them more than anyone should ever have to bear. I know not everybody cares that much about plain text files on the Internet; I will readily admit I am a huge nerd about this stuff in a way that maybe most normal people are not. But I do think everybody cares about some part of the wonderful stuff on the Internet in this way, and I want to fight to make sure that everybody can understand that itâ€™s not just five terrible tycoons who built this shit. Real people did. Good people. I saw them do it.\nThe trillion-dollar AI industry's system for controlling their most advanced platforms is a plain text format one guy made up for his blog and then bounced off of a 17-year-old kid before sharing it with the world for free. You're welcome, Time Magazine's people of the year, The Architects of AI. Their achievement is every bit as impressive as yours.\n\nThe Ten Technical Reasons Markdown Won\nOkay, with some of the narrative covered, what can we learn from Markdownâ€™s success? How did this thing really take off? What could we do if we wanted to replicate something like this in the modern era? Letâ€™s consider a few key points:\n1. Had a great brand.\nOkay, letâ€™s be real: â€œMarkdownâ€ as a name is clever as hell. Get it itâ€™s not markup, itâ€™s mark down. You just canâ€™t argue with that kind of logic. People who knew what the â€œMâ€ in â€œHTMLâ€ stood for could understand the reference, and to everyone else, it was just a clearly-understandable name for a useful utility.\n2. Solved a real problem.\nThis one is not obvious, but itâ€™s really important that a new technology have a real problem that itâ€™s trying to solve, instead of just being an abstract attempt to do something vague, like â€œmake text files betterâ€. Millions of people were encountering the idea that it was too difficult or inconvenient to write out full HTML by hand, and even if one had the necessary skills, it was nice to be able to do so in a format that was legible as plain text as well.\n3. Built on behaviors that already existed.\nThis is one of the most quietly genius parts of Markdown: The format is based on the ways people had been adding emphasis and formatting to their text for years or even decades. Some of the formatting choices dated back to the early days of email, so theyâ€™d been ingrained in the culture of the internet for a full generation before Markdown existed. It was so familiar, people could be writing Markdown without even knowing it.\n4. Mirrored RSS in its origin.\nAround the same time that Markdown was taking off, RSS was maturing into its ubiquitous form as well. The format had existed for some years already, enabling various kinds of content syndication, but at this time, it was adding support for the technologies that would come to be known as podcasting as well. And just like RSS, Markdown was spearheaded by a smart technologist who was also more than a little stubborn about defining a format that would go on to change the way we share content on the internet. In RSSâ€™ case, it was pioneered by Dave Winer, and with Markdown it was John Gruber, and both were tireless in extolling the virtues of the plain text formats theyâ€™d helped pioneer. They could both leverage blogs to get the word out, and to get feedback on how to build on their wins.\n5. There was a community ready to help.\nOne great thing about a format like Markdown is that its success is never just the result of one person. Vitally, Markdown was part of a community that could build on it right from the start. Right from the beginning, Markdown was inspired by earlier works like Textile, a formatting system for plain text created by Dean Allen. Many of us appreciated and were inspired by Dean, who was a pioneer of blogging tools in the early days of social media, but if thereâ€™s a bigger fan of Dean Allen on the internet than John Gruber, Iâ€™ve never met them. Similarly, Aaron Swartz, the brilliant young technologist whoâ€™s known best known as an activist for digital rights and access, was at that time just a super brilliant teenager that a lot of us loved hacking with. He was the most valuable beta tester of Markdown prior to its release, helping to shape it into a durable and flexible format thatâ€™s stood the test of time.\n6. Had the right flavor for every different context.\nBecause Markdownâ€™s format was frozen in place (and had some super-technical details that people could debate about) and people wanted to add features over time, various communities that were implementing Markdown could add their own â€œflavorsâ€ of it as they needed. Popular ones came to be called Commonmark and Github-Flavored, led by various companies or teams that had divergent needs for the tool. While tech geeks tend to obsess over needing everything to be â€œcorrectâ€, in reality it often just doesnâ€™t matter that much, and in the real world, the entire Internet is made up of content that barely follows the technical rules that itâ€™s supposed to.\n7. Released at a time of change in behaviors and habits.\nThis is a subtle point, but an important one: Markdown came along at the right time in the evolution of its medium. You can get people to change their behaviors when theyâ€™re using a new tool, or adopting a new technology. In this case, blogging (and all of social media!) were new, so saying â€œhereâ€™s a new way of typing a list of bullet pointsâ€ wasnâ€™t much an additional learning curve to add to the mix. If you can take advantage of catching people while theyâ€™re already in a learning mood, you can really tap into the moment when theyâ€™re most open-minded to new things.\n8. Came right on the cusp of the â€œbuild tool eraâ€.\nThis oneâ€™s a bit more technical, but also important to understand. In the first era of building for the web, people often built the webâ€™s languages of HTML, Javascript and CSS by hand, by themselves, or stitched these formats together from subsets or templates. But in many cases, these were fairly simple compositions, made up of smaller pieces that were written in the same languages. As things matured, the roles for web developers specialized (there started to be backend developers vs. front-end, or people who focused on performance vs. those who focused on visual design), and as a result the tooling for developers matured. On the other side of this transition, developers began to use many different programming languages, frameworks and tools, and the standard step before trying to deploy a website was to have an automated build process that transformed the â€œraw materialsâ€ of the site into the finished product. Since Markdown is a raw material that has to be transformed into HTML, it perfectly fit this new workflow as it became the de facto standard method of creation and collaboration.\n9. Worked with â€œView sourceâ€\nMost of the technologies that work best on the web enable creators to â€œview sourceâ€ just like HTML originally did when the first web browsers were created. In this philosophy, one can look at the source code that makes up a web page, and understand how it was constructed so that you can make your own. With Markdown, it only takes one glimpse of a source Markdown file for anyone to understand how they might make a similar file of their own, or to extrapolate how they might apply analogous formatting to their own documents. Thereâ€™s no teaching required when people can just see it for themselves.\n10. Not encumbered in IP\nThis oneâ€™s obvious if you think about it, but it canâ€™t go unsaid: There are no legal restrictions around Markdown. You wouldnâ€™t think that anybody would be foolish or greedy enough to try to patent something as simple as Markdown, but there are many far worse examples of patent abuse in the tech industry. Fortunately, John Gruber is not an awful person, and nobody else has (yet) been brazen enough to try to usurp the format for their own misadventures in intellectual property law. As a result, nobodyâ€™s been afraid, either to use the format, or to support creating or reading the format in their apps."
    },
    {
      "title": "anildash.com: How to know if that job will crush your soul",
      "url": "https://anildash.com/2026/01/12/will-that-job-crush-your-soul/",
      "source": "anildash.com",
      "time": "2026-02-20T10:18:14.933338",
      "raw_desc": "Last week, we talked about one huge question, â€œHow the hell are you supposed to have a career in tech in 2026?â€ Thatâ€™s pretty specific to this current moment, but there are some timeless, more perennial questions I've been sharing with friends for years that I wanted to give to all of you. They're a short list of questions that help you judge whether a job that youâ€™re considering is going to crush your soul or not.\nObviously, not everyone is going to get to work in an environment that has perfect answers to all of these questions; a lot of the time, weâ€™re lucky just to get a place to work at all. But these questions are framed in this way to encourage us all to aspire towards roles that enable us to do our best work, to have the biggest impact, and to live according to our values.\nThe Seven Questions\n\nIf what you do succeeds, will the world be better?\n\nThis question originally started for me when I would talk to people about new startups, where people were judging the basic idea of the product or the company itself, but it actually applies to any institution, at any size. If the organization that youâ€™re considering working for, or the team youâ€™re considering joining, is able to achieve their stated goals, is it ultimately going to have a positive effect? Will you be proud of what it means? Will the people you love and care about respect you for making that choice, and will those with the least to gain feel like youâ€™re the kind of person who cares about their impact on the world?\n\nWhose money do they have to take to stay in business?\n\nWhere does the money in the organization really come from? You need to know this for a lot of reasons. First of all, you need to be sure that they know the answer. (Youâ€™d be surprised how often thatâ€™s not the case!) Even if they do know the answer, it may make you realize that those customers are not the people whose needs or wants youâ€™d like to spend most of your waking hours catering to. This goes beyond the simple basics of the business model â€” it can be about whether they're profitable or not, and what the corporate ownership structure is like.\nItâ€™s also increasingly common for companies to mistake those who are investing in a company with those who are their customers. But thereâ€™s a world of difference between those who are paying you, and those who you have to pay back tenfold. Or thousandfold.\nThe same goes for nonprofits â€” do you know who has to stay happy and smiling in order for the institution to stay stable and successful? If you know those answers, you'll be far more confident about the motivations and incentives that will drive key decisions within the organization.\n\nWhat do you have to believe to think that theyâ€™re going to succeed? In what way does the world have to change or not change?\n\nNow weâ€™re getting a little bit deeper into thinking about the systems that surround the organization that youâ€™re evaluating. Every company, every institution, even every small team, is built around a set of invisible assumptions. Many times, theyâ€™re completely reasonable assumptions that are unlikely to change in the future. But sometimes, the world youâ€™re working in is about to shift in a big way, or things are built on a foundation thatâ€™s speculative or even unrealistic.\nMaybe they're assuming there aren't going to be any big new competitors. Perhaps they think they'll always remain the most popular product in their category. Or their assumptions could be about the stability of the rule of law, or a lack of corruption â€” more fundamental assumptions that they've never seen challenged in their lifetime or in their culture, but that turn out to be far more fragile than they'd imagined.\nThinking through the context that everyone is sharing, and reflecting on whether theyâ€™re really planning for any potential disruptions, is an essential part of judging the psychological health of an organization. Itâ€™s the equivalent of a person having self-awareness, and itâ€™s just as much of a red flag if itâ€™s missing.\n\nWhatâ€™s the lived experience of the workers there whom you trust? Do you have evidence of leaders in the organization making hard choices to do the right thing?\n\nHere is how we can tell the culture and character of an organization. If youâ€™ve got connections into the company, or a backchannel to workers there, finding out as much information as you can about the real story of its working conditions is often one of the best ways of understanding whether itâ€™s a fit for your needs. Now, people can always have a bad day, but overall, workers are usually very good at providing helpful perspectives about their context.\nAnd more broadly, if people can provide examples of those in power within an organization using that power to take care of their workers or customers, or to fight for the company to be more responsible, then youâ€™ve got an extremely positive sign about the health of the place even before youâ€™ve joined. Itâ€™s vital that these be stories you are able to find and discover on your own, not the ones amplified by the institution itself for PR purposes.\n\nWhat were you wrong about?\n\nAnd here we have perhaps one of the easiest and most obvious ways to judge the culture of an organization. This is even a question you can ask people while youâ€™re in an interview process, and you can judge their responses to help form your opinion. A company, and leadership culture, that can change its mind when faced with new information and new circumstances is much more likely to adapt to challenges in a healthy way. (If you want to be nice, phrase it as \"What is a way in which the company has evolved or changed?\")\n\nDoes your actual compensation take care of what you need for all of your current goals and needs â€”Â from day one?\n\nThis is where we go from the abstract and psychological goals to the practical and everyday concerns: can you pay your bills? The phrasing and framing here is very intentional: are they really going to pay you enough? I ask this question very specifically because youâ€™d be surprised how often companies actually dance around this question, or how often we trick ourselves into hearing what we want to hear as the answer to this question when weâ€™re in the exciting (or stressful) process of considering a new job, instead of looking at the facts of whatâ€™s actually written in black-and-white on an offer letter.\nIt's also important not to get distracted with potential, even if you're optimistic about the future. Donâ€™t listen to promises about what might happen, or descriptions of whatâ€™s possible if you advance in your role. Think about what your real life will be like, after taxes, if you take the job that theyâ€™ve described.\n\nIs the role youâ€™re being hired into one where you can credibly advance, and where thereâ€™s sufficient resources for success?\n\nThis is where you can apply your optimism in a practical way: can the organization accurately describe how your career will proceed within the company? Does it have a specific and defined trajectory, or does it involve ambiguous processes or changes in teams or departments? Would you have to lobby for the support of leaders from other parts of the organization? Would making progress require acquiring new skills or knowledge? Have they committed to providing you with the investment and resources required to learn those skills?\nThese questions are essential to understand, because lacking these answers can lead to an ugly later realization that even an initially-exciting position may turn out to be a dead-end job over time.\nTowards better working worlds\nSometimes it can really feel like the deck is stacked against you when you're trying to find a new job. It can feel even worse to be faced with an opportunity and have a nagging sense that something is not quite right. Much of the time, that feeling comes from the vague worry that we're taking a job that is going to make us miserable.\nEven in a tough job market, there are some places that are trying to do their best to treat people decently. In larger organizations, there are often pockets of relative sanity, led by good leaders, who are trying to do the right thing. It can be a massive improvement in quality of life if you can find these places and use them as foundations for the next stage of your career.\nThe best way to navigate towards these better opportunities is to be systematic when evaluating all of your options, and to hold out for as high standards as possible when you're out there looking. These seven questions give you the tools to do exactly that."
    },
    {
      "title": "computer.rip: forecourt networking",
      "url": "https://computer.rip/2026-02-08-forecourt-networking.html",
      "source": "computer.rip",
      "time": "2026-02-20T10:18:15.723674",
      "raw_desc": ""
    },
    {
      "title": "computer.rip: the essence of frigidity",
      "url": "https://computer.rip/2026-01-25-the-essence-of-frigidity.html",
      "source": "computer.rip",
      "time": "2026-02-20T10:18:15.723674",
      "raw_desc": ""
    },
    {
      "title": "computer.rip: air traffic control: the IBM 9020",
      "url": "https://computer.rip/2026-01-17-air-traffic-control-9020.html",
      "source": "computer.rip",
      "time": "2026-02-20T10:18:15.723674",
      "raw_desc": ""
    },
    {
      "title": "mjg59.dreamwidth.org: Not here",
      "url": "https://mjg59.dreamwidth.org/74084.html",
      "source": "mjg59.dreamwidth.org",
      "time": "2026-02-20T10:18:15.806594",
      "raw_desc": ""
    },
    {
      "title": "mjg59.dreamwidth.org: How did IRC ping timeouts end up in a lawsuit?",
      "url": "https://mjg59.dreamwidth.org/73777.html",
      "source": "mjg59.dreamwidth.org",
      "time": "2026-02-20T10:18:15.806594",
      "raw_desc": ""
    },
    {
      "title": "mjg59.dreamwidth.org: Where are we on X Chat security?",
      "url": "https://mjg59.dreamwidth.org/73625.html",
      "source": "mjg59.dreamwidth.org",
      "time": "2026-02-20T10:18:15.806594",
      "raw_desc": ""
    },
    {
      "title": "michael.stapelberg.ch: Coding Agent VMs on NixOS with microvm.nix",
      "url": "https://michael.stapelberg.ch/posts/2026-02-01-coding-agent-microvm-nix/",
      "source": "michael.stapelberg.ch",
      "time": "2026-02-20T10:18:15.959578",
      "raw_desc": "I have come to appreciate coding\nagents to be\nvaluable tools for working with computer program code in any capacity, such as\nlearning about any programâ€™s architecture, diagnosing bugs or developing proofs\nof concept. Depending on the use-case, reviewing each command the agent wants to\nrun can get tedious and time-consuming very quickly. To safely run a coding\nagent without review, I wanted a Virtual Machine (VM) solution where the agent\nhas no access to my personal files and where itâ€™s no big deal if the agent gets\ncompromised by malware: I can just throw away the VM and start over.\nInstead of setting up a stateful VM and re-installing it when needed (ugh!), I\nprefer the model of ephemeral VMs where nothing persists on disk, except for\nwhat is explicitly shared with the host.\nThe microvm.nix project makes it\neasy to create such VMs on NixOS, and this article shows you how I like to set\nup my VMs.\nSee also\nIf you havenâ€™t heard of NixOS before, check out the NixOS Wikipedia\npage and\nnixos.org. I spoke about why I switched to Nix in\n2025 and have published a few blog posts about\nNix.\nFor understanding the threat model of AI agents, read Simon Willisonâ€™s â€œThe\nlethal trifecta for AI agents: private data, untrusted content, and external\ncommunicationâ€ (June\n2025). This\narticleâ€™s approach to working with the threat model is to remove the â€œprivate\ndataâ€ part from the equation.\nIf you want to learn about the whole field of sandboxing, check out Luis\nCardosoâ€™s â€œA field guide to sandboxes for AIâ€ (Jan\n2026). I will not be\ncomparing different solutions in this article, I will just show you one possible\npath.\nAnd lastly, maybe youâ€™re not in the mood to build/run sandboxing infrastructure\nyourself. Good news: Sandboxing is a hot topic and there are many commercial\nofferings popping up that address this need. For example, David Crawshaw and\nJosh Bleecher Snyder (I know both from the Go community) recently launched\nexe.dev, an agent-friendly VM hosting\nservice. Another example is Fly.io, who launched\nSprites.\nSetting up microvm.nix\nLetâ€™s jump right in! The next sections walk you through how I set up my config.\nStep 1: network prep\nFirst, I created a new microbr bridge which uses 192.168.33.1/24 as IP address range and NATs out of the eno1 network interface. All microvm* interfaces will be added to that bridge:\nsystemd.network.netdevs.\"20-microbr\".netdevConfig = {\n  Kind = \"bridge\";\n  Name = \"microbr\";\n};\n\nsystemd.network.networks.\"20-microbr\" = {\n  matchConfig.Name = \"microbr\";\n  addresses = [ { Address = \"192.168.83.1/24\"; } ];\n  networkConfig = {\n    ConfigureWithoutCarrier = true;\n  };\n};\n\nsystemd.network.networks.\"21-microvm-tap\" = {\n  matchConfig.Name = \"microvm*\";\n  networkConfig.Bridge = \"microbr\";\n};\n\nnetworking.nat = {\n  enable = true;\n  internalInterfaces = [ \"microbr\" ];\n  externalInterface = \"eno1\";\n};\nStep 2: flake.nix\nThen, I added the microvm module as a new input to my flake.nix (check out\nthe microvm.nix documentation for\ndetails) and enabled the microvm.nixosModules.host module on the NixOS\nconfiguration for my PC (midna). I also created a new microvm.nix file, in\nwhich I declare all my VMs. Hereâ€™s what my flake.nix looks like:\n{\n  inputs = {\n    nixpkgs = {\n      url = \"github:nixos/nixpkgs/nixos-25.11\";\n    };\n    # For more recent claude-code\n    nixpkgs-unstable = {\n      url = \"github:nixos/nixpkgs/nixos-unstable\";\n    };\n    stapelbergnix = {\n      url = \"github:stapelberg/nix\";\n      inputs.nixpkgs.follows = \"nixpkgs\";\n    };\n    zkjnastools = {\n      url = \"github:stapelberg/zkj-nas-tools\";\n      inputs.nixpkgs.follows = \"nixpkgs\";\n    };\n    microvm = {\n      url = \"github:microvm-nix/microvm.nix\";\n      inputs.nixpkgs.follows = \"nixpkgs\";\n    };\n    home-manager = {\n      url = \"github:nix-community/home-manager/release-25.11\";\n      inputs.nixpkgs.follows = \"nixpkgs\";\n    };\n    configfiles = {\n      url = \"github:stapelberg/configfiles\";\n      flake = false; # repo is not a flake\n    };\n  };\n\n  outputs =\n    {\n      self,\n      stapelbergnix,\n      zkjnastools,\n      nixpkgs,\n      nixpkgs-unstable,\n      microvm,\n      home-manager,\n      configfiles,\n    }@inputs:\n    let\n      system = \"x86_64-linux\";\n      pkgs = import nixpkgs {\n        inherit system;\n        config.allowUnfree = false;\n      };\n      pkgs-unstable = import nixpkgs-unstable {\n        inherit system;\n        config.allowUnfree = true;\n      };\n    in\n    {\n      nixosConfigurations = {\n        midna = nixpkgs.lib.nixosSystem {\n          system = \"x86_64-linux\";\n          specialArgs = { inherit inputs; };\n          modules = [\n            (import ./configuration.nix)\n            stapelbergnix.lib.userSettings\n            # Use systemd for network configuration\n            stapelbergnix.lib.systemdNetwork\n            # Use systemd-boot as bootloader\n            stapelbergnix.lib.systemdBoot\n            # Run prometheus node exporter in tailnet\n            stapelbergnix.lib.prometheusNode\n            zkjnastools.nixosModules.zkjbackup\n            microvm.nixosModules.host\n            ./microvm.nix\n          ];\n        };\n      };\n    };\n}\nStep 3: microvm.nix\nThe following microvm.nix declares two microvms, one for Emacs (about which I wanted to learn more) and one for Go Protobuf, a code base I am familiar with and can use to understand Claudeâ€™s capabilities:\n{\n  config,\n  lib,\n  pkgs,\n  inputs,\n  ...\n}:\n\nlet\n  inherit (inputs)\n    nixpkgs-unstable\n    stapelbergnix\n    microvm\n    configfiles\n    home-manager\n    ;\n\n  microvmBase = import ./microvm-base.nix;\nin\n{\n  microvm.vms.emacsvm = {\n    autostart = false;\n    config = {\n      imports = [\n        stapelbergnix.lib.userSettings\n        microvm.nixosModules.microvm\n        (microvmBase {\n          hostName = \"emacsvm\";\n          ipAddress = \"192.168.83.6\";\n          tapId = \"microvm4\";\n          mac = \"02:00:00:00:00:05\";\n          workspace = \"/home/michael/microvm/emacs\";\n          inherit\n            nixpkgs-unstable\n            configfiles\n            home-manager\n            stapelbergnix\n            ;\n        })\n        ./microvms/emacs.nix\n      ];\n    };\n  };\n\n  microvm.vms.goprotobufvm = {\n    autostart = false;\n    config = {\n      imports = [\n        stapelbergnix.lib.userSettings\n        microvm.nixosModules.microvm\n        (microvmBase {\n          hostName = \"goprotobufvm\";\n          ipAddress = \"192.168.83.7\";\n          tapId = \"microvm5\";\n          mac = \"02:00:00:00:00:06\";\n          workspace = \"/home/michael/microvm/goprotobuf\";\n          inherit\n            nixpkgs-unstable\n            configfiles\n            home-manager\n            stapelbergnix\n            ;\n          extraZshInit = ''\n            export GOPATH=$HOME/go\n            export PATH=$GOPATH/bin:$PATH\n          '';\n        })\n        ./microvms/goprotobuf.nix\n      ];\n    };\n  };\n}\nStep 4: microvm-base.nix\nThe microvm-base.nix module takes these parameters and declares:\n\nNetwork settings: I like using systemd-networkd(8)\n and systemd-resolved(8)\n.\nShared directories for:\n\nthe workspace directory, e.g. ~/microvm/emacs\nthe hostâ€™s Nix store, so the VM can access software from cache (often)\nthis VMâ€™s SSH host keys\n~/claude-microvm, which is a separate state directory, used only on the microvms.\n\n\nan 8 GB disk overlay (var.img), stored in /var/lib/microvms/<name>\ncloud-hypervisor (QEMU also works well!) as the hypervisor, with 8 vCPUs and 4 GB RAM.\nA workaround for systemd trying to unmount /nix/store (which causes a deadlock).\n\n\nExpand full microvm-base.nix code\n{\n  hostName,\n  ipAddress,\n  tapId,\n  mac,\n  workspace,\n  nixpkgs-unstable,\n  configfiles,\n  home-manager,\n  stapelbergnix,\n  extraZshInit ? \"\",\n}:\n\n{\n  config,\n  lib,\n  pkgs,\n  ...\n}:\n\nlet\n  system = pkgs.stdenv.hostPlatform.system;\n  pkgsUnstable = import nixpkgs-unstable {\n    inherit system;\n    config.allowUnfree = true;\n  };\nin\n{\n  imports = [ home-manager.nixosModules.home-manager ];\n\n  # home-manager configuration\n  home-manager.useGlobalPkgs = true;\n  home-manager.useUserPackages = true;\n  home-manager.extraSpecialArgs = { inherit configfiles stapelbergnix; };\n  home-manager.users.michael = {\n    imports = [ ./microvm-home.nix ];\n    microvm.extraZshInit = extraZshInit;\n  };\n\n  # Claude Code CLI (from nixpkgs-unstable, unfree)\n  environment.systemPackages = [\n    pkgsUnstable.claude-code\n  ];\n  networking.hostName = hostName;\n\n  system.stateVersion = \"25.11\";\n\n  services.openssh.enable = true;\n\n  # To match midna (host)\n  users.groups.michael = {\n    gid = 1000;\n  };\n  users.users.michael = {\n    group = \"michael\";\n  };\n\n  services.resolved.enable = true;\n  networking.useDHCP = false;\n  networking.useNetworkd = true;\n  networking.tempAddresses = \"disabled\";\n  systemd.network.enable = true;\n  systemd.network.networks.\"10-e\" = {\n    matchConfig.Name = \"e*\";\n    addresses = [ { Address = \"${ipAddress}/24\"; } ];\n    routes = [ { Gateway = \"192.168.83.1\"; } ];\n  };\n  networking.nameservers = [\n    \"8.8.8.8\"\n    \"1.1.1.1\"\n  ];\n\n  # Disable firewall for faster boot and less hassle;\n  # we are behind a layer of NAT anyway.\n  networking.firewall.enable = false;\n\n  systemd.settings.Manager = {\n    # fast shutdowns/reboots! https://mas.to/@zekjur/113109742103219075\n    DefaultTimeoutStopSec = \"5s\";\n  };\n\n  # Fix for microvm shutdown hang (issue #170):\n  # Without this, systemd tries to unmount /nix/store during shutdown,\n  # but umount lives in /nix/store, causing a deadlock.\n  systemd.mounts = [\n    {\n      what = \"store\";\n      where = \"/nix/store\";\n      overrideStrategy = \"asDropin\";\n      unitConfig.DefaultDependencies = false;\n    }\n  ];\n\n  # Use SSH host keys mounted from outside the VM (remain identical).\n  services.openssh.hostKeys = [\n    {\n      path = \"/etc/ssh/host-keys/ssh_host_ed25519_key\";\n      type = \"ed25519\";\n    }\n  ];\n\n  microvm = {\n    # Enable writable nix store overlay so nix-daemon works.\n    # This is required for home-manager activation.\n    # Uses tmpfs by default (ephemeral), which is fine since we\n    # don't build anything in the VM.\n    writableStoreOverlay = \"/nix/.rw-store\";\n\n    volumes = [\n      {\n        mountPoint = \"/var\";\n        image = \"var.img\";\n        size = 8192; # MB\n      }\n    ];\n\n    shares = [\n      {\n        # use proto = \"virtiofs\" for MicroVMs that are started by systemd\n        proto = \"virtiofs\";\n        tag = \"ro-store\";\n        # a host's /nix/store will be picked up so that no\n        # squashfs/erofs will be built for it.\n        source = \"/nix/store\";\n        mountPoint = \"/nix/.ro-store\";\n      }\n      {\n        proto = \"virtiofs\";\n        tag = \"ssh-keys\";\n        source = \"${workspace}/ssh-host-keys\";\n        mountPoint = \"/etc/ssh/host-keys\";\n      }\n      {\n        proto = \"virtiofs\";\n        tag = \"claude-credentials\";\n        source = \"/home/michael/claude-microvm\";\n        mountPoint = \"/home/michael/claude-microvm\";\n      }\n      {\n        proto = \"virtiofs\";\n        tag = \"workspace\";\n        source = workspace;\n        mountPoint = workspace;\n      }\n    ];\n\n    interfaces = [\n      {\n        type = \"tap\";\n        id = tapId;\n        mac = mac;\n      }\n    ];\n\n    hypervisor = \"cloud-hypervisor\";\n    vcpu = 8;\n    mem = 4096;\n    socket = \"control.socket\";\n  };\n}\n\nStep 5: microvm-home.nix\nmicrovm-base.nix in turn pulls in microvm-home.nix, which sets up home-manager to:\n\nSet up Zsh with my configuration\nSet up Emacs with my configuration\nSet up Claude Code in shared directory ~/claude-microvm.\n\n\nExpand full microvm-home.nix code\n{\n  config,\n  pkgs,\n  lib,\n  configfiles,\n  stapelbergnix,\n  ...\n}:\n\n{\n  options.microvm = {\n    extraZshInit = lib.mkOption {\n      type = lib.types.lines;\n      default = \"\";\n      description = \"Extra lines to add to zsh initContent\";\n    };\n  };\n\n  config = {\n    home.username = \"michael\";\n    home.homeDirectory = \"/home/michael\";\n\n    programs.zsh = {\n      enable = true;\n      history = {\n        size = 4000;\n        save = 10000000;\n        ignoreDups = true;\n        share = false;\n        append = true;\n      };\n\n      initContent = ''\n        ${builtins.readFile \"${configfiles}/zshrc\"}\n        export CLAUDE_CONFIG_DIR=/home/michael/claude-microvm\n        ${config.microvm.extraZshInit}\n      '';\n    };\n\n    programs.emacs = {\n      enable = true;\n      package = stapelbergnix.lib.emacsWithPackages { inherit pkgs; };\n    };\n\n    home.file.\".config/emacs\" = {\n      source = \"${configfiles}/config/emacs\";\n    };\n\n    home.stateVersion = \"25.11\";\n\n    programs.home-manager.enable = true;\n  };\n}\n\nStep 6: goprotobuf.nix\nThe goprotobuf.nix makes available a bunch of required and convenient packages:\n# Project-specific configuration for goprotobufvm\n{ pkgs, ... }:\n{\n  # Development environment for Go Protobuf\n  environment.systemPackages = with pkgs; [\n    # Go toolchain\n    go\n    gopls\n    delve\n    protobuf\n    gnumake\n    gcc\n    git\n    ripgrep\n  ];\n}\nRunning the VM\nLetâ€™s create the workspace directory and create an SSH host key:\nmkdir -p ~/microvm/emacs/ssh-host-keys\nssh-keygen -t ed25519 -N \"\" \\\n  -f ~/microvm/emacs/ssh-host-keys/ssh_host_ed25519_key\nNow we can start the VM:\nsudo systemctl start microvm@emacsvm\nIt boots and responds to pings within a few seconds.\nThen, SSH into the VM (perhaps in a tmux(1)\n session) and run Claude\n(or your Coding Agent of choice) without permission prompts in the shared\nworkspace directory:\n% ssh 192.168.83.2\nemacsvm% cd microvm/emacs\nemacsvm% claude --dangerously-skip-permissions\nThis is what running Claude in such a setup looks like:\n\nCreating VMs with Claude\nAfter going through the process of setting up a MicroVM once, it becomes tedious.\nI was curious if Claude Skills could\nhelp with a task like this. Skills are markdown files that instruct Claude to do\ncertain steps in certain situations.\nI created .claude/skills/create-microvm/SKILL.md as follows:\n---\nname: create-microvm\ndescription: Creates a new microvm Virtual Machine on midna for running Claude in, with source code repositories and build dependencies available inside the microvm. Use when the user asks to create a new microvm.\n---\n\nInspect the existing structure at ~/machines/midna (NixOS configuration using Flakes), which includes several MicroVMs in the ~/machines/midna/microvms/ directory.\n\nThen, create a similar structure for the microvm the user asked to create. Be sure to consider:\n\n1. Create a new subdirectory for this microvm, named NAME (the microvm name).\n2. Create an entry in microvm.nix similar to an existing microvm's, but:\n3. Change hostname to NAME\n4. Change IP address (e.g., 192.168.83.3): find used ones and chose next free\n5. Change workspace share to /home/michael/microvm/NAME\n6. Include build dependencies for the new microvm based on user request\n7. Create ssh-host-keys directory\n8. Generate SSH host keys\n9. Clone/download source code repositories into this microvm based on user request\n\nOnce the microvm is defined, update ~/machines/midna to pull it in. To verify, build the config (but do not deploy).\n\nYou need to git-commit changes to the microvm before ~/machines/midna can see them.\nWhen using this skill with Claude Code (tested version: v2.0.76 and v2.1.15), with the Opus 4.5 model, I can send a prompt like this:\n\nplease set up a microvm for Debian Code Search (dcs). see ~/dcs for the source code (but clone from https://github.com/Debian/dcs)\n\nNow Claude churns for a few minutes, possibly asking a clarification question\nbefore that. Afterwards, Claude reports back with:\n\nThe dcsvm microvm has been set up successfully. Hereâ€™s what was created:\nConfiguration:\n\n/home/michael/machines/midna/microvms/dcs.nix - Project-specific packages (Go toolchain, protobuf tools, uglify-js, yuicompressor,\nzopfli)\nUpdated /home/michael/machines/midna/microvm.nix to include dcsvm with:\n\nIP: 192.168.83.8\ntapId: microvm6\nmac: 02:00:00:00:00:07\n\n\n\nWorkspace:\n\n/home/michael/microvm/dcs/ - Workspace directory\n/home/michael/microvm/dcs/ssh-host-keys/ - SSH host keys (ed25519)\n/home/michael/microvm/dcs/dcs/ - Cloned repository from https://github.com/Debian/dcs\n\nBuild verified - The configuration builds successfully.\nTo start the microvm after deploying:\nsystemctl start microvm@dcsvm\nTo SSH into it:\nssh michael@192.168.83.8\n\nWonderful! In my experience, Claude always got the VM creation correct.\nReplicating this setup with Claude Code\nIn fact, you can go one step further: Instead of just asking Claude to create\nnew MicroVMs, you can also ask Claude to replicate this entire setup into your\nNixOS configuration!\nTry a prompt like this:\n\nread\nhttps://michael.stapelberg.ch/posts/2026-02-01-coding-agent-microvm-nix/\nâ€” I want the exact same setup in my midna NixOS configuration please!\n\nConclusion\nNixOS has a reputation of being hard to adopt, but once you are using NixOS, you\ncan do powerful things like spinning up ephemeral MicroVMs for a new project\nwithin minutes.\nThe maintenance effort is minimal: When I update my personal PC, my MicroVM\nconfigurations start using the new software versions, too. Customization is easy\nif needed.\nThis actually mirrors my experience with Coding Agents: I donâ€™t feel like\ntheyâ€™re automatically making existing tasks more efficient, I feel that they\nmake things possible that were previously out of reach (similar to Jevons\nparadox).\nIt was fascinating (and scary!) to experience the quality increase of Coding\nAgents during 2025. At the beginning of 2025 I thought that LLMs are an\noverhyped toy, and felt it was almost insulting when people showed me text or\ncode produced by these models. But almost every new frontier model release got\nsignificantly better, and by now I have been positively surprised by Claude\nCodeâ€™s capabilities and quality many times. It has produced code that handles\nlegitimate edge cases I would not have considered.\nWith this article, I showed one possible way to run Coding Agents safely (or any\nworkload that shouldnâ€™t access your private data, really) that you can adjust in\nmany ways for your needs."
    },
    {
      "title": "michael.stapelberg.ch: Can I finally start using Wayland in 2026?",
      "url": "https://michael.stapelberg.ch/posts/2026-01-04-wayland-sway-in-2026/",
      "source": "michael.stapelberg.ch",
      "time": "2026-02-20T10:18:16.005460",
      "raw_desc": "Wayland is the successor to the X server (X11, Xorg) to implement the graphics\nstack on Linux. The Wayland\nproject was actually started in 2008, a year before I created the i3 tiling\nwindow manager for X11 in 2009 â€” but for the last 18 years\n(!), Wayland was never usable on my computers. I donâ€™t want to be stuck on\ndeprecated software, so I try to start using Wayland each year, and this\narticles outlines what keeps me from migrating to Wayland in 2026.\nHistorical context\nFor the first few years, Wayland rarely even started on my machines. When I was\nlucky enough for something to show up, I could start some toy demo apps in the\ndemo compositor Weston.\nAround 2014, GNOME started supporting Wayland. KDE followed a few years later.\nMajor applications (like Firefox, Chrome or Emacs) have been slower to adopt\nWayland and needed users to opt into experimental implementations via custom\nflags or environment variables, until very recently, or â€” in some cases, like\ngeeqie â€” still as of today.\nUnfortunately, the driver support situation remained poor for many years.  With\nnVidia graphics cards, which are the only cards that support my 8K\nmonitor, Wayland would either not work at all\nor exhibit heavy graphics glitches and crashes.\nIn the 2020s, more and more distributions announced looking to switch to Wayland\nby default or even drop their X11\nsessions, and RHEL\nis winding down their contributions to the X\nserver.\nModern Linux distributions like Asahi Linux (for\nMacs, with their own GPU driver!) clearly consider Wayland their primary desktop\nstack, and only support X11 on a best-effort basis.\nSo the pressure to switch to Wayland is mounting! Is it ready now? Whatâ€™s\nmissing?\nMaking Wayland start\nHardware\nIâ€™m testing with my lab PC, which is a slightly upgraded version of my 2022\nhigh-end Linux PC.\nI describe my setup in more details in stapelberg uses this: my 2020 desk\nsetup.\nMost importantly for this article, I use a Dell 8K 32\"\nmonitor (resolution: 7680x4320!), which, in my\nexperience, is only compatible with nVidia graphics cards (I try other cards\nsometimes).\nHence, both the lab PC and my main PC contain an nVidia GPU:\n\nThe lab PC contains a nVidia GeForce RTX 4070 Ti.\nThe main PC contains a nVidia GeForce RTX 3060 Ti.\n\n(In case youâ€™re wondering why I use the older card in my PC: I had a crash once\nwhere I suspected the GPU, so I switched back from the 4070 to my older 3060.)\nnVidia driver support\nFor many years, nVidia drivers were entirely unsupported under Wayland.\nApparently, nVidia refused to support the API that Wayland was using, insisting\nthat their EGLStreams approach was superior. Luckily, with nVidia driver 495\n(late 2021), they added support for GBM (Generic Buffer Manager).\nBut, even with GBM support, while you could now start many Wayland sessions, the\nsession wouldnâ€™t run smoothly: You would see severe graphics glitches and\nartifacts, preventing you from getting any work done.\nThe solution for the glitches was explicit sync support: because the nVidia\ndriver does not support implicit sync (like AMD or Intel), Wayland (and\nwlroots, and sway) needed to get explicit sync\nsupport.\nSway 1.11 (June 2025) and wlroots 0.19.0 are the first version with explicit\nsync support.\nNot working: TILE support for 8K monitor\nWith the nVidia driver now working per se with Wayland, unfortunately thatâ€™s\nstill not good enough to use Wayland in my setup: my Dell UP3218K\nmonitor requires two DisplayPort 1.4\nconnections with MST (Multi Stream Transport) and TILE support. This\ncombination worked just fine under X11 for the last 8+ years.\nWhile GNOME successfully configures the monitor with its native resolution of\n7680x4320@60, the monitor incorrectly shows up as two separate monitors in sway.\nThe reason behind this behavior is that wlroots does not support the TILE\nproperty (issue #1580 from\n2019). Luckily,\nin 2023, contributor EBADBEEF sent draft merge request\n!4154,\nwhich adds support for the TILE property.\nBut, even with the TILE patch, my monitor would not work correctly: The right\nhalf of the monitor would just stay black. The full picture is visible when\ntaking a screenshot with grim, so it seems like an output issue. I had a few\nexchanges about this with EBADBEEF starting in August 2025 (thanks for taking\na look!), but we couldnâ€™t figure out the issue.\nA quarter later, I had made good experiences regarding debugging complex issues\nwith the coding assistant Claude Code\n(Opus 4.5 at the time of writing), so I decided to give it another try. Over two\ndays, I ran a number of tests to narrow down the issue, letting Claude analyze\nsource code (of sway, wlroots, Xorg, mesa, â€¦) and produce test programs that I\ncould run manually.\nUltimately, I ended up with a minimal reproducer program (independent of\nWayland) that shows how the SRC_X DRM property does not work on nVidia (but\ndoes work on Intel, for example!): I posted a bug report with a video in the\nnVidia\nforum\nand hope an nVidia engineer will take a look!\nCrucially, with the bug now identified, I had Claude implement a workaround:\ncopy the right half of the screen (at SRC_X=3840) to another buffer, and then\ndisplay that buffer, but with SRC_X=0.\nWith that\npatch\napplied, for the first time, I can use Sway on my 8K monitor! ğŸ¥³\n\nBy the way, when I mentioned that GNOME successfully configures the native\nresolution, that doesnâ€™t mean the monitor is usable with GNOME! While GNOME\nsupports tiled displays, the updates of individual tiles are not synchronized,\nso you see heavy tearing in the middle of the screen, much worse than anything I\nhave ever observed under X11. GNOME/mutter merge request\n!4822 should\nhopefully address this.\nSoftware: NixOS\nDuring 2025, I switched all my computers to NixOS. Its\ndeclarative approach is really nice for doing such tests, because you can\nreliably restore your system to an earlier version.\nTo make a Wayland/sway session available on my NixOS 25.11 installation, I added\nthe following lines to my NixOS configuration file (configuration.nix):\n# GDM display manager (can launch both X11/i3 and Wayland/Sway sessions)\nservices.displayManager.gdm.enable = true;\nservices.displayManager.gdm.autoSuspend = false;\n\n# enable GNOME (for testing)\nservices.desktopManager.gnome.enable = true;\n\nprograms.sway = {\n  enable = true;\n  wrapperFeatures.gtk = true;\n  extraOptions = [ \"--unsupported-gpu\" ];\n};\nI also added the following Wayland-specific programs to environment.systemPackages:\nenvironment.systemPackages = with pkgs; [\n  # â€¦\n  foot          # terminal emulator\n  wtype         # replacement for xdotool type\n  fuzzel        # fuzzy matching program starter\n  wayland-utils # for wayland-info(1)\n  gammastep     # redshift replacement\n];\nNote that activating this configuration kills your running X11 session, if any.\nJust to be sure, I rebooted the entire machine after changing the configuration.\nExperiment results\nWith this setup, I spent about one full work day in a Wayland session. Trying to\nactually get some work done uncovers issues that might not show in casual\ntesting. Most of the day was spent trying to fix Wayland issues ğŸ˜…. The\nfollowing sections explain what I have learned/observed.\nDesktop: i3 â†’ sway\nMany years ago, when Wayland became more popular, people asked on the i3 issue\ntracker if i3 would be ported to Wayland. I said no: How could I port a program\nto an environment that doesnâ€™t even run on any of my computers? But also, I knew\nthat with working a full-time job, I wouldnâ€™t have time to be an early adopter\nand shape Wayland development.\nThis attitude resulted in Drew DeVault starting the\nSway project around 2016,\nwhich aims to be a Wayland version of i3. I donâ€™t see Sway as\ncompetition. Rather, I thought it was amazing that people liked the i3 project\nso much that they would go through the trouble of creating a similar program for\nother environments! What a nice compliment! ğŸ˜Š\nSway aims to be compatible with i3 configuration files, and it mostly is.\nIf youâ€™re curious, here is what I changed from the Sway defaults, mostly moving\nkey bindings around for the NEO keyboard layout I\nuse, and configuring input/output blocks that I formerly configured in my\n~/.xsession\nfile:\n\nmy changes to the default Sway config\n--- /home/michael/src/sway/config.in\t2025-09-24 19:08:38.876573260 +0200\n+++ /home/michael/.config/sway/config\t2025-12-31 15:50:38.616697542 +0100\n@@ -9,19 +9,76 @@\n # Logo key. Use Mod1 for Alt.\n set $mod Mod4\n # Home row direction keys, like vim\n-set $left h\n-set $down j\n-set $up k\n-set $right l\n+set $left n\n+set $down r\n+set $up t\n+set $right d\n # Your preferred terminal emulator\n set $term foot\n # Your preferred application launcher\n-set $menu wmenu-run\n+set $menu fuzzel\n+\n+font pango:Bitstream Vera Sans Mono 8\n+\n+titlebar_padding 4 2\n+\n+# Make Xwayland windows recognizeable:\n+for_window [shell=\"xwayland\"] title_format \"%title [Xwayland]\"\n+\n+workspace_layout stacking\n+\n+# Open two terminal windows side-by-side on new workspaces:\n+# https://github.com/stapelberg/workspace-populate-for-i3\n+exec ~/go/bin/workspace-populate-for-i3\n+\n+exec gammastep -l 47.31:8.50 -b 0.9\n+\n+input * {\n+   xkb_layout \"de\"\n+   xkb_variant \"neo\"\n+\trepeat_delay 250\n+\trepeat_rate 30\n+}\n+\n+input * {\n+\taccel_profile adaptive\n+\tpointer_accel 0.2\n+}\n \n ### Output configuration\n #\n-# Default wallpaper (more resolutions are available in @datadir@/backgrounds/sway/)\n-output * bg @datadir@/backgrounds/sway/Sway_Wallpaper_Blue_1920x1080.png fill\n+output * bg /dev/null fill #333333\n+output * scale 3\n #\n # Example configuration:\n #\n@@ -33,14 +90,41 @@\n #\n # Example configuration:\n #\n-# exec swayidle -w \\\n-#          timeout 300 'swaylock -f -c 000000' \\\n-#          timeout 600 'swaymsg \"output * power off\"' resume 'swaymsg \"output * power on\"' \\\n-#          before-sleep 'swaylock -f -c 000000'\n+exec swayidle -w \\\n+         before-sleep '~/swaylock.sh' \\\n+         lock '~/swaylock.sh'\n #\n # This will lock your screen after 300 seconds of inactivity, then turn off\n # your displays after another 300 seconds, and turn your screens back on when\n # resumed. It will also lock your screen before your computer goes to sleep.\n+bindsym $mod+l exec loginctl lock-session\n+\n+  # Notifications\n+  bindsym $mod+period exec dunstctl close\n \n ### Input configuration\n #\n@@ -63,11 +147,13 @@\n     # Start a terminal\n     bindsym $mod+Return exec $term\n \n     # Kill focused window\n-    bindsym $mod+Shift+q kill\n+    bindsym $mod+Shift+x kill\n \n     # Start your launcher\n-    bindsym $mod+d exec $menu\n+    bindsym $mod+a exec $menu\n \n     # Drag floating windows by holding down $mod and left mouse button.\n     # Resize them with right mouse button + $mod.\n@@ -142,12 +228,11 @@\n     bindsym $mod+v splitv\n \n     # Switch the current container between different layout styles\n-    bindsym $mod+s layout stacking\n+    bindsym $mod+i layout stacking\n     bindsym $mod+w layout tabbed\n-    bindsym $mod+e layout toggle split\n \n     # Make the current focus fullscreen\n-    bindsym $mod+f fullscreen\n+    bindsym $mod+e fullscreen\n \n     # Toggle the current focus between tiling and floating mode\n     bindsym $mod+Shift+space floating toggle\n@@ -156,7 +241,7 @@\n     bindsym $mod+space focus mode_toggle\n \n     # Move focus to the parent container\n-    bindsym $mod+a focus parent\n+    bindsym $mod+u focus parent\n #\n # Scratchpad:\n #\n@@ -192,37 +277,25 @@\n     bindsym Return mode \"default\"\n     bindsym Escape mode \"default\"\n }\n-bindsym $mod+r mode \"resize\"\n+#bindsym $mod+r mode \"resize\"\n \n #\n # Status Bar:\n #\n # Read `man 5 sway-bar` for more information about this section.\n bar {\n-    position top\n \n     # When the status_command prints a new line to stdout, swaybar updates.\n     # The default just shows the current date and time.\n-    status_command while date +'%Y-%m-%d %X'; do sleep 1; done\n+    status_command i3status\n }\n \n\nI encountered the following issues with Sway:\n\n\nI donâ€™t know how I can configure the same libinput settings that I had\nbefore.  See xinput-list-props-mx-ergo.txt\nfor what I have on X11. Swayâ€™s available accel_profile settings do not seem\nto match what I used before.\n\n\nThe mouse cursor / pointer seems laggy, somehow?! It seems to take longer to\nreact when I move the trackball, and it also seems to move less smoothly\nacross the screen.\nSimon Ser suspects that this might be because\nhardware cursor support might not work with the nVidia drivers currently.\n\n\nNo Xwayland scaling: programs started via Xwayland are blurry (by default) or\ndouble-scaled (when setting Xft.dpi: 288). This is a Sway-specific\nlimitation: KDE fixed this in\n2022. From\nSway issue #2966, I can tell\nthat Sway developers do not seem to like this approach for some reason, but\nthatâ€™s very unfortunate for my migration: The backwards compatibility\noption of running older programs through Xwayland is effectively unavailable\nto me.\n\n\nSometimes, keyboard shortcuts seem to be executed twice! Like, when I focused\nthe first of five Chrome windows in a stack and moved that window to another\nworkspace, two windows would be moved instead of one. I also see messages\nlike this one (not exactly correlated with the double-shortcut problem,\nthough):\n[ERROR] [wlr] [libinput] event0  - https: kinT (kint36): client bug: event\nprocessing lagging behind by 32ms, your system is too slow\nâ€¦and that seems wrong to me. My high-end Linux\nPC certainly isnâ€™t slow by any\nmeasure.\n\n\nGTK: Font size\nWhen I first started GTK programs like GIMP or Emacs, I noticed all fonts were\nway too large! Apparently, I still had some scaling-related settings that I\nneeded to reset like so:\ngsettings reset org.gnome.desktop.interface scaling-factor\ngsettings reset org.gnome.desktop.interface text-scaling-factor\nDebugging tip: Display GNOME settings using dconf dump / (stored in\n~/.config/dconf).\nGTK: Backend\nSome programs like geeqie apparently need an explicit export GDK_BACKEND=wayland environment variable, otherwise they run in\nXwayland. Weird.\nFont rendering\nI also noticed that font rendering is different between X11 and Wayland! The\ndifference is visible in Chrome browser tab titles and the URL bar, for example:\n\nAt first I thought that maybe Wayland defaults to different font-antialiasing\nand font-hinting settings, but I tried experimenting with the following settings\n(which default to font-antialiasing=grayscale and font-hinting=slight), but\ncouldnâ€™t get things to render like they did before:\ngsettings set org.gnome.desktop.interface font-antialiasing 'rgba'\ngsettings set org.gnome.desktop.interface font-hinting 'full'\nUpdate: Thanks to\nHugo for pointing out\nthat under Wayland, GTK3 ignores the ~/.config/gtk-3.0/settings.ini\nconfiguration file and uses dconf exclusively! Setting the following dconf\nsetting makes the font rendering match:\ngsettings set org.gnome.desktop.interface font-name 'Cantarell 11'\nScreen locker: swaylock\nThe obvious replacement for i3lock is\nswaylock.\nI quickly ran into a difference in architecture between the two programs:\n\n\ni3lock shows a screen locker window. When you kill i3lock, the screen is\nunlocked.\n\n\nWhen you kill swaylock, you end up in a Red Screen Of Death.\nTo get out of this state, you need to restart swaylock and unlock. You can\nunlock from the command line by sending SIGUSR1 to the swaylock process.\n\n\nThis was very surprising to me, but is by (Wayland) design! See Sway issue\n#7046 for details, and this quote from\nthe ext-session-lock-v1 Wayland protocol:\n\nâ€œThe compositor must stop rendering and provide input to normal\nclients. Instead the compositor must blank all outputs with an opaque color\nsuch that their normal content is fully hidden.â€\n\nOK, so when you start swaylock via SSH for testing, remember to always unlock\ninstead of just cancelling swaylock with Ctrl+C. And hope it never crashes.\nI used to start i3lock via a wrapper script, which turns off the monitor\n(input wakes it up):\n#!/bin/sh\n# Turns on DPMS, mutes all output, locks the screen.\n# Reverts all settings on unlock, or when killed.\n\nrevert() {\n    xset dpms 0 0 0\n    pactl set-sink-mute @DEFAULT_SINK@ 0\n}\ntrap revert SIGHUP SIGINT SIGTERM\nxset +dpms dpms 15 15 15\n(sleep 1 && xset dpms force off) &\npactl set-sink-mute @DEFAULT_SINK@ 1\ni3lock --raw 3840x2160:rgb --image ~/i3lock-wallpaper-3840x2160.rgb -n \nrevert\nWith Wayland, the DPMS behavior has to be implemented differently, with swayidle:\n#!/bin/sh\n# Turns on DPMS, mutes all output, locks the screen.\n# Reverts all settings on unlock, or when killed.\n\nswayidle -w \\\n  timeout 5 'swaymsg \"output * dpms off\"' \\\n  resume 'swaymsg \"output * dpms on\"' &\nswayidle=$!\n\nrevert() {\n    kill $swayidle\n    pactl set-sink-mute @DEFAULT_SINK@ 0\n}\ntrap revert SIGHUP SIGINT SIGTERM\n\npactl set-sink-mute @DEFAULT_SINK@ 1\nswaylock --image ~/i3lock-wallpaper-3840x2160.jpg\nrevert\ni3 IPC automation\nThe i3 window manager can be extended via its IPC interface (interprocess\ncommunication).\nI use a few small tools that use this interface.\nI noticed the following issues when using these tools with Sway:\n\n\nTools using the go.i3wm.org/i3/v4 Go\npackage need a special socket path\nhook\ncurrently. We\nshould probably include transparent handling in the package to ease the\ntransition.\n\n\nTools started with exec from the Sway config unexpectedly keep running even\nwhen you exit Sway (swaymsg exit) and log into a new session!\n\n\nMy\nworkspace-populate-for-i3\ndid not work:\n\nSway does not implement i3â€™s layout\nsaving/restoring because Drew\ndecided in 2017 that the feature is â€œtoo complicated and hacky for too\nlittle\nbenefitâ€. Too\nbad. I have a couple of layouts I liked that Iâ€™ll need to replicate\ndifferently.\nSway does not match workspace nodes with [con_id] criteria. Thereâ€™s\npull request #8980 (posted\nindependently, five days ago) to fix that.\n\n\n\nMy wsmgr-for-i3 worked\npartially:\n\nRestoring workspaces (wsmgr restore) worked.\nSwayâ€™s rename workspace\ncommand\nimplementation does not seem to pick up workspace numbers from the target\nname.\n\n\n\nTerminal: foot\nOn X11, I use the rxvt-unicode\n(URxvt) terminal emulator. It has a couple of quality-of-life features that I\ndonâ€™t want to lose, aside from being fast and coming with a minimal look:\n\nBackwards search through your scrollback (= command output)\nOpening URLs in your scrollback using keyboard shortcuts\nOpening a new terminal window in the same working directory\nUpdating the terminal title from your shell\n\nIn earlier experiments, I tried Alacritty or Kitty, but wasnâ€™t happy with\neither.\nThanks to anarcatâ€™s blog post â€œWayland: i3 to Sway\nmigrationâ€, I\ndiscovered the foot terminal emulator, which\nlooks like a really nice option!\nI started a foot.ini config\nfile\nto match my URxvt config, but later I noticed that at least some colors donâ€™t\nseem to match (some text lines with green/red background looked different). Iâ€™m\nnot sure why and have not yet looked into it any further.\nI noticed the following issues using foot:\n\n\nPressing Ctrl+Enter (which I seem to do by mistake quite a bit) results in\nescape sequences, whereas URxvt just treats Ctrl+Enter like Enter.\nThis can be worked around in your shell (Zsh, in my case), see foot issue\n#628 for details.\n\n\nDouble-clicking on part of a URL with the mouse selects the URL (as expected),\nbut without the https: scheme prefix! Annoying when you do want to use the\nmouse.\nI can hold Ctrl to work around this, which will make foot select everything\nunder the pointer up to, and until, the next space characters.\n\n\nStarting screen(1)\n in foot results in not having\ncolor support for programs running inside the screen session. Probably a\nterminfo-related problem somehowâ€¦? I can also reproduce this issue with GNOME\nterminal. But with URxvt or xterm, it\nworks.\n\n\nSelecting text highlights the text within the line, but not the entire line.\nThis is different from other terminal emulators I am used to, but I donâ€™t see\nan option to change it.\nHereâ€™s a screenshot showing foot after triple-clicking on the right of\nâ€œkthreaddâ€:\n\nBut triple-clicking on an echo output line highlights only the contents, not\nthe whole line:\n\n\n\nText editor: Emacs\nI find Emacsâ€™s Wayland support rather disappointing. The standard version of\nEmacs only supports X11, so on Sway, it starts in Xwayland. Because Sway does\nnot support scaling with Xwayland, Emacs shows up blurry (top/background\nwindow):\n\nNative Wayland support (bottom/foreground window) is only available in the\npgtk Emacs version (emacs-pgtk on NixOS). pgtk used to be a separate\nbranch, but was merged in EmacsÂ 29 (July 2023). There seem to be issues\nwith pgtk on X11 (you get a warning when starting Emacs-pgtk on X11), so there\nhave to be two separate versions for nowâ€¦\nUnfortunately, the pgtk text rendering looks different than native X11 text\nrendering! The line height and letter spacing seems different:\n\nIâ€™m not sure why itâ€™s different! Does anybody know how to make it match the old\nbehavior?\nAside from the different text rendering, the other major issue for me is input\nlatency: Emacs-pgtk feels significantly slower (less responsive) than\nEmacs. This was reported on Reddit multiple times (thread\n1,\nthread\n2)\nand Emacs bug #71591, but\nthere doesnâ€™t seem to be any solution.\nIâ€™ll also need a solution for running Emacs remotely. Thus far, I use X11\nforwarding over SSH (which works fine and with low latency over fiber\nconnections). I should probably check out waypipe, but have not yet had a\nchance.\nBrowser: Chrome\nWhen starting Chrome and checking the chrome://gpu debug page, things look\ngood:\n\nBut rather quickly, after moving and resizing browser windows, the GPU process\ndies with messages like the following and, for example, WebGL is no longer\nhardware accelerated:\nERROR:ui/ozone/platform/wayland/gpu/gbm_pixmap_wayland.cc:95] Cannot create bo with format=RGBA_8888 and usage=Scanout|Rendering|Texturing\nERROR:ui/gfx/linux/gbm_wrapper.cc:405] Failed to create BO with modifiers: Invalid argument (22)\nERROR:ui/ozone/platform/wayland/gpu/gbm_pixmap_wayland.cc:95] Cannot create bo with format=RGBA_8888 and usage=Texturing\nERROR:gpu/command_buffer/service/shared_image/shared_image_factory.cc:981] CreateSharedImage: could not create backing.\nERROR:gpu/command_buffer/service/shared_image/shared_image_manager.cc:397] SharedImageManager::ProduceSkia: Trying to Produce a Skia representation from a non-existent mailbox.\nERROR:components/viz/service/gl/exit_code.cc:13] Restarting GPU process due to unrecoverable error. Context was lost.\nR:gpu/ipc/client/command_buffer_proxy_impl.cc:321] GPU state invalid after WaitForGetOffsetInRange.\nERROR:content/browser/gpu/gpu_process_host.cc:1005] GPU process exited unexpectedly: exit_code=8704\nOf course, using a browser without hardware acceleration is very frustrating,\nespecially at high resolutions. Starting Chrome with --disable-gpu-compositing\nseems to work around the GPU process exiting, but Chrome still does not feel as\nsmooth as on X11.\nAnother big issue for me is that Sway does not open Chrome windows on the\nworkspace on which I closed them. Support for tracking and restoring the\n_NET_WM_DESKTOP EWMH atom was added to i3 in January\n2016\nand to Chrome in May\n2016\nand Firefox in March\n2020.\nI typically have 5+ workspaces and even more Chrome windows at any given point,\nso having to sort through 10+ Chrome windows every day (when I boot my work\ncomputer) is very annoying.\nSimon Ser said that this would be addressed with\na new Wayland protocol (xdg-session-management, merge request\n!18).\nScreensharing\nI work remotely a lot, so screen sharing is a table-stakes feature for me.  I\nuse screen sharing in my browser almost every day, in different scenarios and\nwith different requirements.\nIn X11, I am used to the following experience with Chrome. I click the â€œWindowâ€\ntab and see previews of my windows. When I select the window and confirm, its\ncontents get shared:\n\nTo get screen sharing to work in Wayland/sway, you need to install\nxdg-desktop-portal and xdg-desktop-portal-wlr (the latter is specific to\nwlroots, which sway uses).\nWith these packages set up, this is the behavior I see:\n\nI can share a Chrome tab.\nI can share the entire monitor.\nI cannot share a specific window (the entire monitor shows up as a single\nwindow).\n\nThis is a limitation of xdg-desktop-portal-wlr (and\nothers), which\nshould be addressed with the upcoming Sway 1.12 release.\nI changed my NixOS configuration to use sway and wlroots from git to try it\nout. When I click on the â€œWindowâ€ tab, I see a chooser in which I need to select\na window:\n\nAfter selecting the window, I see only that windowâ€™s contents previewed in\nChrome:\n\nAfter confirming, I get another chooser and need to select the window\nagain. Notably, there is no connection between the previewed window and the\nchosen window in this second step â€” if I chose a different window, thatâ€™s what\nwill be shared:\n\nNow that window is screenshared (so the feature now works; nice!), but\nunfortunately in low resolution, meaning the text is blurry for my co-workers.\nI reported this as xdg-desktop-portal-wlr issue\n#364 and it\nseems like the issue is that the wrong scale factor is applied. The patch\nprovided in the issue works for me.\nBut, on a high level, the whole flow seems wrong: I shouldnâ€™t see a chooser when\nclicking on Chromeâ€™s â€œWindowâ€ tab. I should see previews of all windows. I\nshould be able to select the window in Chrome, not with a separate chooser.\nScaling Glitches\nI also noticed a very annoying glitch when output scaling is enabled: the\ncontents of (some!) windows would â€œjump aroundâ€ as I was switching between\nwindows (in a tabbed or stacked container) or between workspaces.\nI first noticed this in the foot terminal emulator, where the behavior is as follows:\n\nSwitch focus to another foot terminal by changing workspaces, or by\nswitching focus within a stacked or tabbed container.\nThe new foot terminal shows up with its text contents slightly offset.\nWithin a few milliseconds, footâ€™s text jumps to the correct position.\n\nI captured the following frame with my iPhone just as the content was moving a\nfew pixels, shortly after switching focus to this window:\n\nLater, I also noticed that Chrome windows briefly show up blurry after\nswitching.\nMy guess is that because Sway sets the scale factor to 1 for invisible windows,\nwhen switching focus you see a scale-1 content buffer until the application\nprovided its scale-3 content buffer.\nNotifications: dunst\ndunst supports Wayland natively. I tried dunst 1.13 and did not notice any\nissues.\nPicker: rofi\nrofi works on Wayland since v2.0.0 (2025-09-01).\nI use rofi with rofimoji as my Emoji\npicker. For text input, instead of xdotool, wtype seems to work. I didnâ€™t\nnotice any issues.\nScreenshots: grim?\nInstead of my usual choice maim(1)\n, I tried grim(1)\n, but unfortunately grimâ€™s -T flag to select the\nwindow to capture is rather cumbersome to use (and captures in 1x scale).\nDoes anyone have any suggestions for a good alternative?\nConclusion\nFinally I made some progress on getting a Wayland session to work in my\nenvironment!\nBefore giving my verdict on this Wayland/sway experiment, let me explain that my\nexperience on X11/i3 is really good. I donâ€™t see any tearing or other artifacts\nor glitches in my day-to-day computer usage. I donâ€™t use a compositor, so my\ninput latency is really good: I once measured it to approximately 763 Î¼s in\nEmacs on X11 with my custom-built keyboard (plus output latency), see kinX:\nlatency measurement (2018).\nSo from my perspective, switching from this existing, flawlessly working stack\n(for me) to Sway only brings downsides. I observe new graphical glitches that I\ndidnâ€™t have before. The programs I spend most time in (Chrome and Emacs) run\nnoticeably worse. Because of the different implementations, or because I need to\nswitch programs entirely, I encounter a ton of new bugs.\nFor the first time, an on-par Wayland experience seems within reach, but\nrealistically it will require weeks or even months of work still. In my\nexperience, debugging sessions quickly take hours as I need to switch graphics\ncards and rewire monitors to narrow down bugs. I donâ€™t have the time to\ncontribute much to fixing these numerous issues unfortunately, so Iâ€™ll keep\nusing X11/i3 for now.\nFor me, a Wayland/Sway session will be ready as my daily driver when:\n\nSway no longer triggers some key bindings twice some times (â€œghost key\npressesâ€)\nI no longer see glitches when switching between windows or workspaces in Sway.\nChrome is continuously hardware-accelerated.\nChrome windows are restored to their previous workspace when starting.\nEmacs either:\n\nRuns via Xwayland and Sway makes scaling work.\nOr if its pgtk variant fixes its input latency issues\nand can be made to render text the same as before somehow."
    },
    {
      "title": "michael.stapelberg.ch: Self-hosting my photos with Immich",
      "url": "https://michael.stapelberg.ch/posts/2025-11-29-self-hosting-photos-with-immich/",
      "source": "michael.stapelberg.ch",
      "time": "2026-02-20T10:18:16.015273",
      "raw_desc": "For every cloud service I use, I want to have a local copy of my data for backup\npurposes and independence. Unfortunately, the gphotos-sync tool stopped\nworking in March\n2025 when\nGoogle restricted the OAuth scopes, so I needed an alternative for my existing\nGoogle Photos setup. In this post, I describe how I have set up\nImmich, a self-hostable photo manager.\nHere is the end result: a few (live) photos from NixCon\n2025:\n\nStep 1. Hardware\nI am running Immich on my Ryzen 7 Mini PC (ASRock DeskMini\nX600), which\nconsumes less than 10 W of power in idle and has plenty of resources for VMs (64\nGB RAM, 1 TB disk). You can read more about it in my blog post from July 2024:\n\n\nRyzen 7 Mini-PC makes a power-efficient VM host\n\n\n\n\n\n\t  When I saw the first reviews of the ASRock DeskMini X600 barebone, I was immediately interested in building a home-lab hypervisor (VM host) with it. Apparently, the DeskMini X600 uses less than 10W of power but supports latest-generation AMD CPUs like the Ryzen 7 8700G!\n\n\t  Read more â†’\n\n\n\n\nI installed Proxmox, an Open Source virtualization\nplatform, to divide this mini server into VMs, but you could of course also\ninstall Immich directly on any server.\nStep 2. Install Immich\nI created a VM (named â€œphotosâ€) with 500 GB of disk space, 4 CPU cores and 4 GB of RAM.\nFor the initial import, you could assign more CPU and RAM, but for normal usage, thatâ€™s enough.\nI (declaratively) installed\nNixOS on that VM as described in this blog post:\n\n\nHow I like to install NixOS (declaratively)\n\n\n\n\n\n\t  For one of my network storage PC builds, I was looking for an alternative to Flatcar Container Linux and tried out NixOS again (after an almost 10 year break). There are many ways to install NixOS, and in this article I will outline how I like to install NixOS on physical hardware or virtual machines: over the network and fully declaratively.\n\n\t  Read more â†’\n\n\n\n\nAfterwards, I enabled Immich, with this exact configuration:\nservices.immich = {\n  enable = true;\n};\nAt this point, Immich is available on localhost, but not over the network,\nbecause NixOS enables a firewall by default. I could enable the\nservices.immich.openFirewall option, but I actually want Immich to only be\navailable via my Tailscale VPN, for which I donâ€™t need to open firewall access â€”\ninstead, I use tailscale serve to forward traffic to localhost:2283:\nphotos# tailscale serve --bg http://localhost:2283\nBecause I have Tailscaleâ€™s MagicDNS\nand TLS certificate provisioning\nenabled, that means I can now open https://photos.example.ts.net in my browser\non my PC, laptop or phone.\nStep 2. Initial photos import\nAt first, I tried importing my photos using the official Immich CLI:\n% nix run nixpkgs#immich-cli -- login https://photos.example.ts.net secret\n% nix run nixpkgs#immich-cli -- upload --recursive /home/michael/lib/photo/gphotos-takeout\nUnfortunately, the upload was not running reliably and had to be restarted\nmanually a few times after running into a timeout. Later I realized that this\nwas because the Immich server runs background jobs like thumbnail creation,\nmetadata extraction or face detection, and these background jobs slow down the\nupload to the extent that the upload can fail with a timeout.\nThe other issue was that even after the upload was done, I realized that Google\nTakeout archives for Google Photos contain metadata in separate JSON files next\nto the original image files:\n\nUnfortunately, these files are not considered by immich-cli.\nLuckily, there is a great third-party tool called\nimmich-go, which solves both of these\nissues! It pauses background tasks before uploading and restarts them\nafterwards, which works much better, and it does its best to understand Google\nTakeout archives.\nI ran immich-go as follows and it worked beautifully:\n% immich-go \\\n  upload \\\n  from-google-photos \\\n  --server=https://photos.example.ts.net \\\n  --api-key=secret \\\n  ~/Downloads/takeout-*.zip\nStep 3. Install the Immich iPhone App\nMy main source of new photos is my phone, so I installed the Immich app on my\niPhone, logged into my Immich server via its Tailscale URL and enabled automatic\nbackup of new photos via the icon at the top right.\nI am not 100% sure whether these settings are correct, but it seems like camera\nphotos generally go into Live Photos, and Recent should cover other filesâ€¦?!\nIf anyone knows, please send an explanation (or a link!) and I will update the article.\n\nI also strongly recommend to disable notifications for Immich, because otherwise\nyou get notifications whenever it uploads images in the background. These\nnotifications are not required for background upload to work, as an Immich\ndeveloper confirmed on\nReddit. Open\nSettings â†’ Apps â†’ Immich â†’ Notifications and un-tick the permission checkbox:\n\nStep 4. Backup\nImmichâ€™s documentation on\nbackups contains\nsome good recommendations. The Immich developers recommend backing up the entire\ncontents of UPLOAD_LOCATION, which is /var/lib/immich on NixOS. The\nbackups subdirectory contains SQL dumps, whereas the 3 directories upload,\nlibrary and profile contain all user-uploaded data.\nHence, I have set up a systemd timer that runs rsync to copy /var/lib/immich\nonto my PC, which is enrolled in a 3-2-1 backup\nscheme.\nWhatâ€™s missing?\nImmich (currently?) does not contain photo editing features, so to rotate or\ncrop an image, I download the image and use GIMP.\nTo share images, I still upload them to Google Photos (depending on who I share\nthem with).\nWhy Immich instead ofâ€¦?\nThe two most promising options in the space of self-hosted image management\ntools seem to be Immich and Ente.\nI got the impression that Immich is more popular in my bubble, and Ente made the\nimpression on me that its scope is far larger than what I am looking for:\n\nEnte is a service that provides a fully open source, end-to-end encrypted\nplatform for you to store your data in the cloud without needing to trust the\nservice provider. On top of this platform, we have built two apps so far: Ente\nPhotos (an alternative to Apple and Google Photos) and Ente Auth (a 2FA\nalternative to the deprecated Authy).\n\nI donâ€™t need an end-to-end encrypted platform. I already have encryption on the\ntransit layer (Tailscale) and disk layer (LUKS), no need for more complexity.\nConclusion\nImmich is a delightful app! Itâ€™s very fast and generally seems to work well.\nThe initial import is smooth, but only if you use the right tool. Ideally, the\nofficial immich-cli could be improved. Or maybe immich-go could be made the\nofficial one.\nI think the auto backup is too hard to configure on an iPhone, so that could\nalso be improved.\nBut aside from these initial stumbling blocks, I have no complaints."
    },
    {
      "title": "grantslatton.com: Every Man a Microservice",
      "url": "https://grantslatton.com/every-man-a-microservice",
      "source": "grantslatton.com",
      "time": "2026-02-20T10:18:16.053637",
      "raw_desc": ""
    },
    {
      "title": "grantslatton.com: Manufacturing as Maintenance",
      "url": "https://grantslatton.com/manufacturing-as-maintenance",
      "source": "grantslatton.com",
      "time": "2026-02-20T10:18:16.053637",
      "raw_desc": ""
    },
    {
      "title": "grantslatton.com: LLM Memory",
      "url": "https://grantslatton.com/llm-memory",
      "source": "grantslatton.com",
      "time": "2026-02-20T10:18:16.053637",
      "raw_desc": ""
    },
    {
      "title": "Gemini 3.1 Pro",
      "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/",
      "source": "Hacker News",
      "time": "2026-02-19T23:19:57",
      "raw_desc": ""
    },
    {
      "title": "Show HN: Micasa â€“ track your house from the terminal",
      "url": "https://micasa.dev",
      "source": "Hacker News",
      "time": "2026-02-19T23:54:14",
      "raw_desc": ""
    },
    {
      "title": "Micropayments as a reality check for news sites",
      "url": "https://blog.zgp.org/micropayments-as-a-reality-check-for-news-sites/",
      "source": "Hacker News",
      "time": "2026-02-20T03:42:31",
      "raw_desc": ""
    },
    {
      "title": "An ARM Homelab Server, or a Minisforum MS-R1 Review",
      "url": "https://sour.coffee/2026/02/20/an-arm-homelab-server-or-a-minisforum-ms-r1-review/",
      "source": "Hacker News",
      "time": "2026-02-20T09:33:36",
      "raw_desc": ""
    },
    {
      "title": "America vs. Singapore: You can't save your way out of economic shocks",
      "url": "https://www.governance.fyi/p/america-vs-singapore-you-cant-save",
      "source": "Hacker News",
      "time": "2026-02-19T22:52:18",
      "raw_desc": ""
    },
    {
      "title": "A terminal weather app with ASCII animations driven by real-time weather data",
      "url": "https://github.com/Veirt/weathr",
      "source": "Hacker News",
      "time": "2026-02-20T01:47:25",
      "raw_desc": ""
    },
    {
      "title": "US plans online portal to bypass content bans in Europe and elsewhere",
      "url": "https://www.reuters.com/world/us-plans-online-portal-bypass-content-bans-europe-elsewhere-2026-02-18/",
      "source": "Hacker News",
      "time": "2026-02-19T06:25:13",
      "raw_desc": ""
    },
    {
      "title": "Archaeologists find possible first direct evidence of Hannibal's war elephants",
      "url": "https://www.smithsonianmag.com/smart-news/archaeologists-unearthed-a-2200-year-old-bone-they-say-it-could-be-the-first-direct-evidence-of-hannibals-legendary-war-elephants-180988185/",
      "source": "Hacker News",
      "time": "2026-02-20T02:31:50",
      "raw_desc": ""
    },
    {
      "title": "Paged Out Issue #8 [pdf]",
      "url": "https://pagedout.institute/download/PagedOut_008.pdf",
      "source": "Hacker News",
      "time": "2026-02-19T20:13:44",
      "raw_desc": ""
    },
    {
      "title": "Pebble Production: February Update",
      "url": "https://repebble.com/blog/february-pebble-production-and-software-updates",
      "source": "Hacker News",
      "time": "2026-02-19T20:36:00",
      "raw_desc": ""
    }
  ]
}